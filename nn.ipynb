{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "ebab228a",
   "metadata": {},
   "source": [
    "### Data and libraries import"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "fde621f7",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import label_binarize\n",
    "\n",
    "from sklearn.model_selection import KFold\n",
    "from itertools import product\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import TensorDataset, DataLoader\n",
    "\n",
    "import statsmodels.api as sm\n",
    "from sklearn.metrics import mean_squared_error\n",
    "\n",
    "import os\n",
    "import random\n",
    "\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "from sklearn.metrics import (confusion_matrix, ConfusionMatrixDisplay, accuracy_score, f1_score, \n",
    "                            recall_score, precision_score, roc_auc_score, roc_curve, \n",
    "                            precision_recall_curve, average_precision_score, classification_report)\n",
    "from sklearn.calibration import CalibratedClassifierCV, calibration_curve\n",
    "\n",
    "from matplotlib.colors import TwoSlopeNorm\n",
    "from sklearn.decomposition import PCA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "0c9d5410",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "iPhone Summary\n",
      "                   price.value                                      \n",
      "                         count        mean     min   median      max\n",
      "price_tier_encoded                                                  \n",
      "0                          124  352.649919  215.00  349.915   439.99\n",
      "1                          166  536.551687  446.99  537.975   650.00\n",
      "2                           96  848.694583  650.00  776.995  1449.99 \n",
      "\n",
      "Soccer Summary\n",
      "                   price.value                                 \n",
      "                         count       mean    min median     max\n",
      "price_tier_encoded                                             \n",
      "0                          234  29.398974   5.94  28.99   38.50\n",
      "1                          296  49.936689  38.88  49.99   60.00\n",
      "2                          214  83.014159  60.00  73.50  299.99 \n",
      "\n",
      "iPhone price std dev:\n",
      "213.02099455818362\n",
      "\n",
      " Soccer std dev:\n",
      "28.336472098932514\n"
     ]
    }
   ],
   "source": [
    "tier_order = [\"low\", \"medium\", \"high\"]\n",
    "price_tier_map = {\"low\": 0, \"medium\": 1, \"high\": 2}\n",
    "\n",
    "df_iphone = pd.read_csv(\"data/iphone_kmc.csv\")\n",
    "df_iphone[\"price_tier\"] = pd.Categorical(df_iphone[\"price_tier\"], categories=tier_order, ordered=True)\n",
    "df_iphone[\"price_tier_encoded\"] = df_iphone[\"price_tier\"].map(price_tier_map).astype(int)\n",
    "summary_iphone = df_iphone.groupby([\"price_tier_encoded\"]).agg({\n",
    "    \"price.value\": ['count', 'mean', 'min', \"median\", 'max']})\n",
    "print(\"iPhone Summary\")\n",
    "print(summary_iphone, \"\\n\")\n",
    "\n",
    "df_soccer = pd.read_csv(\"data/soccer_kmc.csv\")\n",
    "df_soccer[\"price_tier\"] = pd.Categorical(df_soccer[\"price_tier\"], categories=tier_order, ordered=True)\n",
    "df_soccer[\"price_tier_encoded\"] = df_soccer[\"price_tier\"].map(price_tier_map).astype(int)\n",
    "df_soccer = df_soccer[df_soccer['year'].notna()]\n",
    "summary_soccer = df_soccer.groupby([\"price_tier_encoded\"]).agg({\n",
    "    \"price.value\": ['count', 'mean', 'min', \"median\", 'max']})\n",
    "print(\"Soccer Summary\")\n",
    "print(summary_soccer, \"\\n\")\n",
    "\n",
    "print(\"iPhone price std dev:\")\n",
    "print(df_iphone[\"price.value\"].std())\n",
    "\n",
    "print(\"\\n\", \"Soccer std dev:\")\n",
    "print(df_soccer[\"price.value\"].std())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9a2a3914",
   "metadata": {},
   "source": [
    "### iPhone"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c4c1e43f",
   "metadata": {},
   "source": [
    "#### Classification"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "71c1928f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(246, 42) (62, 42) (78, 42)\n",
      "(246,) (62,) (78,)\n"
     ]
    }
   ],
   "source": [
    "X_iphone = df_iphone.drop(columns=[\"itemId\", \"title\", \"conditionId\", \"price.value\", \n",
    "                                   \"price_tier\", \"seller.username\", \"condition_desc\", \n",
    "                                   \"category_id\", \"price.currency\", \"cluster\", \n",
    "                                   \"price_tier_encoded\", \"marketingPrice.originalPrice.value\",\n",
    "                                   \"discount_flag\", \"unlocked\"])\n",
    "\n",
    "X_iphone[\"marketingPrice.discountPercentage\"] = X_iphone[\"marketingPrice.discountPercentage\"].fillna(0).astype(int)\n",
    "X_iphone[\"model_number\"] = X_iphone[\"model_number\"].astype(str)\n",
    "\n",
    "bool_cols = X_iphone.select_dtypes(include=['bool']).columns.tolist()\n",
    "numeric_cols = X_iphone.select_dtypes(include=['int', 'float']).columns.tolist()\n",
    "numeric_cols = [col for col in numeric_cols if col not in bool_cols]\n",
    "\n",
    "sc = StandardScaler()\n",
    "X_scaled_num_array = sc.fit_transform(X_iphone[numeric_cols])\n",
    "X_scaled_num_iphone = pd.DataFrame(X_scaled_num_array, columns=numeric_cols, index=X_iphone.index)\n",
    "X_bool_iphone  = X_iphone[bool_cols].astype(int)\n",
    "X_comb = pd.concat([X_scaled_num_iphone, X_bool_iphone], axis=1)\n",
    "\n",
    "categorical_cols = ['condition', 'itemLocation.country', 'category_name', 'specific_carrier', \"model_number\", \"storage\", \"model_variant\"]\n",
    "X_encoded_iphone = pd.get_dummies(X_comb.join(X_iphone[categorical_cols]), columns=categorical_cols, drop_first=True)\n",
    "\n",
    "y_iphone = df_iphone['price_tier_encoded']\n",
    "                         \n",
    "X_train_iphone, X_test_iphone, y_train_iphone, y_test_iphone = train_test_split(X_encoded_iphone, y_iphone,\n",
    "                                                                 test_size=0.20, random_state=1216) \n",
    "\n",
    "#split training set into train / validation for model building\n",
    "X_train_iphone, X_val_iphone, y_train_iphone, y_val_iphone = train_test_split(X_train_iphone, y_train_iphone,\n",
    "                                                                test_size=0.20, random_state=1216)\n",
    "\n",
    "print(X_train_iphone.shape, X_val_iphone.shape, X_test_iphone.shape)\n",
    "print(y_train_iphone.shape, y_val_iphone.shape, y_test_iphone.shape)\n",
    "\n",
    "#for MLR or others, full training set\n",
    "X_train_full_iphone = np.concatenate((X_train_iphone, X_val_iphone), axis=0)\n",
    "y_train_full_iphone = np.concatenate((y_train_iphone, y_val_iphone), axis=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "1cad6274",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "condition                             object\n",
       "seller.feedbackPercentage            float64\n",
       "seller.feedbackScore                   int64\n",
       "topRatedBuyingExperience                bool\n",
       "priorityListing                         bool\n",
       "itemLocation.country                  object\n",
       "marketingPrice.discountPercentage      int64\n",
       "shipping_cost                        float64\n",
       "days_listed                            int64\n",
       "category_name                         object\n",
       "seller_item_count                      int64\n",
       "model_number                          object\n",
       "additional_image_count                 int64\n",
       "title_length                           int64\n",
       "specific_carrier                      object\n",
       "storage                               object\n",
       "model_variant                         object\n",
       "dtype: object"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_iphone.dtypes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "782eb0c4",
   "metadata": {},
   "outputs": [],
   "source": [
    "os.environ['PYTHONHASHSEED'] = '1216'\n",
    "random.seed(1216)\n",
    "np.random.seed(1216)\n",
    "torch.manual_seed(1216)\n",
    "torch.backends.cudnn.deterministic = True\n",
    "torch.backends.cudnn.benchmark = False\n",
    "\n",
    "X_train_iphone = X_train_iphone.astype(np.float32)\n",
    "X_val_iphone = X_val_iphone.astype(np.float32)\n",
    "X_test_iphone = X_test_iphone.astype(np.float32)\n",
    "\n",
    "X_train_tensor_iphone = torch.tensor(X_train_iphone.values, dtype=torch.float32)\n",
    "X_val_tensor_iphone = torch.tensor(X_val_iphone.values, dtype=torch.float32)\n",
    "X_test_tensor_iphone = torch.tensor(X_test_iphone.values, dtype=torch.float32)\n",
    "\n",
    "y_train_tensor_iphone = torch.tensor(y_train_iphone.values, dtype=torch.long)\n",
    "y_val_tensor_iphone = torch.tensor(y_val_iphone.values, dtype=torch.long)\n",
    "y_test_tensor_iphone = torch.tensor(y_test_iphone.values, dtype=torch.long)\n",
    "\n",
    "train_dataset_iphone = TensorDataset(X_train_tensor_iphone, y_train_tensor_iphone)\n",
    "val_dataset_iphone = TensorDataset(X_val_tensor_iphone, y_val_tensor_iphone)\n",
    "test_dataset_iphone = TensorDataset(X_test_tensor_iphone, y_test_tensor_iphone)\n",
    "\n",
    "train_loader_iphone = DataLoader(train_dataset_iphone, batch_size=32, shuffle=True)\n",
    "val_loader_iphone = DataLoader(val_dataset_iphone, batch_size=32, shuffle=False)\n",
    "test_loader_iphone = DataLoader(test_dataset_iphone, batch_size=32, shuffle=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "84097159",
   "metadata": {},
   "outputs": [],
   "source": [
    "class SimpleNN(nn.Module):\n",
    "    def __init__(self, input_dim, units, dropout, n_layers, num_classes):\n",
    "        super(SimpleNN, self).__init__()\n",
    "        layers = []\n",
    "        # First layer\n",
    "        layers.append(nn.Linear(input_dim, units))\n",
    "        layers.append(nn.BatchNorm1d(units)) \n",
    "        layers.append(nn.ReLU())\n",
    "        layers.append(nn.Dropout(dropout))\n",
    "        # Hidden layers\n",
    "        for _ in range(n_layers - 1):\n",
    "            layers.append(nn.Linear(units, units))\n",
    "            layers.append(nn.BatchNorm1d(units)) \n",
    "            layers.append(nn.ReLU())\n",
    "            layers.append(nn.Dropout(dropout))\n",
    "        # Output layer: one output per class\n",
    "        layers.append(nn.Linear(units, num_classes)) \n",
    "        self.net = nn.Sequential(*layers)\n",
    "    def forward(self, x):\n",
    "        return self.net(x)\n",
    "    \n",
    "def train_and_evaluate(units, dropout, lr, n_layers, epochs=10):\n",
    "    model = SimpleNN(input_dim=X_train_tensor_iphone.shape[1], units=units, dropout=dropout, n_layers=n_layers, num_classes=3)\n",
    "    criterion = torch.nn.CrossEntropyLoss()\n",
    "    optimizer = torch.optim.AdamW(model.parameters(), lr=lr)\n",
    "\n",
    "    for epoch in range(epochs):\n",
    "        model.train()\n",
    "        for xb, yb in train_loader_iphone:\n",
    "            optimizer.zero_grad()\n",
    "            preds = model(xb)\n",
    "            loss = criterion(preds, yb)\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "\n",
    "        # Validation per epoch\n",
    "        model.eval()\n",
    "        val_loss = 0\n",
    "        val_correct = 0\n",
    "        val_total = 0\n",
    "        with torch.no_grad():\n",
    "            for xb, yb in val_loader_iphone:\n",
    "                preds = model(xb)\n",
    "                loss = criterion(preds, yb)\n",
    "                val_loss += loss.item() * xb.size(0)\n",
    "                val_correct += (preds.argmax(dim=1) == yb).sum().item()\n",
    "                val_total += yb.size(0)\n",
    "        val_loss /= val_total\n",
    "        val_acc = val_correct / val_total\n",
    "        print(f\"Epoch {epoch+1}, Val Loss: {val_loss:.4f}, Val Accuracy: {val_acc:.4f}\")\n",
    "    return val_loss, val_acc\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "3eb0255f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# from itertools import product\n",
    "# param_grid = {\n",
    "#     'units': [32, 64],\n",
    "#     'dropout': [0.3, 0.4, 0.5],\n",
    "#     'learning_rate': [0.0005, 0.001, 0.005, 0.01],\n",
    "#     'n_layers': [1, 2, 3]\n",
    "# }\n",
    "# results = []\n",
    "# for units, dropout, lr, n_layers in product(param_grid['units'], param_grid['dropout'], param_grid['learning_rate'],\n",
    "#                                  param_grid['n_layers']):\n",
    "#     val_loss, val_acc = train_and_evaluate(units, dropout, lr, n_layers, 15)\n",
    "#     print(f\"units={units}, dropout={dropout}, lr={lr}, n_layers={n_layers}, val_loss={val_loss:.4f}, val_acc={val_acc:.4f}\")\n",
    "#     results.append({'units': units, 'dropout': dropout, 'lr': lr, 'n_layers': n_layers, 'val_loss': val_loss, 'val_acc': val_acc})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "b0de199c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# best = min(results, key=lambda x: x['val_loss'])\n",
    "# print(f\"Best hyperparams: {best}\")\n",
    "# best_params = {k: best[k] for k in ['units', 'dropout', 'lr', 'n_layers']}"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6b6d11ae",
   "metadata": {},
   "source": [
    "Best hyperparams: {'units': 32, 'dropout': 0.3, 'lr': 0.005, 'n_layers': 1, 'val_loss': 0.3963206458476282, 'val_acc': 0.8870967741935484}\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f8de1dbc",
   "metadata": {},
   "outputs": [],
   "source": [
    "# X_combined_iphone = torch.cat([X_train_tensor_iphone, X_val_tensor_iphone], dim=0)\n",
    "# y_combined_iphone = torch.cat([y_train_tensor_iphone, y_val_tensor_iphone], dim=0)\n",
    "# combined_dataset_iphone = TensorDataset(X_combined_iphone, y_combined_iphone)\n",
    "# combined_loader_iphone = DataLoader(combined_dataset_iphone, batch_size=32, shuffle=True)\n",
    "\n",
    "# # Create new model using best hyperparameters\n",
    "# final_model = SimpleNN(\n",
    "#     input_dim=X_train_tensor_iphone.shape[1],\n",
    "#     units=32,\n",
    "#     dropout=0.3,\n",
    "#     n_layers=1,\n",
    "#     num_classes=3\n",
    "# )\n",
    "# optimizer = torch.optim.AdamW(final_model.parameters(), lr=0.005)\n",
    "# criterion = torch.nn.CrossEntropyLoss()\n",
    "\n",
    "# train_losses = []\n",
    "# train_accuracies = []\n",
    "\n",
    "# for epoch in range(50):\n",
    "#     final_model.train()\n",
    "#     running_loss = 0\n",
    "#     correct = 0\n",
    "#     total = 0\n",
    "#     for xb, yb in combined_loader_iphone:\n",
    "#         optimizer.zero_grad()\n",
    "#         preds = final_model(xb)  \n",
    "#         loss = criterion(preds, yb)\n",
    "#         loss.backward()\n",
    "#         optimizer.step()\n",
    "#         running_loss += loss.item() * xb.size(0)\n",
    "#         predicted_classes = preds.argmax(dim=1)\n",
    "#         correct += (predicted_classes == yb).sum().item()\n",
    "#         total += yb.size(0)\n",
    "#     epoch_loss = running_loss / total\n",
    "#     epoch_acc = correct / total\n",
    "#     train_losses.append(epoch_loss)\n",
    "#     train_accuracies.append(epoch_acc)\n",
    "#     print(f\"Epoch {epoch+1}, Loss: {epoch_loss:.4f}, Accuracy: {epoch_acc:.4f}\")\n",
    "\n",
    "#torch.save(final_model.state_dict(), \"models/final_model_iphone.pth\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "851a5b17",
   "metadata": {},
   "outputs": [],
   "source": [
    "# import matplotlib.pyplot as plt\n",
    "# from matplotlib.animation import FuncAnimation\n",
    "# from IPython.display import HTML\n",
    "# import numpy as np\n",
    "\n",
    "# train_losses = np.array(train_losses)\n",
    "# train_accuracies = np.array(train_accuracies)\n",
    "\n",
    "# epochs = np.arange(1, len(train_losses) + 1)\n",
    "\n",
    "# fig, ax1 = plt.subplots(figsize=(8,5))\n",
    "\n",
    "# ax2 = ax1.twinx()\n",
    "\n",
    "# line_loss, = ax1.plot([], [], 'r-', label='Loss')\n",
    "# line_acc, = ax2.plot([], [], 'b-', label='Accuracy')\n",
    "\n",
    "# ax1.set_xlim(1, len(epochs))\n",
    "# ax1.set_ylim(train_losses.min()*0.9, train_losses.max()*1.1)\n",
    "# ax2.set_ylim(train_accuracies.min()*0.9, train_accuracies.max()*1.1)\n",
    "\n",
    "# ax1.set_xlabel('Epoch')\n",
    "# ax1.set_ylabel('Loss', color='r')\n",
    "# ax2.set_ylabel('Accuracy', color='b')\n",
    "# ax1.set_title('Training Loss and Accuracy Over Epochs')\n",
    "\n",
    "# ax1.legend(loc='upper left')\n",
    "# ax2.legend(loc='upper right')\n",
    "\n",
    "# def update(frame):\n",
    "#     x = epochs[:frame+1]\n",
    "#     y_loss = train_losses[:frame+1]\n",
    "#     y_acc = train_accuracies[:frame+1]\n",
    "#     line_loss.set_data(x, y_loss)\n",
    "#     line_acc.set_data(x, y_acc)\n",
    "#     return line_loss, line_acc\n",
    "\n",
    "# ani = FuncAnimation(fig, update, frames=len(epochs), interval=500, blit=True, repeat=False)\n",
    "# ani.save(\"images/5/training_progress.gif\", writer='pillow', fps=2)\n",
    "\n",
    "# plt.close(fig)  # Prevent static plot output in notebook\n",
    "\n",
    "# HTML(ani.to_jshtml())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "6e2886bd",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "         low       0.91      0.91      0.91        22\n",
      "      medium       0.82      0.82      0.82        17\n",
      "        high       0.87      0.87      0.87        39\n",
      "\n",
      "    accuracy                           0.87        78\n",
      "   macro avg       0.87      0.87      0.87        78\n",
      "weighted avg       0.87      0.87      0.87        78\n",
      "\n"
     ]
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 500x300 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy: 0.8718\n",
      "F1 Score: 0.8718\n",
      "Recall Score: 0.8718\n"
     ]
    }
   ],
   "source": [
    "final_model = SimpleNN(\n",
    "    input_dim=X_train_tensor_iphone.shape[1],\n",
    "    units=32,\n",
    "    dropout=0.3,\n",
    "    n_layers=1,\n",
    "    num_classes=3\n",
    ")\n",
    "final_model.load_state_dict(torch.load(\"models/final_model_iphone.pth\"))\n",
    "\n",
    "final_model.eval()\n",
    "all_preds = []\n",
    "all_labels = []\n",
    "all_probs = []\n",
    "\n",
    "with torch.no_grad():\n",
    "    for xb, yb in test_loader_iphone:\n",
    "        preds = final_model(xb)\n",
    "        predicted_classes = preds.argmax(dim=1)\n",
    "        logits = final_model(xb)               # raw outputs (logits)\n",
    "        probs = torch.softmax(logits, dim=1)  # convert logits to probabilities\n",
    "        all_probs.append(probs.cpu().numpy())\n",
    "        all_preds.extend(predicted_classes.cpu().numpy())\n",
    "        all_labels.extend(yb.cpu().numpy())\n",
    "\n",
    "all_probs = np.vstack(all_probs)\n",
    "\n",
    "idx_to_label = {0: \"low\", 1: \"medium\", 2: \"high\"}\n",
    "tier_order = [\"low\", \"medium\", \"high\"]\n",
    "\n",
    "# Map numeric labels back to strings\n",
    "all_labels_str = [idx_to_label[idx] for idx in all_labels]\n",
    "all_preds_str = [idx_to_label[idx] for idx in all_preds]\n",
    "all_labels_bin = label_binarize(all_labels, classes=[0,1,2])\n",
    "print(classification_report(all_labels_str, all_preds_str, target_names=tier_order))\n",
    "\n",
    "accuracy = accuracy_score(all_labels_str, all_preds_str)\n",
    "f1 = f1_score(all_labels_str, all_preds_str, average=\"weighted\")\n",
    "recall = recall_score(all_labels_str, all_preds_str, average=\"weighted\")\n",
    "prec = precision_score(all_labels_str, all_preds_str, average=\"weighted\")\n",
    "roc_auc = roc_auc_score(all_labels_bin, all_probs)\n",
    "avg_prec = average_precision_score(all_labels_bin, all_probs)\n",
    "\n",
    "cm_iphone = confusion_matrix(all_labels_str, all_preds_str, labels=tier_order)\n",
    "fig, ax = plt.subplots(figsize=(5,3))\n",
    "disp_iphone = ConfusionMatrixDisplay(confusion_matrix=cm_iphone, display_labels=tier_order)\n",
    "disp_iphone.plot(ax=ax, cmap=\"Purples\", colorbar=False)\n",
    "disp_iphone.ax_.set_title('Confusion Matrix for NN - iPhones')\n",
    "\n",
    "textstr = f'Accuracy: {accuracy:.2f}\\nF1 Score: {f1:.2f}\\nRecall: {recall:.2f}\\nPrecision: {prec:.2f}\\nROC-AUC: {roc_auc:.2f}\\nAvg Prec: {avg_prec:.2f}'\n",
    "ax.text(1.05, 0.5, textstr, transform=ax.transAxes, fontsize=10, verticalalignment='center')\n",
    "plt.tight_layout()\n",
    "plt.savefig(\"images/5/cm_nn_iphone.png\", dpi=300, bbox_inches='tight')#, facecolor=\"#E7E7E7\")\n",
    "plt.show()\n",
    "\n",
    "print(f'Accuracy: {accuracy:.4f}')\n",
    "print(f'F1 Score: {f1:.4f}')\n",
    "print(f'Recall Score: {recall:.4f}')\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "bd9f0b49",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Underpriced Candidates:\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>title</th>\n",
       "      <th>price.value</th>\n",
       "      <th>seller.feedbackScore</th>\n",
       "      <th>days_listed</th>\n",
       "      <th>title_length</th>\n",
       "      <th>true_label</th>\n",
       "      <th>predicted_label</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "Empty DataFrame\n",
       "Columns: [title, price.value, seller.feedbackScore, days_listed, title_length, true_label, predicted_label]\n",
       "Index: []"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Overpriced Candidates:\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>title</th>\n",
       "      <th>price.value</th>\n",
       "      <th>seller.feedbackScore</th>\n",
       "      <th>days_listed</th>\n",
       "      <th>title_length</th>\n",
       "      <th>true_label</th>\n",
       "      <th>predicted_label</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "Empty DataFrame\n",
       "Columns: [title, price.value, seller.feedbackScore, days_listed, title_length, true_label, predicted_label]\n",
       "Index: []"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "labels_df = pd.DataFrame({\n",
    "    \"true_label\": all_labels_str,\n",
    "    \"predicted_label\": all_preds_str\n",
    "}, index=y_test_iphone.index)\n",
    "\n",
    "# Merge labels with original iPhone dataframe on index\n",
    "merged = df_iphone.loc[y_test_iphone.index].copy()\n",
    "merged = merged.join(labels_df)\n",
    "\n",
    "# Identify underpriced and overpriced cases based on true vs predicted\n",
    "underpriced = merged[(merged[\"true_label\"] == \"low\") & (merged[\"predicted_label\"] == \"high\")]\n",
    "overpriced = merged[(merged[\"true_label\"] == \"high\") & (merged[\"predicted_label\"] == \"low\")]\n",
    "\n",
    "print(\"Underpriced Candidates:\")\n",
    "display(underpriced[[\"title\", \"price.value\", \"seller.feedbackScore\", \"days_listed\", \"title_length\", \"true_label\", \"predicted_label\"]])\n",
    "\n",
    "print(\"Overpriced Candidates:\")\n",
    "display(overpriced[[\"title\", \"price.value\", \"seller.feedbackScore\", \"days_listed\", \"title_length\", \"true_label\", \"predicted_label\"]])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "9ce98d93",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "net.0.weight: shape torch.Size([32, 42])\n",
      "tensor([[ 3.7532e-02, -4.7558e-02,  1.6302e-01,  ...,  1.5895e-01,\n",
      "         -4.5745e-01,  1.9295e-01],\n",
      "        [-7.7137e-03,  9.3463e-02,  1.9352e-02,  ..., -2.3568e-01,\n",
      "          2.1308e-01,  1.3299e-01],\n",
      "        [-9.7914e-02,  5.6846e-02, -3.3914e-02,  ...,  3.5116e-01,\n",
      "         -4.7371e-01,  3.8527e-01],\n",
      "        ...,\n",
      "        [ 1.6822e-02,  1.1287e-01,  1.4975e-02,  ..., -2.3990e-01,\n",
      "          1.9548e-01,  1.6425e-01],\n",
      "        [-2.8147e-03,  5.0253e-02,  3.2259e-02,  ..., -2.5435e-01,\n",
      "          4.1138e-02,  1.2131e-01],\n",
      "        [-8.8828e-02,  9.4076e-03, -3.0626e-04,  ...,  2.9607e-01,\n",
      "         -2.0641e-01,  1.0014e-01]])\n",
      "net.0.bias: shape torch.Size([32])\n",
      "tensor([ 0.0597, -0.1808,  0.0673,  0.0689, -0.0526,  0.0423,  0.0821, -0.0552,\n",
      "        -0.1163,  0.0559,  0.1538, -0.0656,  0.1005, -0.1164,  0.0992,  0.0233,\n",
      "        -0.0630, -0.0878, -0.0095, -0.1222, -0.0083,  0.0415,  0.0974, -0.1665,\n",
      "        -0.0129, -0.1068,  0.0629,  0.1503, -0.1506, -0.0910,  0.1148, -0.1245])\n",
      "net.1.weight: shape torch.Size([32])\n",
      "tensor([1.1575, 1.4441, 1.4040, 1.2455, 1.6333, 1.4890, 1.2890, 1.3218, 1.5424,\n",
      "        1.6174, 1.4755, 1.6014, 1.3076, 1.1979, 1.5036, 1.8908, 1.7929, 1.3158,\n",
      "        1.4566, 1.2140, 1.2350, 1.3200, 1.5277, 1.3730, 1.3833, 1.2827, 1.4070,\n",
      "        1.4211, 1.8328, 1.3134, 1.3195, 1.4790])\n",
      "net.1.bias: shape torch.Size([32])\n",
      "tensor([-0.2231, -0.0698,  0.0126,  0.0034,  0.1952,  0.0915, -0.1695, -0.1984,\n",
      "         0.1425,  0.0117,  0.0382,  0.1006, -0.2070,  0.1647,  0.0014,  0.2057,\n",
      "         0.1625, -0.2887,  0.1929, -0.2151,  0.1569, -0.1472,  0.1073,  0.0399,\n",
      "        -0.0061, -0.1963, -0.2974,  0.1023,  0.3819, -0.0071,  0.0421,  0.0108])\n",
      "net.4.weight: shape torch.Size([3, 32])\n",
      "tensor([[-0.0379,  0.2918, -0.2981, -0.0257,  0.3754, -0.7036, -0.3214, -0.2287,\n",
      "          0.2952,  0.4593,  0.1535,  0.1532, -0.2266,  0.1076,  0.2012, -0.6021,\n",
      "         -0.5288, -0.2425, -0.5423, -0.2035,  0.0734,  0.3122,  0.4097, -0.6456,\n",
      "          0.2132, -0.3122, -0.4026,  0.2574, -0.5954,  0.3483,  0.3674, -0.3985],\n",
      "        [-0.2234, -0.0892,  0.0132,  0.2448,  0.1267, -0.0257, -0.2426,  0.2394,\n",
      "          0.2347, -0.1468,  0.1547,  0.1275,  0.3268,  0.1040,  0.1533,  0.2440,\n",
      "          0.1543,  0.3700,  0.1556, -0.2789,  0.1479, -0.3703,  0.0364,  0.0141,\n",
      "         -0.3694,  0.2775,  0.3719, -0.0596,  0.2432, -0.1092, -0.1136,  0.3042],\n",
      "        [ 0.3897, -0.3445,  0.3779, -0.2245, -0.5376,  0.2374,  0.4406, -0.3188,\n",
      "         -0.5901, -0.3670, -0.5014, -0.5788, -0.2168, -0.3970, -0.5391,  0.3038,\n",
      "          0.3847, -0.0594,  0.3430,  0.3617, -0.3222,  0.1084, -0.5428,  0.3184,\n",
      "         -0.1549, -0.0219, -0.2323, -0.3467,  0.3965, -0.3278, -0.4766,  0.1988]])\n",
      "net.4.bias: shape torch.Size([3])\n",
      "tensor([-0.0293,  0.0109,  0.0024])\n"
     ]
    }
   ],
   "source": [
    "for name, param in final_model.named_parameters():\n",
    "    if param.requires_grad:\n",
    "        print(f\"{name}: shape {param.shape}\")\n",
    "        print(param.data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "21551431",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "SimpleNN(\n",
      "  (net): Sequential(\n",
      "    (0): Linear(in_features=42, out_features=32, bias=True)\n",
      "    (1): BatchNorm1d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "    (2): ReLU()\n",
      "    (3): Dropout(p=0.3, inplace=False)\n",
      "    (4): Linear(in_features=32, out_features=3, bias=True)\n",
      "  )\n",
      ")\n"
     ]
    }
   ],
   "source": [
    "print(final_model)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "eabe1ff6",
   "metadata": {},
   "source": [
    "#### Regression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "d6f7b3e4",
   "metadata": {},
   "outputs": [],
   "source": [
    "y_iphone = df_iphone['price.value']\n",
    "                         \n",
    "X_train_iphone, X_test_iphone, y_train_iphone, y_test_iphone = train_test_split(X_encoded_iphone, y_iphone,\n",
    "                                                                 test_size=0.20, random_state=1216) \n",
    "\n",
    "X_train_iphone, X_val_iphone, y_train_iphone, y_val_iphone = train_test_split(X_train_iphone, y_train_iphone,\n",
    "                                                                test_size=0.20, random_state=1216)\n",
    "\n",
    "y_train_tensor_iphone = torch.tensor(y_train_iphone.values, dtype=torch.float32).view(-1, 1)\n",
    "y_val_tensor_iphone = torch.tensor(y_val_iphone.values, dtype=torch.float32).view(-1, 1)\n",
    "y_test_tensor_iphone = torch.tensor(y_test_iphone.values, dtype=torch.float32).view(-1, 1)\n",
    "\n",
    "train_dataset_iphone = TensorDataset(X_train_tensor_iphone, y_train_tensor_iphone)\n",
    "val_dataset_iphone = TensorDataset(X_val_tensor_iphone, y_val_tensor_iphone)\n",
    "test_dataset_iphone = TensorDataset(X_test_tensor_iphone, y_test_tensor_iphone)\n",
    "\n",
    "train_loader_iphone = DataLoader(train_dataset_iphone, batch_size=32, shuffle=True)\n",
    "val_loader_iphone = DataLoader(val_dataset_iphone, batch_size=32, shuffle=False)\n",
    "test_loader_iphone = DataLoader(test_dataset_iphone, batch_size=32, shuffle=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "e8a7cd62",
   "metadata": {},
   "outputs": [],
   "source": [
    "class SimpleNN(nn.Module):\n",
    "    def __init__(self, input_dim, units, dropout, n_layers):\n",
    "        super(SimpleNN, self).__init__()\n",
    "        layers = []\n",
    "        # First layer\n",
    "        layers.append(nn.Linear(input_dim, units))\n",
    "        layers.append(nn.BatchNorm1d(units)) \n",
    "        layers.append(nn.ReLU())\n",
    "        layers.append(nn.Dropout(dropout))\n",
    "        # Hidden layers\n",
    "        for _ in range(n_layers - 1):\n",
    "            layers.append(nn.Linear(units, units))\n",
    "            layers.append(nn.BatchNorm1d(units)) \n",
    "            layers.append(nn.ReLU())\n",
    "            layers.append(nn.Dropout(dropout))\n",
    "        layers.append(nn.Linear(units, 1))  \n",
    "        self.net = nn.Sequential(*layers)\n",
    "\n",
    "    def forward(self, x):\n",
    "        return self.net(x)\n",
    "    \n",
    "def train_and_evaluate(units, dropout, lr, n_layers, epochs=10):\n",
    "    model = SimpleNN(input_dim=X_train_tensor_iphone.shape[1], units=units, dropout=dropout, n_layers=n_layers)\n",
    "    criterion = torch.nn.MSELoss()\n",
    "    optimizer = torch.optim.AdamW(model.parameters(), lr=lr)\n",
    "\n",
    "    for epoch in range(epochs):\n",
    "        model.train()\n",
    "        for xb, yb in train_loader_iphone:\n",
    "            optimizer.zero_grad()\n",
    "            preds = model(xb)\n",
    "            loss = criterion(preds, yb)\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "\n",
    "        # Validation per epoch\n",
    "        model.eval()\n",
    "        val_loss = 0\n",
    "        val_total = 0\n",
    "        with torch.no_grad():\n",
    "            for xb, yb in val_loader_iphone:\n",
    "                preds = model(xb)\n",
    "                loss = criterion(preds, yb)\n",
    "                val_loss += loss.item() * xb.size(0)\n",
    "                val_total += yb.size(0)\n",
    "        val_loss /= val_total\n",
    "        print(f\"Epoch {epoch+1}, Val Loss: {val_loss:.4f}\")\n",
    "    return val_loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "9bb93da7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# from itertools import product\n",
    "# param_grid = {\n",
    "#     'units': [32, 64],\n",
    "#     'dropout': [0.3, 0.4, 0.5],\n",
    "#     'learning_rate': [0.001, 0.005, 0.01],\n",
    "#     'n_layers': [1, 2, 3]\n",
    "# }\n",
    "# results = []\n",
    "# for units, dropout, lr, n_layers in product(param_grid['units'], param_grid['dropout'], param_grid['learning_rate'],\n",
    "#                                  param_grid['n_layers']):\n",
    "#     val_loss = train_and_evaluate(units, dropout, lr, n_layers, 15)\n",
    "#     print(f\"units={units}, dropout={dropout}, lr={lr}, n_layers={n_layers}, val_loss={val_loss:.4f}\")\n",
    "#     results.append({'units': units, 'dropout': dropout, 'lr': lr, 'n_layers': n_layers, 'val_loss': val_loss})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "26d504d6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# best = min(results, key=lambda x: x['val_loss'])\n",
    "# print(f\"Best hyperparams: {best}\")\n",
    "# best_params = {k: best[k] for k in ['units', 'dropout', 'lr', 'n_layers']}"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "449bdfcb",
   "metadata": {},
   "source": [
    "Best hyperparams: {'units': 64, 'dropout': 0.3, 'lr': 0.01, 'n_layers': 3, 'val_loss': 149725.18800403227}\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3a190886",
   "metadata": {},
   "outputs": [],
   "source": [
    "# X_combined_iphone = torch.cat([X_train_tensor_iphone, X_val_tensor_iphone], dim=0)\n",
    "# y_combined_iphone = torch.cat([y_train_tensor_iphone, y_val_tensor_iphone], dim=0)\n",
    "# combined_dataset_iphone = TensorDataset(X_combined_iphone, y_combined_iphone)\n",
    "# combined_loader_iphone = DataLoader(combined_dataset_iphone, batch_size=32, shuffle=True)\n",
    "\n",
    "# # Create new model using best hyperparameters\n",
    "# final_model = SimpleNN(\n",
    "#     input_dim=X_train_tensor_iphone.shape[1],\n",
    "#     units=64,\n",
    "#     dropout=0.3,\n",
    "#     n_layers=3,\n",
    "# )\n",
    "# optimizer = torch.optim.AdamW(final_model.parameters(), lr=0.01)\n",
    "# criterion = torch.nn.MSELoss()\n",
    "\n",
    "# train_losses = []\n",
    "\n",
    "# for epoch in range(50):\n",
    "#     final_model.train()\n",
    "#     running_loss = 0\n",
    "#     total = 0\n",
    "#     for xb, yb in combined_loader_iphone:\n",
    "#         optimizer.zero_grad()\n",
    "#         preds = final_model(xb)  # shape: [batch_size, num_classes]\n",
    "#         loss = criterion(preds, yb)\n",
    "#         loss.backward()\n",
    "#         optimizer.step()\n",
    "#         running_loss += loss.item() * xb.size(0)\n",
    "#         total += yb.size(0)\n",
    "#     epoch_loss = running_loss / total\n",
    "#     train_losses.append(epoch_loss)\n",
    "#     print(f\"Epoch {epoch+1}, Loss: {epoch_loss:.4f}\")\n",
    "\n",
    "# torch.save(final_model.state_dict(), \"models/final_model_iphone_reg.pth\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fa0c655d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test MSE: 8926.94, RMSE: 94.48\n"
     ]
    }
   ],
   "source": [
    "final_model = SimpleNN(\n",
    "    input_dim=X_train_tensor_iphone.shape[1],\n",
    "    units=64,\n",
    "    dropout=0.3,\n",
    "    n_layers=3,\n",
    ")\n",
    "final_model.load_state_dict(torch.load(\"models/final_model_iphone_reg.pth\"))\n",
    "criterion = torch.nn.MSELoss()\n",
    "\n",
    "final_model.eval()\n",
    "with torch.no_grad():\n",
    "    preds = final_model(X_test_tensor_iphone)\n",
    "    mse = criterion(preds, y_test_tensor_iphone).item()\n",
    "    rmse = mse ** 0.5\n",
    "    print(f\"Test MSE: {mse:.2f}, RMSE: {rmse:.2f}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "b46c1cbc",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "SimpleNN(\n",
      "  (net): Sequential(\n",
      "    (0): Linear(in_features=42, out_features=64, bias=True)\n",
      "    (1): BatchNorm1d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "    (2): ReLU()\n",
      "    (3): Dropout(p=0.3, inplace=False)\n",
      "    (4): Linear(in_features=64, out_features=64, bias=True)\n",
      "    (5): BatchNorm1d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "    (6): ReLU()\n",
      "    (7): Dropout(p=0.3, inplace=False)\n",
      "    (8): Linear(in_features=64, out_features=64, bias=True)\n",
      "    (9): BatchNorm1d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "    (10): ReLU()\n",
      "    (11): Dropout(p=0.3, inplace=False)\n",
      "    (12): Linear(in_features=64, out_features=1, bias=True)\n",
      "  )\n",
      ")\n"
     ]
    }
   ],
   "source": [
    "print(final_model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f07dbc09",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Top 10 biggest misses:\n",
      "                                                 title  model_number storage  \\\n",
      "18   Apple iPhone 16 Pro - 256 GB - Space Black - AT&T          16.0   256GB   \n",
      "19              Apple iPhone 16 Pro Max 256GB Unlocked          16.0   256GB   \n",
      "149  Apple iPhone 16 6.1\" REAL Dual SIM Hong Kong A...          16.0   128GB   \n",
      "201  Apple iPhone 16 Pro Max - Unlocked - Natural T...          16.0   512GB   \n",
      "1    Apple iPhone 16 Plus A3082 - AT&T Only - 128GB...          16.0   128GB   \n",
      "57   Apple iPhone 16 Pro A3083 MYMG3LL/A 256GB  AT&...          16.0   256GB   \n",
      "190  NEW*  Apple iPhone 16 | 256GB Ultramarine Blu...          16.0   256GB   \n",
      "229              Apple iPhone 14 - 128 GB 5G Black ATT          14.0   128GB   \n",
      "285  Apple iPhone 16 - Unlocked - Teal - 128GB - Ex...          16.0   128GB   \n",
      "148  NEW! Apple iPhone 14 6.1\"  100%  Battery  Fac...          14.0   512GB   \n",
      "\n",
      "    model_variant                 condition    Predicted   Actual       Error  \\\n",
      "18            Pro                      Used   913.040833   500.00  413.040833   \n",
      "19        Pro Max                       New  1108.308350   750.00  358.308350   \n",
      "149          none                       New   883.678284  1098.00 -214.321716   \n",
      "201       Pro Max                       New  1201.513184  1379.00 -177.486816   \n",
      "1            Plus                      Used   731.251465   567.99  163.261465   \n",
      "57            Pro  For parts or not working   642.890137   800.00 -157.109863   \n",
      "190          none                       New   704.295471   849.99 -145.694529   \n",
      "229          none                      Used   358.860779   215.00  143.860779   \n",
      "285          none                      Used   623.701111   765.00 -141.298889   \n",
      "148          none                  Open box   383.509247   519.00 -135.490753   \n",
      "\n",
      "                   Price_Tag  \n",
      "18   Potentially Underpriced  \n",
      "19   Potentially Underpriced  \n",
      "149   Potentially Overpriced  \n",
      "201   Potentially Overpriced  \n",
      "1    Potentially Underpriced  \n",
      "57    Potentially Overpriced  \n",
      "190   Potentially Overpriced  \n",
      "229  Potentially Underpriced  \n",
      "285   Potentially Overpriced  \n",
      "148   Potentially Overpriced  \n"
     ]
    }
   ],
   "source": [
    "final_model = SimpleNN(\n",
    "    input_dim=X_train_tensor_iphone.shape[1],\n",
    "    units=64,\n",
    "    dropout=0.3,\n",
    "    n_layers=3,\n",
    ")\n",
    "final_model.load_state_dict(torch.load(\"models/final_model_iphone_reg.pth\"))\n",
    "\n",
    "final_model.eval()\n",
    "with torch.no_grad():\n",
    "    preds = final_model(X_test_tensor_iphone).cpu().numpy().flatten()\n",
    "    targets = y_test_iphone.values.flatten()\n",
    "\n",
    "errors = preds - targets   # positive = underpriced, negative = overpriced\n",
    "abs_errors = np.abs(errors)\n",
    "\n",
    "top_n = 10\n",
    "biggest_misses_pos = np.argsort(abs_errors)[-top_n:][::-1]  # positions in test set\n",
    "orig_idx = X_test_iphone.index[biggest_misses_pos]           # original df_iphone index\n",
    "\n",
    "\n",
    "misses_df = df_iphone.loc[orig_idx, [\"title\", \"model_number\", \"storage\", \"model_variant\", \"condition\"]].copy()\n",
    "misses_df[\"Predicted\"] = preds[biggest_misses_pos]\n",
    "misses_df[\"Actual\"] = targets[biggest_misses_pos]\n",
    "misses_df[\"Error\"] = errors[biggest_misses_pos]\n",
    "misses_df[\"Price_Tag\"] = np.where(\n",
    "    misses_df[\"Error\"] > 0,\n",
    "    \"Potentially Underpriced\",\n",
    "    \"Potentially Overpriced\"\n",
    ")\n",
    "\n",
    "print(f\"Top {top_n} biggest misses:\")\n",
    "print(misses_df.sort_values(by=\"Error\", key=np.abs, ascending=False))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "56b0baa5",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>title</th>\n",
       "      <th>model_number</th>\n",
       "      <th>storage</th>\n",
       "      <th>model_variant</th>\n",
       "      <th>condition</th>\n",
       "      <th>Predicted</th>\n",
       "      <th>Actual</th>\n",
       "      <th>Error</th>\n",
       "      <th>Price_Tag</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>18</th>\n",
       "      <td>Apple iPhone 16 Pro - 256 GB - Space Black - AT&amp;T</td>\n",
       "      <td>16.0</td>\n",
       "      <td>256GB</td>\n",
       "      <td>Pro</td>\n",
       "      <td>Used</td>\n",
       "      <td>913.040833</td>\n",
       "      <td>500.00</td>\n",
       "      <td>413.040833</td>\n",
       "      <td>Potentially Underpriced</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19</th>\n",
       "      <td>Apple iPhone 16 Pro Max 256GB Unlocked</td>\n",
       "      <td>16.0</td>\n",
       "      <td>256GB</td>\n",
       "      <td>Pro Max</td>\n",
       "      <td>New</td>\n",
       "      <td>1108.308350</td>\n",
       "      <td>750.00</td>\n",
       "      <td>358.308350</td>\n",
       "      <td>Potentially Underpriced</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>149</th>\n",
       "      <td>Apple iPhone 16 6.1\" REAL Dual SIM Hong Kong A...</td>\n",
       "      <td>16.0</td>\n",
       "      <td>128GB</td>\n",
       "      <td>none</td>\n",
       "      <td>New</td>\n",
       "      <td>883.678284</td>\n",
       "      <td>1098.00</td>\n",
       "      <td>-214.321716</td>\n",
       "      <td>Potentially Overpriced</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>201</th>\n",
       "      <td>Apple iPhone 16 Pro Max - Unlocked - Natural T...</td>\n",
       "      <td>16.0</td>\n",
       "      <td>512GB</td>\n",
       "      <td>Pro Max</td>\n",
       "      <td>New</td>\n",
       "      <td>1201.513184</td>\n",
       "      <td>1379.00</td>\n",
       "      <td>-177.486816</td>\n",
       "      <td>Potentially Overpriced</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Apple iPhone 16 Plus A3082 - AT&amp;T Only - 128GB...</td>\n",
       "      <td>16.0</td>\n",
       "      <td>128GB</td>\n",
       "      <td>Plus</td>\n",
       "      <td>Used</td>\n",
       "      <td>731.251465</td>\n",
       "      <td>567.99</td>\n",
       "      <td>163.261465</td>\n",
       "      <td>Potentially Underpriced</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>57</th>\n",
       "      <td>Apple iPhone 16 Pro A3083 MYMG3LL/A 256GB  AT&amp;...</td>\n",
       "      <td>16.0</td>\n",
       "      <td>256GB</td>\n",
       "      <td>Pro</td>\n",
       "      <td>For parts or not working</td>\n",
       "      <td>642.890137</td>\n",
       "      <td>800.00</td>\n",
       "      <td>-157.109863</td>\n",
       "      <td>Potentially Overpriced</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>190</th>\n",
       "      <td>NEW*  Apple iPhone 16 | 256GB Ultramarine Blu...</td>\n",
       "      <td>16.0</td>\n",
       "      <td>256GB</td>\n",
       "      <td>none</td>\n",
       "      <td>New</td>\n",
       "      <td>704.295471</td>\n",
       "      <td>849.99</td>\n",
       "      <td>-145.694529</td>\n",
       "      <td>Potentially Overpriced</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>229</th>\n",
       "      <td>Apple iPhone 14 - 128 GB 5G Black ATT</td>\n",
       "      <td>14.0</td>\n",
       "      <td>128GB</td>\n",
       "      <td>none</td>\n",
       "      <td>Used</td>\n",
       "      <td>358.860779</td>\n",
       "      <td>215.00</td>\n",
       "      <td>143.860779</td>\n",
       "      <td>Potentially Underpriced</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>285</th>\n",
       "      <td>Apple iPhone 16 - Unlocked - Teal - 128GB - Ex...</td>\n",
       "      <td>16.0</td>\n",
       "      <td>128GB</td>\n",
       "      <td>none</td>\n",
       "      <td>Used</td>\n",
       "      <td>623.701111</td>\n",
       "      <td>765.00</td>\n",
       "      <td>-141.298889</td>\n",
       "      <td>Potentially Overpriced</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>148</th>\n",
       "      <td>NEW! Apple iPhone 14 6.1\"  100%  Battery  Fac...</td>\n",
       "      <td>14.0</td>\n",
       "      <td>512GB</td>\n",
       "      <td>none</td>\n",
       "      <td>Open box</td>\n",
       "      <td>383.509247</td>\n",
       "      <td>519.00</td>\n",
       "      <td>-135.490753</td>\n",
       "      <td>Potentially Overpriced</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                 title  model_number storage  \\\n",
       "18   Apple iPhone 16 Pro - 256 GB - Space Black - AT&T          16.0   256GB   \n",
       "19              Apple iPhone 16 Pro Max 256GB Unlocked          16.0   256GB   \n",
       "149  Apple iPhone 16 6.1\" REAL Dual SIM Hong Kong A...          16.0   128GB   \n",
       "201  Apple iPhone 16 Pro Max - Unlocked - Natural T...          16.0   512GB   \n",
       "1    Apple iPhone 16 Plus A3082 - AT&T Only - 128GB...          16.0   128GB   \n",
       "57   Apple iPhone 16 Pro A3083 MYMG3LL/A 256GB  AT&...          16.0   256GB   \n",
       "190  NEW*  Apple iPhone 16 | 256GB Ultramarine Blu...          16.0   256GB   \n",
       "229              Apple iPhone 14 - 128 GB 5G Black ATT          14.0   128GB   \n",
       "285  Apple iPhone 16 - Unlocked - Teal - 128GB - Ex...          16.0   128GB   \n",
       "148  NEW! Apple iPhone 14 6.1\"  100%  Battery  Fac...          14.0   512GB   \n",
       "\n",
       "    model_variant                 condition    Predicted   Actual       Error  \\\n",
       "18            Pro                      Used   913.040833   500.00  413.040833   \n",
       "19        Pro Max                       New  1108.308350   750.00  358.308350   \n",
       "149          none                       New   883.678284  1098.00 -214.321716   \n",
       "201       Pro Max                       New  1201.513184  1379.00 -177.486816   \n",
       "1            Plus                      Used   731.251465   567.99  163.261465   \n",
       "57            Pro  For parts or not working   642.890137   800.00 -157.109863   \n",
       "190          none                       New   704.295471   849.99 -145.694529   \n",
       "229          none                      Used   358.860779   215.00  143.860779   \n",
       "285          none                      Used   623.701111   765.00 -141.298889   \n",
       "148          none                  Open box   383.509247   519.00 -135.490753   \n",
       "\n",
       "                   Price_Tag  \n",
       "18   Potentially Underpriced  \n",
       "19   Potentially Underpriced  \n",
       "149   Potentially Overpriced  \n",
       "201   Potentially Overpriced  \n",
       "1    Potentially Underpriced  \n",
       "57    Potentially Overpriced  \n",
       "190   Potentially Overpriced  \n",
       "229  Potentially Underpriced  \n",
       "285   Potentially Overpriced  \n",
       "148   Potentially Overpriced  "
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "misses_df.sort_values(by=\"Error\", key=np.abs, ascending=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4d3f6229",
   "metadata": {},
   "source": [
    "#### Log price regression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "a9d79412",
   "metadata": {},
   "outputs": [],
   "source": [
    "y_iphone = df_iphone['price.value']\n",
    "                         \n",
    "X_train_iphone, X_test_iphone, y_train_iphone, y_test_iphone = train_test_split(X_encoded_iphone, y_iphone,\n",
    "                                                                 test_size=0.20, random_state=1216) \n",
    "\n",
    "X_train_iphone, X_val_iphone, y_train_iphone, y_val_iphone = train_test_split(X_train_iphone, y_train_iphone,\n",
    "                                                                test_size=0.20, random_state=1216)\n",
    "\n",
    "y_train_log = np.log(y_train_iphone.values)\n",
    "y_val_log = np.log(y_val_iphone.values)\n",
    "y_test_log = np.log(y_test_iphone.values)\n",
    "\n",
    "y_train_tensor_iphone_log = torch.tensor(y_train_log, dtype=torch.float32).view(-1, 1)\n",
    "y_val_tensor_iphone_log = torch.tensor(y_val_log, dtype=torch.float32).view(-1, 1)\n",
    "y_test_tensor_iphone_log = torch.tensor(y_test_log, dtype=torch.float32).view(-1, 1)\n",
    "\n",
    "train_dataset_iphone_log = TensorDataset(X_train_tensor_iphone, y_train_tensor_iphone_log)\n",
    "val_dataset_iphone_log = TensorDataset(X_val_tensor_iphone, y_val_tensor_iphone_log)\n",
    "test_dataset_iphone_log = TensorDataset(X_test_tensor_iphone, y_test_tensor_iphone_log)\n",
    "\n",
    "train_loader_iphone_log = DataLoader(train_dataset_iphone_log, batch_size=32, shuffle=True)\n",
    "val_loader_iphone_log = DataLoader(val_dataset_iphone_log, batch_size=32, shuffle=False)\n",
    "test_loader_iphone_log = DataLoader(test_dataset_iphone_log, batch_size=32, shuffle=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "09a5d7ae",
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.optim.lr_scheduler import ReduceLROnPlateau\n",
    "\n",
    "def train_and_evaluate(units, dropout, lr, n_layers, epochs=30, patience=5):\n",
    "    model = SimpleNN(input_dim=X_train_tensor_iphone.shape[1], units=units, dropout=dropout, n_layers=n_layers)\n",
    "    criterion = torch.nn.MSELoss()\n",
    "    optimizer = torch.optim.AdamW(model.parameters(), lr=lr)\n",
    "    scheduler = ReduceLROnPlateau(optimizer, 'min', patience=2, factor=0.5)\n",
    "    \n",
    "    best_val_loss = float('inf')\n",
    "    epochs_no_improve = 0\n",
    "\n",
    "    for epoch in range(epochs):\n",
    "        model.train()\n",
    "        for xb, yb in train_loader_iphone_log:\n",
    "            optimizer.zero_grad()\n",
    "            preds = model(xb)\n",
    "            loss = criterion(preds, yb)\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "\n",
    "        # Validation\n",
    "        model.eval()\n",
    "        val_loss = 0\n",
    "        val_total = 0\n",
    "        with torch.no_grad():\n",
    "            for xb, yb in val_loader_iphone_log:\n",
    "                preds = model(xb)\n",
    "                loss = criterion(preds, yb)\n",
    "                val_loss += loss.item() * xb.size(0)\n",
    "                val_total += yb.size(0)\n",
    "        val_loss /= val_total\n",
    "        print(f\"Epoch {epoch+1}, Val Loss: {val_loss:.4f}\")\n",
    "\n",
    "        scheduler.step(val_loss)\n",
    "\n",
    "        # Early stopping\n",
    "        if val_loss < best_val_loss:\n",
    "            best_val_loss = val_loss\n",
    "            epochs_no_improve = 0\n",
    "        else:\n",
    "            epochs_no_improve += 1\n",
    "            if epochs_no_improve >= patience:\n",
    "                print(\"Early stopping!\")\n",
    "                break\n",
    "    return best_val_loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "a7c6d5cf",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1, Val Loss: 39.3861\n",
      "Epoch 2, Val Loss: 39.2588\n",
      "Epoch 3, Val Loss: 38.7927\n",
      "Epoch 4, Val Loss: 38.0460\n",
      "Epoch 5, Val Loss: 37.2108\n",
      "Epoch 6, Val Loss: 36.3806\n",
      "Epoch 7, Val Loss: 35.6058\n",
      "Epoch 8, Val Loss: 34.7062\n",
      "Epoch 9, Val Loss: 33.9468\n",
      "Epoch 10, Val Loss: 33.2737\n",
      "units=32, dropout=0.3, lr=0.0005, n_layers=1, val_loss=33.2737\n",
      "Epoch 1, Val Loss: 38.5952\n",
      "Epoch 2, Val Loss: 37.6184\n",
      "Epoch 3, Val Loss: 36.1198\n",
      "Epoch 4, Val Loss: 34.8539\n",
      "Epoch 5, Val Loss: 34.0069\n",
      "Epoch 6, Val Loss: 32.9855\n",
      "Epoch 7, Val Loss: 32.1992\n",
      "Epoch 8, Val Loss: 31.4933\n",
      "Epoch 9, Val Loss: 30.7558\n",
      "Epoch 10, Val Loss: 30.1369\n",
      "units=32, dropout=0.3, lr=0.0005, n_layers=2, val_loss=30.1369\n",
      "Epoch 1, Val Loss: 37.5502\n",
      "Epoch 2, Val Loss: 36.2027\n",
      "Epoch 3, Val Loss: 35.2254\n",
      "Epoch 4, Val Loss: 34.3117\n",
      "Epoch 5, Val Loss: 33.9696\n",
      "Epoch 6, Val Loss: 33.4708\n",
      "Epoch 7, Val Loss: 32.9167\n",
      "Epoch 8, Val Loss: 32.3284\n",
      "Epoch 9, Val Loss: 31.7154\n",
      "Epoch 10, Val Loss: 30.8827\n",
      "units=32, dropout=0.3, lr=0.0005, n_layers=3, val_loss=30.8827\n",
      "Epoch 1, Val Loss: 35.3183\n",
      "Epoch 2, Val Loss: 33.3851\n",
      "Epoch 3, Val Loss: 31.1223\n",
      "Epoch 4, Val Loss: 28.8902\n",
      "Epoch 5, Val Loss: 26.8752\n",
      "Epoch 6, Val Loss: 25.1507\n",
      "Epoch 7, Val Loss: 23.6358\n",
      "Epoch 8, Val Loss: 22.1102\n",
      "Epoch 9, Val Loss: 20.8949\n",
      "Epoch 10, Val Loss: 19.5443\n",
      "units=32, dropout=0.3, lr=0.001, n_layers=1, val_loss=19.5443\n",
      "Epoch 1, Val Loss: 40.6950\n",
      "Epoch 2, Val Loss: 40.3968\n",
      "Epoch 3, Val Loss: 39.2511\n",
      "Epoch 4, Val Loss: 37.9536\n",
      "Epoch 5, Val Loss: 36.4567\n",
      "Epoch 6, Val Loss: 35.0152\n",
      "Epoch 7, Val Loss: 33.5749\n",
      "Epoch 8, Val Loss: 32.5053\n",
      "Epoch 9, Val Loss: 31.1482\n",
      "Epoch 10, Val Loss: 30.0103\n",
      "units=32, dropout=0.3, lr=0.001, n_layers=2, val_loss=30.0103\n",
      "Epoch 1, Val Loss: 39.3976\n",
      "Epoch 2, Val Loss: 39.1558\n",
      "Epoch 3, Val Loss: 38.9308\n",
      "Epoch 4, Val Loss: 38.7587\n",
      "Epoch 5, Val Loss: 38.1729\n",
      "Epoch 6, Val Loss: 37.0828\n",
      "Epoch 7, Val Loss: 36.1687\n",
      "Epoch 8, Val Loss: 35.1550\n",
      "Epoch 9, Val Loss: 34.5699\n",
      "Epoch 10, Val Loss: 33.9344\n",
      "units=32, dropout=0.3, lr=0.001, n_layers=3, val_loss=33.9344\n",
      "Epoch 1, Val Loss: 33.7750\n",
      "Epoch 2, Val Loss: 27.7461\n",
      "Epoch 3, Val Loss: 20.3345\n",
      "Epoch 4, Val Loss: 13.1738\n",
      "Epoch 5, Val Loss: 7.1163\n",
      "Epoch 6, Val Loss: 3.3959\n",
      "Epoch 7, Val Loss: 1.7053\n",
      "Epoch 8, Val Loss: 0.8751\n",
      "Epoch 9, Val Loss: 0.6921\n",
      "Epoch 10, Val Loss: 0.6896\n",
      "units=32, dropout=0.3, lr=0.005, n_layers=1, val_loss=0.6896\n",
      "Epoch 1, Val Loss: 38.2519\n",
      "Epoch 2, Val Loss: 33.8884\n",
      "Epoch 3, Val Loss: 28.2283\n",
      "Epoch 4, Val Loss: 21.9767\n",
      "Epoch 5, Val Loss: 16.4023\n",
      "Epoch 6, Val Loss: 11.4916\n",
      "Epoch 7, Val Loss: 6.8004\n",
      "Epoch 8, Val Loss: 3.8151\n",
      "Epoch 9, Val Loss: 1.6895\n",
      "Epoch 10, Val Loss: 1.0970\n",
      "units=32, dropout=0.3, lr=0.005, n_layers=2, val_loss=1.0970\n",
      "Epoch 1, Val Loss: 37.2567\n",
      "Epoch 2, Val Loss: 32.5905\n",
      "Epoch 3, Val Loss: 25.9037\n",
      "Epoch 4, Val Loss: 19.0953\n",
      "Epoch 5, Val Loss: 13.4592\n",
      "Epoch 6, Val Loss: 8.4815\n",
      "Epoch 7, Val Loss: 5.3632\n",
      "Epoch 8, Val Loss: 3.2467\n",
      "Epoch 9, Val Loss: 1.5792\n",
      "Epoch 10, Val Loss: 1.3233\n",
      "units=32, dropout=0.3, lr=0.005, n_layers=3, val_loss=1.3233\n",
      "Epoch 1, Val Loss: 26.6582\n",
      "Epoch 2, Val Loss: 12.7057\n",
      "Epoch 3, Val Loss: 2.3983\n",
      "Epoch 4, Val Loss: 0.6543\n",
      "Epoch 5, Val Loss: 1.0276\n",
      "Epoch 6, Val Loss: 0.9957\n",
      "Epoch 7, Val Loss: 0.8680\n",
      "Epoch 8, Val Loss: 1.3206\n",
      "Epoch 9, Val Loss: 1.2104\n",
      "Early stopping!\n",
      "units=32, dropout=0.3, lr=0.01, n_layers=1, val_loss=0.6543\n",
      "Epoch 1, Val Loss: 33.2992\n",
      "Epoch 2, Val Loss: 25.0807\n",
      "Epoch 3, Val Loss: 14.6468\n",
      "Epoch 4, Val Loss: 8.2932\n",
      "Epoch 5, Val Loss: 3.7681\n",
      "Epoch 6, Val Loss: 1.1095\n",
      "Epoch 7, Val Loss: 0.9183\n",
      "Epoch 8, Val Loss: 1.2744\n",
      "Epoch 9, Val Loss: 1.8301\n",
      "Epoch 10, Val Loss: 1.7941\n",
      "units=32, dropout=0.3, lr=0.01, n_layers=2, val_loss=0.9183\n",
      "Epoch 1, Val Loss: 33.5716\n",
      "Epoch 2, Val Loss: 22.8884\n",
      "Epoch 3, Val Loss: 10.9979\n",
      "Epoch 4, Val Loss: 3.6143\n",
      "Epoch 5, Val Loss: 1.0975\n",
      "Epoch 6, Val Loss: 0.6461\n",
      "Epoch 7, Val Loss: 1.4857\n",
      "Epoch 8, Val Loss: 1.4411\n",
      "Epoch 9, Val Loss: 1.3349\n",
      "Epoch 10, Val Loss: 0.9401\n",
      "units=32, dropout=0.3, lr=0.01, n_layers=3, val_loss=0.6461\n",
      "Epoch 1, Val Loss: 37.5485\n",
      "Epoch 2, Val Loss: 36.3160\n",
      "Epoch 3, Val Loss: 34.9002\n",
      "Epoch 4, Val Loss: 33.5308\n",
      "Epoch 5, Val Loss: 32.3596\n",
      "Epoch 6, Val Loss: 31.5719\n",
      "Epoch 7, Val Loss: 30.6093\n",
      "Epoch 8, Val Loss: 29.8707\n",
      "Epoch 9, Val Loss: 29.1104\n",
      "Epoch 10, Val Loss: 28.4149\n",
      "units=32, dropout=0.4, lr=0.0005, n_layers=1, val_loss=28.4149\n",
      "Epoch 1, Val Loss: 39.2366\n",
      "Epoch 2, Val Loss: 39.2070\n",
      "Epoch 3, Val Loss: 38.7952\n",
      "Epoch 4, Val Loss: 38.1350\n",
      "Epoch 5, Val Loss: 37.7432\n",
      "Epoch 6, Val Loss: 36.8495\n",
      "Epoch 7, Val Loss: 36.2723\n",
      "Epoch 8, Val Loss: 35.8256\n",
      "Epoch 9, Val Loss: 35.1380\n",
      "Epoch 10, Val Loss: 34.3842\n",
      "units=32, dropout=0.4, lr=0.0005, n_layers=2, val_loss=34.3842\n",
      "Epoch 1, Val Loss: 38.8732\n",
      "Epoch 2, Val Loss: 38.1413\n",
      "Epoch 3, Val Loss: 37.2555\n",
      "Epoch 4, Val Loss: 36.1120\n",
      "Epoch 5, Val Loss: 35.4606\n",
      "Epoch 6, Val Loss: 34.9668\n",
      "Epoch 7, Val Loss: 34.3343\n",
      "Epoch 8, Val Loss: 33.7913\n",
      "Epoch 9, Val Loss: 33.6910\n",
      "Epoch 10, Val Loss: 32.8877\n",
      "units=32, dropout=0.4, lr=0.0005, n_layers=3, val_loss=32.8877\n",
      "Epoch 1, Val Loss: 36.6218\n",
      "Epoch 2, Val Loss: 35.2966\n",
      "Epoch 3, Val Loss: 33.5471\n",
      "Epoch 4, Val Loss: 31.4744\n",
      "Epoch 5, Val Loss: 29.7084\n",
      "Epoch 6, Val Loss: 27.9198\n",
      "Epoch 7, Val Loss: 26.5315\n",
      "Epoch 8, Val Loss: 25.1077\n",
      "Epoch 9, Val Loss: 23.7649\n",
      "Epoch 10, Val Loss: 22.4609\n",
      "units=32, dropout=0.4, lr=0.001, n_layers=1, val_loss=22.4609\n",
      "Epoch 1, Val Loss: 40.2182\n",
      "Epoch 2, Val Loss: 39.5500\n",
      "Epoch 3, Val Loss: 38.8452\n",
      "Epoch 4, Val Loss: 37.9751\n",
      "Epoch 5, Val Loss: 37.0184\n",
      "Epoch 6, Val Loss: 36.0143\n",
      "Epoch 7, Val Loss: 34.9845\n",
      "Epoch 8, Val Loss: 33.8175\n",
      "Epoch 9, Val Loss: 32.7475\n",
      "Epoch 10, Val Loss: 31.5287\n",
      "units=32, dropout=0.4, lr=0.001, n_layers=2, val_loss=31.5287\n",
      "Epoch 1, Val Loss: 40.2835\n",
      "Epoch 2, Val Loss: 39.4838\n",
      "Epoch 3, Val Loss: 38.6283\n",
      "Epoch 4, Val Loss: 37.6906\n",
      "Epoch 5, Val Loss: 36.4875\n",
      "Epoch 6, Val Loss: 35.3144\n",
      "Epoch 7, Val Loss: 34.2791\n",
      "Epoch 8, Val Loss: 33.5797\n",
      "Epoch 9, Val Loss: 32.4879\n",
      "Epoch 10, Val Loss: 31.2753\n",
      "units=32, dropout=0.4, lr=0.001, n_layers=3, val_loss=31.2753\n",
      "Epoch 1, Val Loss: 34.4698\n",
      "Epoch 2, Val Loss: 28.4188\n",
      "Epoch 3, Val Loss: 21.2486\n",
      "Epoch 4, Val Loss: 14.4879\n",
      "Epoch 5, Val Loss: 8.6952\n",
      "Epoch 6, Val Loss: 4.3226\n",
      "Epoch 7, Val Loss: 1.9261\n",
      "Epoch 8, Val Loss: 0.9686\n",
      "Epoch 9, Val Loss: 0.6994\n",
      "Epoch 10, Val Loss: 0.6103\n",
      "units=32, dropout=0.4, lr=0.005, n_layers=1, val_loss=0.6103\n",
      "Epoch 1, Val Loss: 37.1584\n",
      "Epoch 2, Val Loss: 33.2302\n",
      "Epoch 3, Val Loss: 27.3570\n",
      "Epoch 4, Val Loss: 21.3427\n",
      "Epoch 5, Val Loss: 16.1603\n",
      "Epoch 6, Val Loss: 12.1294\n",
      "Epoch 7, Val Loss: 8.1644\n",
      "Epoch 8, Val Loss: 5.0331\n",
      "Epoch 9, Val Loss: 3.5907\n",
      "Epoch 10, Val Loss: 2.9588\n",
      "units=32, dropout=0.4, lr=0.005, n_layers=2, val_loss=2.9588\n",
      "Epoch 1, Val Loss: 38.6757\n",
      "Epoch 2, Val Loss: 34.8690\n",
      "Epoch 3, Val Loss: 29.9378\n",
      "Epoch 4, Val Loss: 24.6367\n",
      "Epoch 5, Val Loss: 19.0669\n",
      "Epoch 6, Val Loss: 13.8593\n",
      "Epoch 7, Val Loss: 8.7247\n",
      "Epoch 8, Val Loss: 6.1902\n",
      "Epoch 9, Val Loss: 3.6931\n",
      "Epoch 10, Val Loss: 2.7926\n",
      "units=32, dropout=0.4, lr=0.005, n_layers=3, val_loss=2.7926\n",
      "Epoch 1, Val Loss: 31.9791\n",
      "Epoch 2, Val Loss: 20.7842\n",
      "Epoch 3, Val Loss: 9.6103\n",
      "Epoch 4, Val Loss: 2.3636\n",
      "Epoch 5, Val Loss: 0.7899\n",
      "Epoch 6, Val Loss: 0.9582\n",
      "Epoch 7, Val Loss: 0.8539\n",
      "Epoch 8, Val Loss: 0.8317\n",
      "Epoch 9, Val Loss: 0.9462\n",
      "Epoch 10, Val Loss: 0.9223\n",
      "Early stopping!\n",
      "units=32, dropout=0.4, lr=0.01, n_layers=1, val_loss=0.7899\n",
      "Epoch 1, Val Loss: 31.2656\n",
      "Epoch 2, Val Loss: 21.8221\n",
      "Epoch 3, Val Loss: 12.7365\n",
      "Epoch 4, Val Loss: 6.0158\n",
      "Epoch 5, Val Loss: 2.8517\n",
      "Epoch 6, Val Loss: 2.6379\n",
      "Epoch 7, Val Loss: 2.3442\n",
      "Epoch 8, Val Loss: 1.8242\n",
      "Epoch 9, Val Loss: 1.5900\n",
      "Epoch 10, Val Loss: 1.9202\n",
      "units=32, dropout=0.4, lr=0.01, n_layers=2, val_loss=1.5900\n",
      "Epoch 1, Val Loss: 31.5412\n",
      "Epoch 2, Val Loss: 20.0662\n",
      "Epoch 3, Val Loss: 8.8629\n",
      "Epoch 4, Val Loss: 2.7111\n",
      "Epoch 5, Val Loss: 1.4986\n",
      "Epoch 6, Val Loss: 1.9603\n",
      "Epoch 7, Val Loss: 1.5568\n",
      "Epoch 8, Val Loss: 1.7400\n",
      "Epoch 9, Val Loss: 1.5177\n",
      "Epoch 10, Val Loss: 1.5275\n",
      "Early stopping!\n",
      "units=32, dropout=0.4, lr=0.01, n_layers=3, val_loss=1.4986\n",
      "Epoch 1, Val Loss: 34.6029\n",
      "Epoch 2, Val Loss: 33.5966\n",
      "Epoch 3, Val Loss: 32.5562\n",
      "Epoch 4, Val Loss: 31.4307\n",
      "Epoch 5, Val Loss: 30.4958\n",
      "Epoch 6, Val Loss: 29.6099\n",
      "Epoch 7, Val Loss: 28.8830\n",
      "Epoch 8, Val Loss: 28.2529\n",
      "Epoch 9, Val Loss: 27.5181\n",
      "Epoch 10, Val Loss: 26.9558\n",
      "units=32, dropout=0.5, lr=0.0005, n_layers=1, val_loss=26.9558\n",
      "Epoch 1, Val Loss: 37.9974\n",
      "Epoch 2, Val Loss: 37.7350\n",
      "Epoch 3, Val Loss: 36.8592\n",
      "Epoch 4, Val Loss: 35.7684\n",
      "Epoch 5, Val Loss: 35.1818\n",
      "Epoch 6, Val Loss: 34.5556\n",
      "Epoch 7, Val Loss: 34.0475\n",
      "Epoch 8, Val Loss: 33.7917\n",
      "Epoch 9, Val Loss: 33.0645\n",
      "Epoch 10, Val Loss: 32.5296\n",
      "units=32, dropout=0.5, lr=0.0005, n_layers=2, val_loss=32.5296\n",
      "Epoch 1, Val Loss: 40.2866\n",
      "Epoch 2, Val Loss: 40.4572\n",
      "Epoch 3, Val Loss: 40.3740\n",
      "Epoch 4, Val Loss: 39.9665\n",
      "Epoch 5, Val Loss: 39.6254\n",
      "Epoch 6, Val Loss: 39.4568\n",
      "Epoch 7, Val Loss: 39.2161\n",
      "Epoch 8, Val Loss: 38.6363\n",
      "Epoch 9, Val Loss: 38.4878\n",
      "Epoch 10, Val Loss: 38.1401\n",
      "units=32, dropout=0.5, lr=0.0005, n_layers=3, val_loss=38.1401\n",
      "Epoch 1, Val Loss: 37.4302\n",
      "Epoch 2, Val Loss: 35.4088\n",
      "Epoch 3, Val Loss: 32.9996\n",
      "Epoch 4, Val Loss: 30.6196\n",
      "Epoch 5, Val Loss: 28.4722\n",
      "Epoch 6, Val Loss: 26.6778\n",
      "Epoch 7, Val Loss: 25.0104\n",
      "Epoch 8, Val Loss: 23.5734\n",
      "Epoch 9, Val Loss: 22.0709\n",
      "Epoch 10, Val Loss: 20.6857\n",
      "units=32, dropout=0.5, lr=0.001, n_layers=1, val_loss=20.6857\n",
      "Epoch 1, Val Loss: 37.6579\n",
      "Epoch 2, Val Loss: 36.4495\n",
      "Epoch 3, Val Loss: 34.9930\n",
      "Epoch 4, Val Loss: 33.3651\n",
      "Epoch 5, Val Loss: 31.4344\n",
      "Epoch 6, Val Loss: 30.2430\n",
      "Epoch 7, Val Loss: 29.2104\n",
      "Epoch 8, Val Loss: 27.9731\n",
      "Epoch 9, Val Loss: 26.9628\n",
      "Epoch 10, Val Loss: 26.1376\n",
      "units=32, dropout=0.5, lr=0.001, n_layers=2, val_loss=26.1376\n",
      "Epoch 1, Val Loss: 38.2743\n",
      "Epoch 2, Val Loss: 38.1105\n",
      "Epoch 3, Val Loss: 37.4829\n",
      "Epoch 4, Val Loss: 36.9177\n",
      "Epoch 5, Val Loss: 36.0437\n",
      "Epoch 6, Val Loss: 34.8391\n",
      "Epoch 7, Val Loss: 33.9510\n",
      "Epoch 8, Val Loss: 33.2533\n",
      "Epoch 9, Val Loss: 32.6165\n",
      "Epoch 10, Val Loss: 31.8674\n",
      "units=32, dropout=0.5, lr=0.001, n_layers=3, val_loss=31.8674\n",
      "Epoch 1, Val Loss: 35.9736\n",
      "Epoch 2, Val Loss: 30.6813\n",
      "Epoch 3, Val Loss: 24.1052\n",
      "Epoch 4, Val Loss: 17.0825\n",
      "Epoch 5, Val Loss: 10.8926\n",
      "Epoch 6, Val Loss: 6.0437\n",
      "Epoch 7, Val Loss: 3.1093\n",
      "Epoch 8, Val Loss: 1.4096\n",
      "Epoch 9, Val Loss: 0.9504\n",
      "Epoch 10, Val Loss: 0.7822\n",
      "units=32, dropout=0.5, lr=0.005, n_layers=1, val_loss=0.7822\n",
      "Epoch 1, Val Loss: 37.0062\n",
      "Epoch 2, Val Loss: 33.4841\n",
      "Epoch 3, Val Loss: 29.1948\n",
      "Epoch 4, Val Loss: 24.9890\n",
      "Epoch 5, Val Loss: 20.2150\n",
      "Epoch 6, Val Loss: 16.2495\n",
      "Epoch 7, Val Loss: 12.8606\n",
      "Epoch 8, Val Loss: 8.9867\n",
      "Epoch 9, Val Loss: 6.1483\n",
      "Epoch 10, Val Loss: 4.0214\n",
      "units=32, dropout=0.5, lr=0.005, n_layers=2, val_loss=4.0214\n",
      "Epoch 1, Val Loss: 34.6879\n",
      "Epoch 2, Val Loss: 29.9212\n",
      "Epoch 3, Val Loss: 24.0705\n",
      "Epoch 4, Val Loss: 18.7352\n",
      "Epoch 5, Val Loss: 13.7907\n",
      "Epoch 6, Val Loss: 9.3836\n",
      "Epoch 7, Val Loss: 5.1080\n",
      "Epoch 8, Val Loss: 3.5734\n",
      "Epoch 9, Val Loss: 2.5557\n",
      "Epoch 10, Val Loss: 2.1801\n",
      "units=32, dropout=0.5, lr=0.005, n_layers=3, val_loss=2.1801\n",
      "Epoch 1, Val Loss: 29.7748\n",
      "Epoch 2, Val Loss: 18.1509\n",
      "Epoch 3, Val Loss: 7.8331\n",
      "Epoch 4, Val Loss: 2.0521\n",
      "Epoch 5, Val Loss: 0.7372\n",
      "Epoch 6, Val Loss: 0.7870\n",
      "Epoch 7, Val Loss: 1.1014\n",
      "Epoch 8, Val Loss: 1.4394\n",
      "Epoch 9, Val Loss: 1.2135\n",
      "Epoch 10, Val Loss: 1.1391\n",
      "Early stopping!\n",
      "units=32, dropout=0.5, lr=0.01, n_layers=1, val_loss=0.7372\n",
      "Epoch 1, Val Loss: 37.5819\n",
      "Epoch 2, Val Loss: 30.6866\n",
      "Epoch 3, Val Loss: 21.0024\n",
      "Epoch 4, Val Loss: 12.4285\n",
      "Epoch 5, Val Loss: 7.7471\n",
      "Epoch 6, Val Loss: 3.6931\n",
      "Epoch 7, Val Loss: 2.3123\n",
      "Epoch 8, Val Loss: 3.2951\n",
      "Epoch 9, Val Loss: 3.7014\n",
      "Epoch 10, Val Loss: 3.6423\n",
      "units=32, dropout=0.5, lr=0.01, n_layers=2, val_loss=2.3123\n",
      "Epoch 1, Val Loss: 36.0003\n",
      "Epoch 2, Val Loss: 27.0198\n",
      "Epoch 3, Val Loss: 16.7228\n",
      "Epoch 4, Val Loss: 6.6899\n",
      "Epoch 5, Val Loss: 2.1120\n",
      "Epoch 6, Val Loss: 1.9663\n",
      "Epoch 7, Val Loss: 3.4261\n",
      "Epoch 8, Val Loss: 4.7678\n",
      "Epoch 9, Val Loss: 4.6801\n",
      "Epoch 10, Val Loss: 3.9616\n",
      "units=32, dropout=0.5, lr=0.01, n_layers=3, val_loss=1.9663\n",
      "Epoch 1, Val Loss: 37.6141\n",
      "Epoch 2, Val Loss: 36.3018\n",
      "Epoch 3, Val Loss: 34.5788\n",
      "Epoch 4, Val Loss: 32.7918\n",
      "Epoch 5, Val Loss: 31.0575\n",
      "Epoch 6, Val Loss: 29.5074\n",
      "Epoch 7, Val Loss: 28.0563\n",
      "Epoch 8, Val Loss: 26.7871\n",
      "Epoch 9, Val Loss: 25.5388\n",
      "Epoch 10, Val Loss: 24.3415\n",
      "units=64, dropout=0.3, lr=0.0005, n_layers=1, val_loss=24.3415\n",
      "Epoch 1, Val Loss: 37.4763\n",
      "Epoch 2, Val Loss: 36.7218\n",
      "Epoch 3, Val Loss: 35.7571\n",
      "Epoch 4, Val Loss: 34.5134\n",
      "Epoch 5, Val Loss: 33.0276\n",
      "Epoch 6, Val Loss: 31.4211\n",
      "Epoch 7, Val Loss: 30.2158\n",
      "Epoch 8, Val Loss: 28.7903\n",
      "Epoch 9, Val Loss: 27.8567\n",
      "Epoch 10, Val Loss: 26.8586\n",
      "units=64, dropout=0.3, lr=0.0005, n_layers=2, val_loss=26.8586\n",
      "Epoch 1, Val Loss: 38.0868\n",
      "Epoch 2, Val Loss: 36.7530\n",
      "Epoch 3, Val Loss: 35.7236\n",
      "Epoch 4, Val Loss: 34.9591\n",
      "Epoch 5, Val Loss: 33.9408\n",
      "Epoch 6, Val Loss: 32.6353\n",
      "Epoch 7, Val Loss: 31.6092\n",
      "Epoch 8, Val Loss: 30.8532\n",
      "Epoch 9, Val Loss: 29.9446\n",
      "Epoch 10, Val Loss: 28.9970\n",
      "units=64, dropout=0.3, lr=0.0005, n_layers=3, val_loss=28.9970\n",
      "Epoch 1, Val Loss: 39.9919\n",
      "Epoch 2, Val Loss: 38.0680\n",
      "Epoch 3, Val Loss: 35.2337\n",
      "Epoch 4, Val Loss: 31.9014\n",
      "Epoch 5, Val Loss: 28.8235\n",
      "Epoch 6, Val Loss: 26.0019\n",
      "Epoch 7, Val Loss: 23.6162\n",
      "Epoch 8, Val Loss: 21.5078\n",
      "Epoch 9, Val Loss: 19.4022\n",
      "Epoch 10, Val Loss: 17.5820\n",
      "units=64, dropout=0.3, lr=0.001, n_layers=1, val_loss=17.5820\n",
      "Epoch 1, Val Loss: 38.5776\n",
      "Epoch 2, Val Loss: 37.6913\n",
      "Epoch 3, Val Loss: 35.7243\n",
      "Epoch 4, Val Loss: 33.2058\n",
      "Epoch 5, Val Loss: 30.6419\n",
      "Epoch 6, Val Loss: 28.3242\n",
      "Epoch 7, Val Loss: 26.2487\n",
      "Epoch 8, Val Loss: 24.3094\n",
      "Epoch 9, Val Loss: 22.7719\n",
      "Epoch 10, Val Loss: 21.0341\n",
      "units=64, dropout=0.3, lr=0.001, n_layers=2, val_loss=21.0341\n",
      "Epoch 1, Val Loss: 37.7598\n",
      "Epoch 2, Val Loss: 36.4653\n",
      "Epoch 3, Val Loss: 34.6797\n",
      "Epoch 4, Val Loss: 32.6547\n",
      "Epoch 5, Val Loss: 30.4229\n",
      "Epoch 6, Val Loss: 28.3288\n",
      "Epoch 7, Val Loss: 26.4859\n",
      "Epoch 8, Val Loss: 24.7347\n",
      "Epoch 9, Val Loss: 22.6285\n",
      "Epoch 10, Val Loss: 21.1343\n",
      "units=64, dropout=0.3, lr=0.001, n_layers=3, val_loss=21.1343\n",
      "Epoch 1, Val Loss: 31.6716\n",
      "Epoch 2, Val Loss: 22.0014\n",
      "Epoch 3, Val Loss: 11.7775\n",
      "Epoch 4, Val Loss: 4.5785\n",
      "Epoch 5, Val Loss: 1.3747\n",
      "Epoch 6, Val Loss: 0.9129\n",
      "Epoch 7, Val Loss: 0.9649\n",
      "Epoch 8, Val Loss: 0.8989\n",
      "Epoch 9, Val Loss: 0.9036\n",
      "Epoch 10, Val Loss: 0.8870\n",
      "units=64, dropout=0.3, lr=0.005, n_layers=1, val_loss=0.8870\n",
      "Epoch 1, Val Loss: 36.3542\n",
      "Epoch 2, Val Loss: 28.6313\n",
      "Epoch 3, Val Loss: 18.7599\n",
      "Epoch 4, Val Loss: 10.4793\n",
      "Epoch 5, Val Loss: 4.4496\n",
      "Epoch 6, Val Loss: 2.1451\n",
      "Epoch 7, Val Loss: 1.2108\n",
      "Epoch 8, Val Loss: 1.1856\n",
      "Epoch 9, Val Loss: 1.1959\n",
      "Epoch 10, Val Loss: 1.1757\n",
      "units=64, dropout=0.3, lr=0.005, n_layers=2, val_loss=1.1757\n",
      "Epoch 1, Val Loss: 32.6432\n",
      "Epoch 2, Val Loss: 24.9357\n",
      "Epoch 3, Val Loss: 15.5993\n",
      "Epoch 4, Val Loss: 8.2714\n",
      "Epoch 5, Val Loss: 3.8051\n",
      "Epoch 6, Val Loss: 2.0609\n",
      "Epoch 7, Val Loss: 1.4745\n",
      "Epoch 8, Val Loss: 1.6326\n",
      "Epoch 9, Val Loss: 1.2490\n",
      "Epoch 10, Val Loss: 1.6397\n",
      "units=64, dropout=0.3, lr=0.005, n_layers=3, val_loss=1.2490\n",
      "Epoch 1, Val Loss: 25.9786\n",
      "Epoch 2, Val Loss: 6.7769\n",
      "Epoch 3, Val Loss: 1.0819\n",
      "Epoch 4, Val Loss: 1.0573\n",
      "Epoch 5, Val Loss: 1.0079\n",
      "Epoch 6, Val Loss: 1.1578\n",
      "Epoch 7, Val Loss: 0.8765\n",
      "Epoch 8, Val Loss: 0.9344\n",
      "Epoch 9, Val Loss: 0.7505\n",
      "Epoch 10, Val Loss: 0.8730\n",
      "units=64, dropout=0.3, lr=0.01, n_layers=1, val_loss=0.7505\n",
      "Epoch 1, Val Loss: 29.4835\n",
      "Epoch 2, Val Loss: 16.9675\n",
      "Epoch 3, Val Loss: 6.2310\n",
      "Epoch 4, Val Loss: 1.7200\n",
      "Epoch 5, Val Loss: 1.0943\n",
      "Epoch 6, Val Loss: 1.5437\n",
      "Epoch 7, Val Loss: 1.1029\n",
      "Epoch 8, Val Loss: 0.5875\n",
      "Epoch 9, Val Loss: 0.6925\n",
      "Epoch 10, Val Loss: 0.7873\n",
      "units=64, dropout=0.3, lr=0.01, n_layers=2, val_loss=0.5875\n",
      "Epoch 1, Val Loss: 31.7253\n",
      "Epoch 2, Val Loss: 15.5925\n",
      "Epoch 3, Val Loss: 5.4154\n",
      "Epoch 4, Val Loss: 1.3209\n",
      "Epoch 5, Val Loss: 1.4854\n",
      "Epoch 6, Val Loss: 0.9482\n",
      "Epoch 7, Val Loss: 1.1857\n",
      "Epoch 8, Val Loss: 0.6321\n",
      "Epoch 9, Val Loss: 0.8264\n",
      "Epoch 10, Val Loss: 0.9552\n",
      "units=64, dropout=0.3, lr=0.01, n_layers=3, val_loss=0.6321\n",
      "Epoch 1, Val Loss: 38.4788\n",
      "Epoch 2, Val Loss: 37.2605\n",
      "Epoch 3, Val Loss: 35.6785\n",
      "Epoch 4, Val Loss: 33.9898\n",
      "Epoch 5, Val Loss: 32.2547\n",
      "Epoch 6, Val Loss: 30.7208\n",
      "Epoch 7, Val Loss: 29.3365\n",
      "Epoch 8, Val Loss: 28.1230\n",
      "Epoch 9, Val Loss: 26.9949\n",
      "Epoch 10, Val Loss: 25.8324\n",
      "units=64, dropout=0.4, lr=0.0005, n_layers=1, val_loss=25.8324\n",
      "Epoch 1, Val Loss: 39.0801\n",
      "Epoch 2, Val Loss: 38.5261\n",
      "Epoch 3, Val Loss: 37.8008\n",
      "Epoch 4, Val Loss: 36.3301\n",
      "Epoch 5, Val Loss: 34.8687\n",
      "Epoch 6, Val Loss: 33.6353\n",
      "Epoch 7, Val Loss: 32.4377\n",
      "Epoch 8, Val Loss: 31.5185\n",
      "Epoch 9, Val Loss: 30.6155\n",
      "Epoch 10, Val Loss: 29.7321\n",
      "units=64, dropout=0.4, lr=0.0005, n_layers=2, val_loss=29.7321\n",
      "Epoch 1, Val Loss: 36.8463\n",
      "Epoch 2, Val Loss: 36.0696\n",
      "Epoch 3, Val Loss: 35.4881\n",
      "Epoch 4, Val Loss: 34.9036\n",
      "Epoch 5, Val Loss: 34.4763\n",
      "Epoch 6, Val Loss: 33.7348\n",
      "Epoch 7, Val Loss: 33.0981\n",
      "Epoch 8, Val Loss: 32.5228\n",
      "Epoch 9, Val Loss: 31.8437\n",
      "Epoch 10, Val Loss: 31.0529\n",
      "units=64, dropout=0.4, lr=0.0005, n_layers=3, val_loss=31.0529\n",
      "Epoch 1, Val Loss: 36.5159\n",
      "Epoch 2, Val Loss: 33.9597\n",
      "Epoch 3, Val Loss: 30.7552\n",
      "Epoch 4, Val Loss: 27.4228\n",
      "Epoch 5, Val Loss: 24.3891\n",
      "Epoch 6, Val Loss: 21.7274\n",
      "Epoch 7, Val Loss: 19.4876\n",
      "Epoch 8, Val Loss: 17.3674\n",
      "Epoch 9, Val Loss: 15.4804\n",
      "Epoch 10, Val Loss: 13.9688\n",
      "units=64, dropout=0.4, lr=0.001, n_layers=1, val_loss=13.9688\n",
      "Epoch 1, Val Loss: 37.0214\n",
      "Epoch 2, Val Loss: 35.6773\n",
      "Epoch 3, Val Loss: 34.0589\n",
      "Epoch 4, Val Loss: 31.9034\n",
      "Epoch 5, Val Loss: 29.6526\n",
      "Epoch 6, Val Loss: 27.8843\n",
      "Epoch 7, Val Loss: 26.0429\n",
      "Epoch 8, Val Loss: 24.3919\n",
      "Epoch 9, Val Loss: 22.8976\n",
      "Epoch 10, Val Loss: 21.4232\n",
      "units=64, dropout=0.4, lr=0.001, n_layers=2, val_loss=21.4232\n",
      "Epoch 1, Val Loss: 37.4462\n",
      "Epoch 2, Val Loss: 35.5999\n",
      "Epoch 3, Val Loss: 33.7194\n",
      "Epoch 4, Val Loss: 31.7297\n",
      "Epoch 5, Val Loss: 29.7813\n",
      "Epoch 6, Val Loss: 28.4887\n",
      "Epoch 7, Val Loss: 26.8890\n",
      "Epoch 8, Val Loss: 25.4142\n",
      "Epoch 9, Val Loss: 24.1236\n",
      "Epoch 10, Val Loss: 22.5869\n",
      "units=64, dropout=0.4, lr=0.001, n_layers=3, val_loss=22.5869\n",
      "Epoch 1, Val Loss: 31.8068\n",
      "Epoch 2, Val Loss: 21.4528\n",
      "Epoch 3, Val Loss: 10.3894\n",
      "Epoch 4, Val Loss: 3.1218\n",
      "Epoch 5, Val Loss: 0.7135\n",
      "Epoch 6, Val Loss: 0.7722\n",
      "Epoch 7, Val Loss: 0.7129\n",
      "Epoch 8, Val Loss: 0.6724\n",
      "Epoch 9, Val Loss: 0.6073\n",
      "Epoch 10, Val Loss: 0.6597\n",
      "units=64, dropout=0.4, lr=0.005, n_layers=1, val_loss=0.6073\n",
      "Epoch 1, Val Loss: 36.3205\n",
      "Epoch 2, Val Loss: 27.7459\n",
      "Epoch 3, Val Loss: 18.2808\n",
      "Epoch 4, Val Loss: 10.5939\n",
      "Epoch 5, Val Loss: 5.9381\n",
      "Epoch 6, Val Loss: 3.7074\n",
      "Epoch 7, Val Loss: 2.3170\n",
      "Epoch 8, Val Loss: 1.6505\n",
      "Epoch 9, Val Loss: 1.7838\n",
      "Epoch 10, Val Loss: 2.1478\n",
      "units=64, dropout=0.4, lr=0.005, n_layers=2, val_loss=1.6505\n",
      "Epoch 1, Val Loss: 36.0771\n",
      "Epoch 2, Val Loss: 27.6800\n",
      "Epoch 3, Val Loss: 18.4010\n",
      "Epoch 4, Val Loss: 10.2476\n",
      "Epoch 5, Val Loss: 5.7596\n",
      "Epoch 6, Val Loss: 2.9681\n",
      "Epoch 7, Val Loss: 1.6676\n",
      "Epoch 8, Val Loss: 2.0803\n",
      "Epoch 9, Val Loss: 2.4072\n",
      "Epoch 10, Val Loss: 2.3963\n",
      "units=64, dropout=0.4, lr=0.005, n_layers=3, val_loss=1.6676\n",
      "Epoch 1, Val Loss: 22.5515\n",
      "Epoch 2, Val Loss: 5.5173\n",
      "Epoch 3, Val Loss: 0.6693\n",
      "Epoch 4, Val Loss: 0.9466\n",
      "Epoch 5, Val Loss: 1.0299\n",
      "Epoch 6, Val Loss: 0.8839\n",
      "Epoch 7, Val Loss: 0.7228\n",
      "Epoch 8, Val Loss: 0.5631\n",
      "Epoch 9, Val Loss: 0.5138\n",
      "Epoch 10, Val Loss: 0.5658\n",
      "units=64, dropout=0.4, lr=0.01, n_layers=1, val_loss=0.5138\n",
      "Epoch 1, Val Loss: 30.0639\n",
      "Epoch 2, Val Loss: 16.3090\n",
      "Epoch 3, Val Loss: 5.6157\n",
      "Epoch 4, Val Loss: 2.3798\n",
      "Epoch 5, Val Loss: 2.4466\n",
      "Epoch 6, Val Loss: 3.5070\n",
      "Epoch 7, Val Loss: 2.8812\n",
      "Epoch 8, Val Loss: 1.6737\n",
      "Epoch 9, Val Loss: 1.1551\n",
      "Epoch 10, Val Loss: 1.0104\n",
      "units=64, dropout=0.4, lr=0.01, n_layers=2, val_loss=1.0104\n",
      "Epoch 1, Val Loss: 31.6679\n",
      "Epoch 2, Val Loss: 17.0081\n",
      "Epoch 3, Val Loss: 4.5040\n",
      "Epoch 4, Val Loss: 1.6766\n",
      "Epoch 5, Val Loss: 1.8935\n",
      "Epoch 6, Val Loss: 3.7480\n",
      "Epoch 7, Val Loss: 3.7437\n",
      "Epoch 8, Val Loss: 2.5327\n",
      "Epoch 9, Val Loss: 1.5112\n",
      "Epoch 10, Val Loss: 1.2256\n",
      "units=64, dropout=0.4, lr=0.01, n_layers=3, val_loss=1.2256\n",
      "Epoch 1, Val Loss: 36.9190\n",
      "Epoch 2, Val Loss: 35.8351\n",
      "Epoch 3, Val Loss: 34.5048\n",
      "Epoch 4, Val Loss: 33.1169\n",
      "Epoch 5, Val Loss: 31.6440\n",
      "Epoch 6, Val Loss: 30.3002\n",
      "Epoch 7, Val Loss: 29.0764\n",
      "Epoch 8, Val Loss: 27.8896\n",
      "Epoch 9, Val Loss: 26.7316\n",
      "Epoch 10, Val Loss: 25.5720\n",
      "units=64, dropout=0.5, lr=0.0005, n_layers=1, val_loss=25.5720\n",
      "Epoch 1, Val Loss: 36.9666\n",
      "Epoch 2, Val Loss: 36.3078\n",
      "Epoch 3, Val Loss: 35.8022\n",
      "Epoch 4, Val Loss: 34.7114\n",
      "Epoch 5, Val Loss: 33.5659\n",
      "Epoch 6, Val Loss: 32.6362\n",
      "Epoch 7, Val Loss: 31.8045\n",
      "Epoch 8, Val Loss: 30.9839\n",
      "Epoch 9, Val Loss: 30.1023\n",
      "Epoch 10, Val Loss: 29.3559\n",
      "units=64, dropout=0.5, lr=0.0005, n_layers=2, val_loss=29.3559\n",
      "Epoch 1, Val Loss: 37.0354\n",
      "Epoch 2, Val Loss: 35.8801\n",
      "Epoch 3, Val Loss: 35.2281\n",
      "Epoch 4, Val Loss: 34.4844\n",
      "Epoch 5, Val Loss: 33.6132\n",
      "Epoch 6, Val Loss: 32.9399\n",
      "Epoch 7, Val Loss: 32.0486\n",
      "Epoch 8, Val Loss: 31.5000\n",
      "Epoch 9, Val Loss: 30.8987\n",
      "Epoch 10, Val Loss: 30.1056\n",
      "units=64, dropout=0.5, lr=0.0005, n_layers=3, val_loss=30.1056\n",
      "Epoch 1, Val Loss: 34.9691\n",
      "Epoch 2, Val Loss: 32.5271\n",
      "Epoch 3, Val Loss: 29.4568\n",
      "Epoch 4, Val Loss: 26.1074\n",
      "Epoch 5, Val Loss: 23.3198\n",
      "Epoch 6, Val Loss: 20.6911\n",
      "Epoch 7, Val Loss: 18.3974\n",
      "Epoch 8, Val Loss: 16.3489\n",
      "Epoch 9, Val Loss: 14.6504\n",
      "Epoch 10, Val Loss: 13.0169\n",
      "units=64, dropout=0.5, lr=0.001, n_layers=1, val_loss=13.0169\n",
      "Epoch 1, Val Loss: 37.3928\n",
      "Epoch 2, Val Loss: 35.9157\n",
      "Epoch 3, Val Loss: 34.0976\n",
      "Epoch 4, Val Loss: 32.1263\n",
      "Epoch 5, Val Loss: 30.0853\n",
      "Epoch 6, Val Loss: 28.4781\n",
      "Epoch 7, Val Loss: 26.7638\n",
      "Epoch 8, Val Loss: 25.0671\n",
      "Epoch 9, Val Loss: 23.8457\n",
      "Epoch 10, Val Loss: 22.7105\n",
      "units=64, dropout=0.5, lr=0.001, n_layers=2, val_loss=22.7105\n",
      "Epoch 1, Val Loss: 38.6201\n",
      "Epoch 2, Val Loss: 36.7314\n",
      "Epoch 3, Val Loss: 34.7472\n",
      "Epoch 4, Val Loss: 33.3798\n",
      "Epoch 5, Val Loss: 31.8099\n",
      "Epoch 6, Val Loss: 30.6137\n",
      "Epoch 7, Val Loss: 29.5539\n",
      "Epoch 8, Val Loss: 28.3050\n",
      "Epoch 9, Val Loss: 26.8497\n",
      "Epoch 10, Val Loss: 25.3321\n",
      "units=64, dropout=0.5, lr=0.001, n_layers=3, val_loss=25.3321\n",
      "Epoch 1, Val Loss: 29.9496\n",
      "Epoch 2, Val Loss: 21.0038\n",
      "Epoch 3, Val Loss: 11.4492\n",
      "Epoch 4, Val Loss: 4.8272\n",
      "Epoch 5, Val Loss: 1.8848\n",
      "Epoch 6, Val Loss: 1.3963\n",
      "Epoch 7, Val Loss: 1.0802\n",
      "Epoch 8, Val Loss: 1.0251\n",
      "Epoch 9, Val Loss: 0.8421\n",
      "Epoch 10, Val Loss: 0.6661\n",
      "units=64, dropout=0.5, lr=0.005, n_layers=1, val_loss=0.6661\n",
      "Epoch 1, Val Loss: 35.8948\n",
      "Epoch 2, Val Loss: 29.0731\n",
      "Epoch 3, Val Loss: 21.4444\n",
      "Epoch 4, Val Loss: 14.3117\n",
      "Epoch 5, Val Loss: 8.8535\n",
      "Epoch 6, Val Loss: 5.2153\n",
      "Epoch 7, Val Loss: 3.1099\n",
      "Epoch 8, Val Loss: 2.3087\n",
      "Epoch 9, Val Loss: 2.4894\n",
      "Epoch 10, Val Loss: 3.0452\n",
      "units=64, dropout=0.5, lr=0.005, n_layers=2, val_loss=2.3087\n",
      "Epoch 1, Val Loss: 36.8571\n",
      "Epoch 2, Val Loss: 30.2601\n",
      "Epoch 3, Val Loss: 22.3273\n",
      "Epoch 4, Val Loss: 15.3749\n",
      "Epoch 5, Val Loss: 10.4414\n",
      "Epoch 6, Val Loss: 7.1109\n",
      "Epoch 7, Val Loss: 3.4691\n",
      "Epoch 8, Val Loss: 1.6015\n",
      "Epoch 9, Val Loss: 2.0388\n",
      "Epoch 10, Val Loss: 3.1977\n",
      "units=64, dropout=0.5, lr=0.005, n_layers=3, val_loss=1.6015\n",
      "Epoch 1, Val Loss: 26.7285\n",
      "Epoch 2, Val Loss: 9.3287\n",
      "Epoch 3, Val Loss: 1.3609\n",
      "Epoch 4, Val Loss: 1.3008\n",
      "Epoch 5, Val Loss: 1.1295\n",
      "Epoch 6, Val Loss: 1.3015\n",
      "Epoch 7, Val Loss: 1.5299\n",
      "Epoch 8, Val Loss: 1.3927\n",
      "Epoch 9, Val Loss: 1.3456\n",
      "Epoch 10, Val Loss: 1.2413\n",
      "Early stopping!\n",
      "units=64, dropout=0.5, lr=0.01, n_layers=1, val_loss=1.1295\n",
      "Epoch 1, Val Loss: 30.3167\n",
      "Epoch 2, Val Loss: 17.8932\n",
      "Epoch 3, Val Loss: 7.3494\n",
      "Epoch 4, Val Loss: 1.9405\n",
      "Epoch 5, Val Loss: 2.6626\n",
      "Epoch 6, Val Loss: 5.0250\n",
      "Epoch 7, Val Loss: 4.9864\n",
      "Epoch 8, Val Loss: 3.9902\n",
      "Epoch 9, Val Loss: 2.4705\n",
      "Early stopping!\n",
      "units=64, dropout=0.5, lr=0.01, n_layers=2, val_loss=1.9405\n",
      "Epoch 1, Val Loss: 30.8070\n",
      "Epoch 2, Val Loss: 17.9576\n",
      "Epoch 3, Val Loss: 6.4557\n",
      "Epoch 4, Val Loss: 2.9552\n",
      "Epoch 5, Val Loss: 4.4055\n",
      "Epoch 6, Val Loss: 4.3339\n",
      "Epoch 7, Val Loss: 2.8631\n",
      "Epoch 8, Val Loss: 2.4625\n",
      "Epoch 9, Val Loss: 2.4121\n",
      "Epoch 10, Val Loss: 1.9891\n",
      "units=64, dropout=0.5, lr=0.01, n_layers=3, val_loss=1.9891\n"
     ]
    }
   ],
   "source": [
    "from itertools import product\n",
    "param_grid = {\n",
    "    'units': [32, 64],\n",
    "    'dropout': [0.3, 0.4, 0.5],\n",
    "    'learning_rate': [0.0005, 0.001, 0.005, 0.01],\n",
    "    'n_layers': [1, 2, 3]\n",
    "}\n",
    "results = []\n",
    "for units, dropout, lr, n_layers in product(param_grid['units'], param_grid['dropout'], param_grid['learning_rate'],\n",
    "                                 param_grid['n_layers']):\n",
    "    val_loss = train_and_evaluate(units, dropout, lr, n_layers, 10)\n",
    "    print(f\"units={units}, dropout={dropout}, lr={lr}, n_layers={n_layers}, val_loss={val_loss:.4f}\")\n",
    "    results.append({'units': units, 'dropout': dropout, 'lr': lr, 'n_layers': n_layers, 'val_loss': val_loss})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "40c6cd8e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Best hyperparams: {'units': 64, 'dropout': 0.4, 'lr': 0.01, 'n_layers': 1, 'val_loss': 0.513751805790009}\n"
     ]
    }
   ],
   "source": [
    "best = min(results, key=lambda x: x['val_loss'])\n",
    "print(f\"Best hyperparams: {best}\")\n",
    "best_params = {k: best[k] for k in ['units', 'dropout', 'lr', 'n_layers']}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "762f3de2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1, Loss: 29.5077\n",
      "Epoch 2, Loss: 8.1022\n",
      "Epoch 3, Loss: 2.2412\n",
      "Epoch 4, Loss: 2.5810\n",
      "Epoch 5, Loss: 1.4321\n",
      "Epoch 6, Loss: 1.6068\n",
      "Epoch 7, Loss: 1.4194\n",
      "Epoch 8, Loss: 1.6474\n",
      "Epoch 9, Loss: 1.8965\n",
      "Epoch 10, Loss: 1.7259\n",
      "Epoch 11, Loss: 1.3779\n",
      "Epoch 12, Loss: 1.4193\n",
      "Epoch 13, Loss: 1.2917\n",
      "Epoch 14, Loss: 1.1263\n",
      "Epoch 15, Loss: 1.1143\n",
      "Epoch 16, Loss: 1.2479\n",
      "Epoch 17, Loss: 1.1078\n",
      "Epoch 18, Loss: 1.0419\n",
      "Epoch 19, Loss: 1.1788\n",
      "Epoch 20, Loss: 1.0533\n",
      "Epoch 21, Loss: 1.0940\n",
      "Epoch 22, Loss: 0.9429\n",
      "Epoch 23, Loss: 0.9113\n",
      "Epoch 24, Loss: 0.9887\n",
      "Epoch 25, Loss: 0.7187\n",
      "Epoch 26, Loss: 0.8476\n",
      "Epoch 27, Loss: 0.8901\n",
      "Epoch 28, Loss: 0.8954\n",
      "Epoch 29, Loss: 0.9306\n",
      "Epoch 30, Loss: 0.9085\n",
      "Epoch 31, Loss: 0.9178\n",
      "Epoch 32, Loss: 0.7245\n",
      "Epoch 33, Loss: 0.7827\n",
      "Epoch 34, Loss: 0.8243\n",
      "Epoch 35, Loss: 0.7981\n",
      "Epoch 36, Loss: 0.7834\n",
      "Epoch 37, Loss: 0.7921\n",
      "Epoch 38, Loss: 0.6947\n",
      "Epoch 39, Loss: 0.7723\n",
      "Epoch 40, Loss: 0.7038\n",
      "Epoch 41, Loss: 0.7414\n",
      "Epoch 42, Loss: 0.7244\n",
      "Epoch 43, Loss: 0.7197\n",
      "Epoch 44, Loss: 0.6966\n",
      "Epoch 45, Loss: 0.6593\n",
      "Epoch 46, Loss: 0.6412\n",
      "Epoch 47, Loss: 0.6429\n",
      "Epoch 48, Loss: 0.6893\n",
      "Epoch 49, Loss: 0.6690\n",
      "Epoch 50, Loss: 0.6157\n"
     ]
    }
   ],
   "source": [
    "X_combined_iphone = torch.cat([X_train_tensor_iphone, X_val_tensor_iphone], dim=0)\n",
    "y_combined_iphone_log = torch.cat([y_train_tensor_iphone_log, y_val_tensor_iphone_log], dim=0)\n",
    "combined_dataset_iphone_log = TensorDataset(X_combined_iphone, y_combined_iphone_log)\n",
    "combined_loader_iphone_log = DataLoader(combined_dataset_iphone_log, batch_size=32, shuffle=True)\n",
    "\n",
    "# Create new model using best hyperparameters\n",
    "final_model = SimpleNN(\n",
    "    input_dim=X_train_tensor_iphone.shape[1],\n",
    "    units=best_params['units'],\n",
    "    dropout=best_params['dropout'],\n",
    "    n_layers=best_params['n_layers'],\n",
    ")\n",
    "optimizer = torch.optim.AdamW(final_model.parameters(), lr=best_params['lr'])\n",
    "criterion = torch.nn.MSELoss()\n",
    "\n",
    "train_losses = []\n",
    "\n",
    "for epoch in range(50):\n",
    "    final_model.train()\n",
    "    running_loss = 0\n",
    "    total = 0\n",
    "    for xb, yb in combined_loader_iphone_log:\n",
    "        optimizer.zero_grad()\n",
    "        preds = final_model(xb)\n",
    "        loss = criterion(preds, yb)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        running_loss += loss.item() * xb.size(0)\n",
    "        total += yb.size(0)\n",
    "    epoch_loss = running_loss / total\n",
    "    train_losses.append(epoch_loss)\n",
    "    print(f\"Epoch {epoch+1}, Loss: {epoch_loss:.4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "7edfd2f1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test RMSE (log scale): 0.3469\n",
      "Test RMSE (original scale): 209.1262\n"
     ]
    }
   ],
   "source": [
    "# Compute residuals on val set to estimate sigma for back-transformation\n",
    "final_model.eval()\n",
    "with torch.no_grad():\n",
    "    preds_val_log = final_model(X_val_tensor_iphone)\n",
    "    residuals_log = preds_val_log - y_val_tensor_iphone_log\n",
    "    sigma2 = torch.var(residuals_log)  # used in log-normal correction\n",
    "\n",
    "# Evaluate on test set with log-normal correction\n",
    "all_preds = []\n",
    "all_targets = []\n",
    "all_preds_log = []\n",
    "all_targets_log = []\n",
    "with torch.no_grad():\n",
    "    for xb, yb in test_loader_iphone_log:  \n",
    "        preds_log = final_model(xb)\n",
    "        preds = torch.exp(preds_log + sigma2 / 2)  # apply correction\n",
    "        targets = torch.exp(yb)\n",
    "        all_preds_log.append(preds_log)\n",
    "        all_targets_log.append(yb)\n",
    "        all_preds.append(preds)\n",
    "        all_targets.append(targets)\n",
    "\n",
    "all_preds_log = torch.cat(all_preds_log).cpu().numpy()\n",
    "all_targets_log = torch.cat(all_targets_log).cpu().numpy()\n",
    "all_preds = torch.cat(all_preds).cpu().numpy()\n",
    "all_targets = torch.cat(all_targets).cpu().numpy()\n",
    "\n",
    "rmse_log = np.sqrt(np.mean((all_preds_log - all_targets_log)**2))\n",
    "print(f\"Test RMSE (log scale): {rmse_log:.4f}\")\n",
    "rmse = np.sqrt(np.mean((all_preds - all_targets)**2))\n",
    "print(f\"Test RMSE (original scale): {rmse:.4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "c5780b7e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Raw predictions (log scale): tensor([[6.1005],\n",
      "        [5.9197],\n",
      "        [5.9172],\n",
      "        [6.0687],\n",
      "        [6.1211]])\n",
      "Exp predictions: tensor([[463.5291],\n",
      "        [386.8769],\n",
      "        [385.9118],\n",
      "        [449.0351],\n",
      "        [473.1976]])\n",
      "Raw targets (log scale): tensor([[6.9276],\n",
      "        [6.3116],\n",
      "        [6.1737],\n",
      "        [6.3526],\n",
      "        [6.1420]])\n",
      "Exp targets: tensor([[1020.0798],\n",
      "        [ 550.9500],\n",
      "        [ 479.9499],\n",
      "        [ 573.9901],\n",
      "        [ 464.9900]])\n"
     ]
    }
   ],
   "source": [
    "print(\"Raw predictions (log scale):\", preds_log[:5])\n",
    "print(\"Exp predictions:\", preds[:5])\n",
    "print(\"Raw targets (log scale):\", yb[:5])\n",
    "print(\"Exp targets:\", targets[:5])\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "15663ead",
   "metadata": {},
   "source": [
    "#### MLR"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "f120b1d0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                            OLS Regression Results                            \n",
      "==============================================================================\n",
      "Dep. Variable:            price.value   R-squared:                       0.855\n",
      "Model:                            OLS   Adj. R-squared:                  0.833\n",
      "Method:                 Least Squares   F-statistic:                     39.29\n",
      "Date:                Sun, 10 Aug 2025   Prob (F-statistic):           5.48e-90\n",
      "Time:                        14:16:23   Log-Likelihood:                -1787.7\n",
      "No. Observations:                 308   AIC:                             3657.\n",
      "Df Residuals:                     267   BIC:                             3810.\n",
      "Df Model:                          40                                         \n",
      "Covariance Type:            nonrobust                                         \n",
      "======================================================================================================\n",
      "                                         coef    std err          t      P>|t|      [0.025      0.975]\n",
      "------------------------------------------------------------------------------------------------------\n",
      "const                                442.7935     92.925      4.765      0.000     259.835     625.752\n",
      "seller.feedbackPercentage            -12.7884      5.715     -2.238      0.026     -24.041      -1.535\n",
      "seller.feedbackScore                 -12.7434      7.730     -1.649      0.100     -27.964       2.477\n",
      "marketingPrice.discountPercentage     -3.5605      6.876     -0.518      0.605     -17.098       9.977\n",
      "shipping_cost                         -3.6134      9.036     -0.400      0.690     -21.405      14.178\n",
      "days_listed                            6.4481      6.724      0.959      0.338      -6.790      19.686\n",
      "seller_item_count                    -16.5210      9.636     -1.714      0.088     -35.494       2.452\n",
      "additional_image_count                 1.0292      6.070      0.170      0.865     -10.923      12.981\n",
      "title_length                         -10.7505      6.695     -1.606      0.109     -23.932       2.431\n",
      "topRatedBuyingExperience              11.4320     18.171      0.629      0.530     -24.345      47.209\n",
      "priorityListing                       15.5760     13.354      1.166      0.245     -10.717      41.869\n",
      "condition_For parts or not working  -252.7013     44.794     -5.641      0.000    -340.896    -164.507\n",
      "condition_Good - Refurbished         -47.9800     21.841     -2.197      0.029     -90.982      -4.978\n",
      "condition_New                         48.2542     27.424      1.760      0.080      -5.740     102.248\n",
      "condition_Open box                    -1.2821     24.528     -0.052      0.958     -49.575      47.010\n",
      "condition_Used                       -25.3911     17.803     -1.426      0.155     -60.444       9.662\n",
      "condition_Very Good - Refurbished    -18.6014     16.502     -1.127      0.261     -51.092      13.890\n",
      "itemLocation.country_HK            -2.585e-13   1.58e-13     -1.639      0.102   -5.69e-13    5.19e-14\n",
      "itemLocation.country_IT              421.5100    151.849      2.776      0.006     122.536     720.484\n",
      "itemLocation.country_SE            -1.244e-13   4.77e-14     -2.611      0.010   -2.18e-13   -3.06e-14\n",
      "itemLocation.country_US               29.6974     45.376      0.654      0.513     -59.644     119.038\n",
      "specific_carrier_ATT                -171.6743    102.005     -1.683      0.094    -372.512      29.163\n",
      "specific_carrier_Argon                16.9259     96.118      0.176      0.860    -172.321     206.172\n",
      "specific_carrier_Cellular             27.1016     53.127      0.510      0.610     -77.500     131.703\n",
      "specific_carrier_Cricket              18.6690     74.146      0.252      0.801    -127.315     164.654\n",
      "specific_carrier_Midnight             76.8017     98.912      0.776      0.438    -117.946     271.549\n",
      "specific_carrier_Spectrum             -7.7922     57.476     -0.136      0.892    -120.956     105.372\n",
      "specific_carrier_T-Mobile              7.7537     43.154      0.180      0.858     -77.211      92.719\n",
      "specific_carrier_Verizon              19.6175     51.633      0.380      0.704     -82.042     121.277\n",
      "specific_carrier_Xfinity              11.7638     57.609      0.204      0.838    -101.661     125.189\n",
      "specific_carrier_no_carrier           83.8926     41.324      2.030      0.043       2.531     165.254\n",
      "model_number_15.0                    179.2838     16.014     11.196      0.000     147.755     210.813\n",
      "model_number_16.0                    334.8874     15.620     21.439      0.000     304.133     365.642\n",
      "storage_1TB                          152.7854     32.007      4.774      0.000      89.767     215.804\n",
      "storage_256GB                         53.7861     14.850      3.622      0.000      24.548      83.024\n",
      "storage_512GB                         42.7281     22.868      1.869      0.063      -2.296      87.752\n",
      "storage_Unknown                       99.1326     32.733      3.029      0.003      34.685     163.580\n",
      "model_variant_PRO MAX                 -8.6178    110.925     -0.078      0.938    -227.016     209.780\n",
      "model_variant_Plus                  -166.4301     71.429     -2.330      0.021    -307.065     -25.795\n",
      "model_variant_Pro                    -56.4551     70.333     -0.803      0.423    -194.934      82.024\n",
      "model_variant_Pro Max                 47.9180     70.480      0.680      0.497     -90.850     186.685\n",
      "model_variant_none                  -236.0976     69.719     -3.386      0.001    -373.367     -98.828\n",
      "model_variant_pro max                175.1643    113.896      1.538      0.125     -49.084     399.412\n",
      "==============================================================================\n",
      "Omnibus:                      135.341   Durbin-Watson:                   1.949\n",
      "Prob(Omnibus):                  0.000   Jarque-Bera (JB):             2553.741\n",
      "Skew:                           1.298   Prob(JB):                         0.00\n",
      "Kurtosis:                      16.866   Cond. No.                     1.16e+16\n",
      "==============================================================================\n",
      "\n",
      "Notes:\n",
      "[1] Standard Errors assume that the covariance matrix of the errors is correctly specified.\n",
      "[2] The smallest eigenvalue is 9.55e-30. This might indicate that there are\n",
      "strong multicollinearity problems or that the design matrix is singular.\n"
     ]
    }
   ],
   "source": [
    "X_combined_iphone = pd.concat([X_train_iphone, X_val_iphone], axis=0)\n",
    "y_combined_iphone = pd.concat([y_train_iphone, y_val_iphone], axis=0)\n",
    "\n",
    "# Add constant (intercept)\n",
    "X = sm.add_constant(X_combined_iphone.astype(float))  \n",
    "model = sm.OLS(y_combined_iphone, X).fit()\n",
    "\n",
    "# Get summary\n",
    "print(model.summary())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "9a8e0709",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                            OLS Regression Results                            \n",
      "==============================================================================\n",
      "Dep. Variable:            price.value   R-squared:                       0.869\n",
      "Model:                            OLS   Adj. R-squared:                  0.850\n",
      "Method:                 Least Squares   F-statistic:                     44.33\n",
      "Date:                Sun, 10 Aug 2025   Prob (F-statistic):           6.90e-96\n",
      "Time:                        14:16:23   Log-Likelihood:                 194.65\n",
      "No. Observations:                 308   AIC:                            -307.3\n",
      "Df Residuals:                     267   BIC:                            -154.4\n",
      "Df Model:                          40                                         \n",
      "Covariance Type:            nonrobust                                         \n",
      "======================================================================================================\n",
      "                                         coef    std err          t      P>|t|      [0.025      0.975]\n",
      "------------------------------------------------------------------------------------------------------\n",
      "const                                  6.2178      0.149     41.760      0.000       5.925       6.511\n",
      "seller.feedbackPercentage             -0.0312      0.009     -3.407      0.001      -0.049      -0.013\n",
      "seller.feedbackScore                  -0.0111      0.012     -0.899      0.370      -0.036       0.013\n",
      "marketingPrice.discountPercentage     -0.0081      0.011     -0.731      0.466      -0.030       0.014\n",
      "shipping_cost                         -0.0078      0.014     -0.537      0.592      -0.036       0.021\n",
      "days_listed                            0.0091      0.011      0.848      0.397      -0.012       0.030\n",
      "seller_item_count                     -0.0300      0.015     -1.945      0.053      -0.060       0.000\n",
      "additional_image_count                -0.0099      0.010     -1.022      0.308      -0.029       0.009\n",
      "title_length                          -0.0171      0.011     -1.590      0.113      -0.038       0.004\n",
      "topRatedBuyingExperience               0.0347      0.029      1.192      0.234      -0.023       0.092\n",
      "priorityListing                        0.0527      0.021      2.462      0.014       0.011       0.095\n",
      "condition_For parts or not working    -0.4673      0.072     -6.511      0.000      -0.609      -0.326\n",
      "condition_Good - Refurbished          -0.1180      0.035     -3.371      0.001      -0.187      -0.049\n",
      "condition_New                          0.0242      0.044      0.550      0.583      -0.062       0.111\n",
      "condition_Open box                    -0.0153      0.039     -0.388      0.698      -0.093       0.062\n",
      "condition_Used                        -0.0562      0.029     -1.971      0.050      -0.112   -6.33e-05\n",
      "condition_Very Good - Refurbished     -0.0417      0.026     -1.576      0.116      -0.094       0.010\n",
      "itemLocation.country_HK            -3.095e-16   2.53e-16     -1.225      0.222   -8.07e-16    1.88e-16\n",
      "itemLocation.country_IT                0.4331      0.243      1.780      0.076      -0.046       0.912\n",
      "itemLocation.country_SE            -1.708e-16   7.64e-17     -2.237      0.026   -3.21e-16   -2.05e-17\n",
      "itemLocation.country_US               -0.0090      0.073     -0.123      0.902      -0.152       0.134\n",
      "specific_carrier_ATT                  -0.1299      0.163     -0.795      0.428      -0.452       0.192\n",
      "specific_carrier_Argon                 0.0707      0.154      0.459      0.647      -0.233       0.374\n",
      "specific_carrier_Cellular              0.0559      0.085      0.657      0.512      -0.112       0.224\n",
      "specific_carrier_Cricket               0.0141      0.119      0.119      0.905      -0.220       0.248\n",
      "specific_carrier_Midnight              0.1596      0.158      1.007      0.315      -0.152       0.472\n",
      "specific_carrier_Spectrum             -0.0029      0.092     -0.032      0.975      -0.184       0.178\n",
      "specific_carrier_T-Mobile             -0.0050      0.069     -0.072      0.943      -0.141       0.131\n",
      "specific_carrier_Verizon               0.0470      0.083      0.568      0.571      -0.116       0.210\n",
      "specific_carrier_Xfinity               0.0531      0.092      0.576      0.565      -0.129       0.235\n",
      "specific_carrier_no_carrier            0.1205      0.066      1.820      0.070      -0.010       0.251\n",
      "model_number_15.0                      0.3608      0.026     14.063      0.000       0.310       0.411\n",
      "model_number_16.0                      0.5688      0.025     22.724      0.000       0.519       0.618\n",
      "storage_1TB                            0.2329      0.051      4.542      0.000       0.132       0.334\n",
      "storage_256GB                          0.0879      0.024      3.693      0.000       0.041       0.135\n",
      "storage_512GB                          0.0983      0.037      2.684      0.008       0.026       0.170\n",
      "storage_Unknown                        0.1471      0.052      2.805      0.005       0.044       0.250\n",
      "model_variant_PRO MAX                 -0.0310      0.178     -0.175      0.862      -0.381       0.319\n",
      "model_variant_Plus                    -0.3774      0.114     -3.298      0.001      -0.603      -0.152\n",
      "model_variant_Pro                     -0.1601      0.113     -1.420      0.157      -0.382       0.062\n",
      "model_variant_Pro Max                 -0.0219      0.113     -0.194      0.846      -0.244       0.200\n",
      "model_variant_none                    -0.4970      0.112     -4.449      0.000      -0.717      -0.277\n",
      "model_variant_pro max                  0.0715      0.182      0.392      0.696      -0.288       0.431\n",
      "==============================================================================\n",
      "Omnibus:                       63.094   Durbin-Watson:                   1.884\n",
      "Prob(Omnibus):                  0.000   Jarque-Bera (JB):              481.193\n",
      "Skew:                           0.569   Prob(JB):                    3.24e-105\n",
      "Kurtosis:                       9.017   Cond. No.                     1.16e+16\n",
      "==============================================================================\n",
      "\n",
      "Notes:\n",
      "[1] Standard Errors assume that the covariance matrix of the errors is correctly specified.\n",
      "[2] The smallest eigenvalue is 9.55e-30. This might indicate that there are\n",
      "strong multicollinearity problems or that the design matrix is singular.\n"
     ]
    }
   ],
   "source": [
    "# Add constant (intercept)\n",
    "y_combined_iphone_log = np.log(y_combined_iphone)\n",
    "X = sm.add_constant(X_combined_iphone.astype(float))  # or: sm.add_constant(df[['feature1', 'feature2', ...]])\n",
    "model_log = sm.OLS(y_combined_iphone_log, X).fit()\n",
    "\n",
    "# Get summary\n",
    "print(model_log.summary())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "f1a54a64",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Original Price - Test RMSE: 100.3040\n",
      "Log Price - Test RMSE: 0.1479\n",
      "Log Price Converted - Test RMSE: 105.1955\n"
     ]
    }
   ],
   "source": [
    "X_test_iphone_const = sm.add_constant(X_test_iphone, has_constant='add')\n",
    "\n",
    "y_test_pred_orig = model.predict(X_test_iphone_const)\n",
    "test_rmse_orig = np.sqrt(mean_squared_error(y_test_iphone, y_test_pred_orig))\n",
    "print(f\"Original Price - Test RMSE: {test_rmse_orig:.4f}\")\n",
    "\n",
    "y_test_pred_log = model_log.predict(X_test_iphone_const)\n",
    "y_test_pred_log = np.array(y_test_pred_log, dtype=float)\n",
    "y_test_pred_log_exp = np.exp(y_test_pred_log)  # back-transform\n",
    "\n",
    "test_rmse_log = np.sqrt(mean_squared_error(np.log(y_test_iphone), y_test_pred_log))\n",
    "print(f\"Log Price - Test RMSE: {test_rmse_log:.4f}\")\n",
    "test_rmse_log_exp = np.sqrt(mean_squared_error(y_test_iphone, y_test_pred_log_exp))\n",
    "print(f\"Log Price Converted - Test RMSE: {test_rmse_log_exp:.4f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5c43faba",
   "metadata": {},
   "source": [
    "### Soccer"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5e8f74c4",
   "metadata": {},
   "source": [
    "#### Classification"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "b1079e1a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(476, 73) (119, 73) (149, 73)\n",
      "(476,) (119,) (149,)\n"
     ]
    }
   ],
   "source": [
    "X_soccer = df_soccer.drop(columns=[\"itemId\", \"title\", \"conditionId\", \"price.value\", \n",
    "                                   \"price_tier\", \"seller.username\", \"condition_desc\", \n",
    "                                   \"category_id\", \"price.currency\", \"cluster\", \n",
    "                                   \"price_tier_encoded\", \"marketingPrice.originalPrice.value\",\n",
    "                                   \"discount_flag\", \"top_country\", \"top_club\"])\n",
    "\n",
    "X_soccer[\"marketingPrice.discountPercentage\"] = X_soccer[\"marketingPrice.discountPercentage\"].fillna(0).astype(int)\n",
    "\n",
    "bool_cols = X_soccer.select_dtypes(include=['bool']).columns.tolist()\n",
    "numeric_cols = X_soccer.select_dtypes(include=['int', 'float']).columns.tolist()\n",
    "numeric_cols = [col for col in numeric_cols if col not in bool_cols]\n",
    "\n",
    "sc = StandardScaler()\n",
    "X_scaled_num_array = sc.fit_transform(X_soccer[numeric_cols])\n",
    "X_scaled_num_soccer = pd.DataFrame(X_scaled_num_array, columns=numeric_cols, index=X_soccer.index)\n",
    "X_bool_soccer  = X_soccer[bool_cols].astype(int)\n",
    "X_comb = pd.concat([X_scaled_num_soccer, X_bool_soccer], axis=1)\n",
    "\n",
    "categorical_cols = ['condition', 'itemLocation.country', 'category_name', \"country\", \"club\"]\n",
    "X_encoded_soccer = pd.get_dummies(X_comb.join(X_soccer[categorical_cols]), columns=categorical_cols, drop_first=True)\n",
    "\n",
    "y_soccer = df_soccer['price_tier_encoded']\n",
    "                         \n",
    "X_train_soccer, X_test_soccer, y_train_soccer, y_test_soccer = train_test_split(X_encoded_soccer, y_soccer,\n",
    "                                                                 test_size=0.20, random_state=1216) \n",
    "\n",
    "#split training set into train / validation for model building\n",
    "X_train_soccer, X_val_soccer, y_train_soccer, y_val_soccer = train_test_split(X_train_soccer, y_train_soccer,\n",
    "                                                                test_size=0.20, random_state=1216)\n",
    "\n",
    "print(X_train_soccer.shape, X_val_soccer.shape, X_test_soccer.shape)\n",
    "print(y_train_soccer.shape, y_val_soccer.shape, y_test_soccer.shape)\n",
    "\n",
    "#for MLR or others, full training set\n",
    "X_train_full_soccer = np.concatenate((X_train_soccer, X_val_soccer), axis=0)\n",
    "y_train_full_soccer = np.concatenate((y_train_soccer, y_val_soccer), axis=0)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "95d1b575",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "condition                             object\n",
       "seller.feedbackPercentage            float64\n",
       "seller.feedbackScore                   int64\n",
       "itemLocation.country                  object\n",
       "topRatedBuyingExperience                bool\n",
       "priorityListing                         bool\n",
       "marketingPrice.discountPercentage      int64\n",
       "shipping_cost                        float64\n",
       "days_listed                            int64\n",
       "category_name                         object\n",
       "seller_item_count                      int64\n",
       "club                                  object\n",
       "country                               object\n",
       "year                                 float64\n",
       "additional_image_count                 int64\n",
       "title_length                           int64\n",
       "Messi                                   bool\n",
       "dtype: object"
      ]
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_soccer.dtypes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "43d2811f",
   "metadata": {},
   "outputs": [],
   "source": [
    "os.environ['PYTHONHASHSEED'] = '1216'\n",
    "random.seed(1216)\n",
    "np.random.seed(1216)\n",
    "torch.manual_seed(1216)\n",
    "torch.backends.cudnn.deterministic = True\n",
    "torch.backends.cudnn.benchmark = False\n",
    "\n",
    "X_train_soccer = X_train_soccer.astype(np.float32)\n",
    "X_val_soccer = X_val_soccer.astype(np.float32)\n",
    "X_test_soccer = X_test_soccer.astype(np.float32)\n",
    "\n",
    "X_train_tensor_soccer = torch.tensor(X_train_soccer.values, dtype=torch.float32)\n",
    "X_val_tensor_soccer = torch.tensor(X_val_soccer.values, dtype=torch.float32)\n",
    "X_test_tensor_soccer = torch.tensor(X_test_soccer.values, dtype=torch.float32)\n",
    "\n",
    "y_train_tensor_soccer = torch.tensor(y_train_soccer.values, dtype=torch.long)\n",
    "y_val_tensor_soccer = torch.tensor(y_val_soccer.values, dtype=torch.long)\n",
    "y_test_tensor_soccer = torch.tensor(y_test_soccer.values, dtype=torch.long)\n",
    "\n",
    "train_dataset_soccer = TensorDataset(X_train_tensor_soccer, y_train_tensor_soccer)\n",
    "val_dataset_soccer = TensorDataset(X_val_tensor_soccer, y_val_tensor_soccer)\n",
    "test_dataset_soccer = TensorDataset(X_test_tensor_soccer, y_test_tensor_soccer)\n",
    "\n",
    "train_loader_soccer = DataLoader(train_dataset_soccer, batch_size=32, shuffle=True)\n",
    "val_loader_soccer = DataLoader(val_dataset_soccer, batch_size=32, shuffle=False)\n",
    "test_loader_soccer = DataLoader(test_dataset_soccer, batch_size=32, shuffle=False)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "b5041af5",
   "metadata": {},
   "outputs": [],
   "source": [
    "class SimpleNN(nn.Module):\n",
    "    def __init__(self, input_dim, units, dropout, n_layers, num_classes):\n",
    "        super(SimpleNN, self).__init__()\n",
    "        layers = []\n",
    "        # First layer\n",
    "        layers.append(nn.Linear(input_dim, units))\n",
    "        layers.append(nn.BatchNorm1d(units)) \n",
    "        layers.append(nn.ReLU())\n",
    "        layers.append(nn.Dropout(dropout))\n",
    "        # Hidden layers\n",
    "        for _ in range(n_layers - 1):\n",
    "            layers.append(nn.Linear(units, units))\n",
    "            layers.append(nn.BatchNorm1d(units)) \n",
    "            layers.append(nn.ReLU())\n",
    "            layers.append(nn.Dropout(dropout))\n",
    "        # Output layer\n",
    "        layers.append(nn.Linear(units, num_classes)) \n",
    "        self.net = nn.Sequential(*layers)\n",
    "    def forward(self, x):\n",
    "        return self.net(x)\n",
    "    \n",
    "def train_and_evaluate(units, dropout, lr, n_layers, epochs=10):\n",
    "    model = SimpleNN(input_dim=X_train_tensor_soccer.shape[1], units=units, dropout=dropout, n_layers=n_layers, num_classes=3)\n",
    "    criterion = torch.nn.CrossEntropyLoss()\n",
    "    optimizer = torch.optim.AdamW(model.parameters(), lr=lr)\n",
    "\n",
    "    for epoch in range(epochs):\n",
    "        model.train()\n",
    "        for xb, yb in train_loader_soccer:\n",
    "            optimizer.zero_grad()\n",
    "            preds = model(xb)\n",
    "            loss = criterion(preds, yb)\n",
    "            loss.backward()\n",
    "            torch.nn.utils.clip_grad_norm_(model.parameters(), max_norm=1.0)\n",
    "            optimizer.step()\n",
    "\n",
    "        # Validation per epoch\n",
    "        model.eval()\n",
    "        val_loss = 0\n",
    "        val_correct = 0\n",
    "        val_total = 0\n",
    "        with torch.no_grad():\n",
    "            for xb, yb in val_loader_soccer:\n",
    "                preds = model(xb)\n",
    "                loss = criterion(preds, yb)\n",
    "                val_loss += loss.item() * xb.size(0)\n",
    "                val_correct += (preds.argmax(dim=1) == yb).sum().item()\n",
    "                val_total += yb.size(0)\n",
    "        val_loss /= val_total\n",
    "        val_acc = val_correct / val_total\n",
    "        print(f\"Epoch {epoch+1}, Val Loss: {val_loss:.4f}, Val Accuracy: {val_acc:.4f}\")\n",
    "    return val_loss, val_acc\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "7f97e7f2-a46b-4d9a-8845-702bfed85c10",
   "metadata": {},
   "outputs": [],
   "source": [
    "# from itertools import product\n",
    "# param_grid = {\n",
    "#     'units': [32, 64],\n",
    "#     'dropout': [0.3, 0.4, 0.5],\n",
    "#     'learning_rate': [0.0005, 0.001, 0.005, 0.01],\n",
    "#     'n_layers': [1, 2, 3]\n",
    "# }\n",
    "# results = []\n",
    "# for units, dropout, lr, n_layers in product(param_grid['units'], param_grid['dropout'], param_grid['learning_rate'],\n",
    "#                                  param_grid['n_layers']):\n",
    "#     val_loss, val_acc = train_and_evaluate(units, dropout, lr, n_layers, 15)\n",
    "#     print(f\"units={units}, dropout={dropout}, lr={lr}, n_layers={n_layers}, val_loss={val_loss:.4f}, val_acc={val_acc:.4f}\")\n",
    "#     results.append({'units': units, 'dropout': dropout, 'lr': lr, 'n_layers': n_layers, 'val_loss': val_loss, 'val_acc': val_acc})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "b23ef10a-a4bb-4f8e-b2d3-a72907c9f2de",
   "metadata": {},
   "outputs": [],
   "source": [
    "# best = min(results, key=lambda x: x['val_loss'])\n",
    "# print(f\"Best hyperparams: {best}\")\n",
    "# best_params = {k: best[k] for k in ['units', 'dropout', 'lr', 'n_layers']}"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5b1229bd",
   "metadata": {},
   "source": [
    "Best hyperparams: {'units': 64, 'dropout': 0.5, 'lr': 0.005, 'n_layers': 3, 'val_loss': 0.6605579302090556, 'val_acc': 0.6722689075630253}\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0485b072-0eef-4b88-be82-73860e0059ba",
   "metadata": {},
   "outputs": [],
   "source": [
    "# X_combined_soccer = torch.cat([X_train_tensor_soccer, X_val_tensor_soccer], dim=0)\n",
    "# y_combined_soccer = torch.cat([y_train_tensor_soccer, y_val_tensor_soccer], dim=0)\n",
    "# combined_dataset_soccer = TensorDataset(X_combined_soccer, y_combined_soccer)\n",
    "# combined_loader_soccer = DataLoader(combined_dataset_soccer, batch_size=32, shuffle=True)\n",
    "\n",
    "# # Create new model using best hyperparameters\n",
    "# final_model = SimpleNN(\n",
    "#     input_dim=X_train_tensor_soccer.shape[1],\n",
    "#     units=64,\n",
    "#     dropout=0.5,\n",
    "#     n_layers=3,\n",
    "#     num_classes=3\n",
    "# )\n",
    "# optimizer = torch.optim.AdamW(final_model.parameters(), lr=0.005)\n",
    "# criterion = torch.nn.CrossEntropyLoss()\n",
    "\n",
    "# train_losses = []\n",
    "# train_accuracies = []\n",
    "\n",
    "# for epoch in range(50):\n",
    "#     final_model.train()\n",
    "#     running_loss = 0\n",
    "#     correct = 0\n",
    "#     total = 0\n",
    "#     for xb, yb in combined_loader_soccer:\n",
    "#         optimizer.zero_grad()\n",
    "#         preds = final_model(xb)  \n",
    "#         loss = criterion(preds, yb)\n",
    "#         loss.backward()\n",
    "#         optimizer.step()\n",
    "#         running_loss += loss.item() * xb.size(0)\n",
    "#         predicted_classes = preds.argmax(dim=1)\n",
    "#         correct += (predicted_classes == yb).sum().item()\n",
    "#         total += yb.size(0)\n",
    "#     epoch_loss = running_loss / total\n",
    "#     epoch_acc = correct / total\n",
    "#     train_losses.append(epoch_loss)\n",
    "#     train_accuracies.append(epoch_acc)\n",
    "#     print(f\"Epoch {epoch+1}, Loss: {epoch_loss:.4f}, Accuracy: {epoch_acc:.4f}\")\n",
    "\n",
    "# torch.save(final_model.state_dict(), \"models/final_model_soccer.pth\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c68660d3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "         low       0.80      0.76      0.78        46\n",
      "      medium       0.93      0.74      0.82        50\n",
      "        high       0.68      0.83      0.75        53\n",
      "\n",
      "    accuracy                           0.78       149\n",
      "   macro avg       0.80      0.78      0.78       149\n",
      "weighted avg       0.80      0.78      0.78       149\n",
      "\n"
     ]
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 500x300 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy: 0.7785\n",
      "F1 Score: 0.7813\n",
      "Recall Score: 0.7785\n"
     ]
    }
   ],
   "source": [
    "final_model = SimpleNN(\n",
    "    input_dim=X_train_tensor_soccer.shape[1],\n",
    "    units=64,\n",
    "    dropout=0.5,\n",
    "    n_layers=3,\n",
    "    num_classes=3\n",
    ")\n",
    "final_model.load_state_dict(torch.load(\"models/final_model_soccer.pth\"))\n",
    "\n",
    "final_model.eval()\n",
    "all_preds = []\n",
    "all_labels = []\n",
    "all_probs = []\n",
    "\n",
    "with torch.no_grad():\n",
    "    for xb, yb in test_loader_soccer:\n",
    "        preds = final_model(xb)\n",
    "        predicted_classes = preds.argmax(dim=1)\n",
    "        logits = final_model(xb)               # raw outputs (logits)\n",
    "        probs = torch.softmax(logits, dim=1)  # convert logits to probabilities\n",
    "        all_probs.append(probs.cpu().numpy())\n",
    "        all_preds.extend(predicted_classes.cpu().numpy())\n",
    "        all_labels.extend(yb.cpu().numpy())\n",
    "\n",
    "all_probs = np.vstack(all_probs)\n",
    "\n",
    "idx_to_label = {0: \"low\", 1: \"medium\", 2: \"high\"}\n",
    "tier_order = [\"low\", \"medium\", \"high\"]\n",
    "\n",
    "# Map numeric labels back to strings\n",
    "all_labels_str = [idx_to_label[idx] for idx in all_labels]\n",
    "all_preds_str = [idx_to_label[idx] for idx in all_preds]\n",
    "all_labels_bin = label_binarize(all_labels, classes=[0,1,2])\n",
    "print(classification_report(all_labels_str, all_preds_str, target_names=tier_order))\n",
    "\n",
    "accuracy = accuracy_score(all_labels_str, all_preds_str)\n",
    "f1 = f1_score(all_labels_str, all_preds_str, average=\"weighted\")\n",
    "recall = recall_score(all_labels_str, all_preds_str, average=\"weighted\")\n",
    "prec = precision_score(all_labels_str, all_preds_str, average=\"weighted\")\n",
    "roc_auc = roc_auc_score(all_labels_bin, all_probs)\n",
    "avg_prec = average_precision_score(all_labels_bin, all_probs)\n",
    "\n",
    "cm_soccer = confusion_matrix(all_labels_str, all_preds_str, labels=tier_order)\n",
    "fig, ax = plt.subplots(figsize=(5,3))\n",
    "disp_soccer = ConfusionMatrixDisplay(confusion_matrix=cm_soccer, display_labels=tier_order)\n",
    "disp_soccer.plot(ax=ax, cmap=\"plasma\", colorbar=False)\n",
    "disp_soccer.ax_.set_title('Confusion Matrix for NN - Soccer')\n",
    "\n",
    "textstr = f'Accuracy: {accuracy:.2f}\\nF1 Score: {f1:.2f}\\nRecall: {recall:.2f}\\nPrecision: {prec:.2f}\\nROC-AUC: {roc_auc:.2f}\\nAvg Prec: {avg_prec:.2f}'\n",
    "ax.text(1.05, 0.5, textstr, transform=ax.transAxes, fontsize=10, verticalalignment='center')\n",
    "plt.tight_layout()\n",
    "plt.savefig(\"images/5/cm_nn_soccer.png\", dpi=300, bbox_inches='tight')#, facecolor=\"#E7E7E7\")\n",
    "plt.show()\n",
    "\n",
    "print(f'Accuracy: {accuracy:.4f}')\n",
    "print(f'F1 Score: {f1:.4f}')\n",
    "print(f'Recall Score: {recall:.4f}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "809a71cc-48d6-4116-a47b-ddf55b8b37e6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Underpriced Candidates:\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>title</th>\n",
       "      <th>price.value</th>\n",
       "      <th>seller.feedbackScore</th>\n",
       "      <th>days_listed</th>\n",
       "      <th>title_length</th>\n",
       "      <th>true_label</th>\n",
       "      <th>predicted_label</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>896</th>\n",
       "      <td>Vitesse 2014-15 Home jersey mens size M macron</td>\n",
       "      <td>29.00</td>\n",
       "      <td>1477</td>\n",
       "      <td>186</td>\n",
       "      <td>46</td>\n",
       "      <td>low</td>\n",
       "      <td>high</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>458</th>\n",
       "      <td>CHELSEA 2006/2008 HOME FOOTBALL SHIRT KIT SOCC...</td>\n",
       "      <td>29.99</td>\n",
       "      <td>2017</td>\n",
       "      <td>76</td>\n",
       "      <td>80</td>\n",
       "      <td>low</td>\n",
       "      <td>high</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                 title  price.value  \\\n",
       "896     Vitesse 2014-15 Home jersey mens size M macron        29.00   \n",
       "458  CHELSEA 2006/2008 HOME FOOTBALL SHIRT KIT SOCC...        29.99   \n",
       "\n",
       "     seller.feedbackScore  days_listed  title_length true_label  \\\n",
       "896                  1477          186            46        low   \n",
       "458                  2017           76            80        low   \n",
       "\n",
       "    predicted_label  \n",
       "896            high  \n",
       "458            high  "
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Overpriced Candidates:\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>title</th>\n",
       "      <th>price.value</th>\n",
       "      <th>seller.feedbackScore</th>\n",
       "      <th>days_listed</th>\n",
       "      <th>title_length</th>\n",
       "      <th>true_label</th>\n",
       "      <th>predicted_label</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>518</th>\n",
       "      <td>Adidas Spain #6 A. Iniesta 2010 Away Soccer Je...</td>\n",
       "      <td>98.0</td>\n",
       "      <td>2582</td>\n",
       "      <td>8</td>\n",
       "      <td>80</td>\n",
       "      <td>high</td>\n",
       "      <td>low</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                 title  price.value  \\\n",
       "518  Adidas Spain #6 A. Iniesta 2010 Away Soccer Je...         98.0   \n",
       "\n",
       "     seller.feedbackScore  days_listed  title_length true_label  \\\n",
       "518                  2582            8            80       high   \n",
       "\n",
       "    predicted_label  \n",
       "518             low  "
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "labels_df = pd.DataFrame({\n",
    "    \"true_label\": all_labels_str,\n",
    "    \"predicted_label\": all_preds_str\n",
    "}, index=y_test_soccer.index)\n",
    "\n",
    "# Merge labels with original soccer dataframe on index\n",
    "merged = df_soccer.loc[y_test_soccer.index].copy()\n",
    "merged = merged.join(labels_df)\n",
    "\n",
    "# Identify underpriced and overpriced cases based on true vs predicted\n",
    "underpriced = merged[(merged[\"true_label\"] == \"low\") & (merged[\"predicted_label\"] == \"high\")]\n",
    "overpriced = merged[(merged[\"true_label\"] == \"high\") & (merged[\"predicted_label\"] == \"low\")]\n",
    "\n",
    "print(\"Underpriced Candidates:\")\n",
    "display(underpriced[[\"title\", \"price.value\", \"seller.feedbackScore\", \"days_listed\", \"title_length\", \"true_label\", \"predicted_label\"]])\n",
    "\n",
    "print(\"Overpriced Candidates:\")\n",
    "display(overpriced[[\"title\", \"price.value\", \"seller.feedbackScore\", \"days_listed\", \"title_length\", \"true_label\", \"predicted_label\"]])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "523b7190",
   "metadata": {},
   "source": [
    "#### Regression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "f250bfd6-3095-41f9-937f-b2ba6effd037",
   "metadata": {},
   "outputs": [],
   "source": [
    "y_soccer = df_soccer['price.value']\n",
    "                         \n",
    "X_train_soccer, X_test_soccer, y_train_soccer, y_test_soccer = train_test_split(X_encoded_soccer, y_soccer,\n",
    "                                                                 test_size=0.20, random_state=1216) \n",
    "\n",
    "X_train_soccer, X_val_soccer, y_train_soccer, y_val_soccer = train_test_split(X_train_soccer, y_train_soccer,\n",
    "                                                                test_size=0.20, random_state=1216)\n",
    "\n",
    "y_train_tensor_soccer = torch.tensor(y_train_soccer.values, dtype=torch.float32).view(-1, 1)\n",
    "y_val_tensor_soccer = torch.tensor(y_val_soccer.values, dtype=torch.float32).view(-1, 1)\n",
    "y_test_tensor_soccer = torch.tensor(y_test_soccer.values, dtype=torch.float32).view(-1, 1)\n",
    "\n",
    "train_dataset_soccer = TensorDataset(X_train_tensor_soccer, y_train_tensor_soccer)\n",
    "val_dataset_soccer = TensorDataset(X_val_tensor_soccer, y_val_tensor_soccer)\n",
    "test_dataset_soccer = TensorDataset(X_test_tensor_soccer, y_test_tensor_soccer)\n",
    "\n",
    "train_loader_soccer = DataLoader(train_dataset_soccer, batch_size=32, shuffle=True)\n",
    "val_loader_soccer = DataLoader(val_dataset_soccer, batch_size=32, shuffle=False)\n",
    "test_loader_soccer = DataLoader(test_dataset_soccer, batch_size=32, shuffle=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "a0228caf-d273-4c89-ad83-7b3a7f7580c8",
   "metadata": {},
   "outputs": [],
   "source": [
    "class SimpleNN(nn.Module):\n",
    "    def __init__(self, input_dim, units, dropout, n_layers):\n",
    "        super(SimpleNN, self).__init__()\n",
    "        layers = []\n",
    "        # First layer\n",
    "        layers.append(nn.Linear(input_dim, units))\n",
    "        layers.append(nn.BatchNorm1d(units)) \n",
    "        layers.append(nn.ReLU())\n",
    "        layers.append(nn.Dropout(dropout))\n",
    "        # Hidden layers\n",
    "        for _ in range(n_layers - 1):\n",
    "            layers.append(nn.Linear(units, units))\n",
    "            layers.append(nn.BatchNorm1d(units)) \n",
    "            layers.append(nn.ReLU())\n",
    "            layers.append(nn.Dropout(dropout))\n",
    "        layers.append(nn.Linear(units, 1))  \n",
    "        self.net = nn.Sequential(*layers)\n",
    "\n",
    "    def forward(self, x):\n",
    "        return self.net(x)\n",
    "    \n",
    "def train_and_evaluate(units, dropout, lr, n_layers, epochs=10):\n",
    "    model = SimpleNN(input_dim=X_train_tensor_soccer.shape[1], units=units, dropout=dropout, n_layers=n_layers)\n",
    "    criterion = torch.nn.MSELoss()\n",
    "    optimizer = torch.optim.AdamW(model.parameters(), lr=lr)\n",
    "\n",
    "    for epoch in range(epochs):\n",
    "        model.train()\n",
    "        for xb, yb in train_loader_soccer:\n",
    "            optimizer.zero_grad()\n",
    "            preds = model(xb)\n",
    "            loss = criterion(preds, yb)\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "\n",
    "        # Validation per epoch\n",
    "        model.eval()\n",
    "        val_loss = 0\n",
    "        val_total = 0\n",
    "        with torch.no_grad():\n",
    "            for xb, yb in val_loader_soccer:\n",
    "                preds = model(xb)\n",
    "                loss = criterion(preds, yb)\n",
    "                val_loss += loss.item() * xb.size(0)\n",
    "                val_total += yb.size(0)\n",
    "        val_loss /= val_total\n",
    "        print(f\"Epoch {epoch+1}, Val Loss: {val_loss:.4f}\")\n",
    "    return val_loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "id": "56042084-fbfb-405e-8cc6-4669fbad61e0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# from itertools import product\n",
    "# param_grid = {\n",
    "#     'units': [32, 64],\n",
    "#     'dropout': [0.3, 0.4, 0.5],\n",
    "#     'learning_rate': [0.0005, 0.001, 0.005, 0.01],\n",
    "#     'n_layers': [1, 2, 3]\n",
    "# }\n",
    "# results = []\n",
    "# for units, dropout, lr, n_layers in product(param_grid['units'], param_grid['dropout'], param_grid['learning_rate'],\n",
    "#                                  param_grid['n_layers']):\n",
    "#     val_loss = train_and_evaluate(units, dropout, lr, n_layers, 15)\n",
    "#     print(f\"units={units}, dropout={dropout}, lr={lr}, n_layers={n_layers}, val_loss={val_loss:.4f}\")\n",
    "#     results.append({'units': units, 'dropout': dropout, 'lr': lr, 'n_layers': n_layers, 'val_loss': val_loss})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "id": "980a48da-a04b-4cbe-916d-6a0e31d70274",
   "metadata": {},
   "outputs": [],
   "source": [
    "# best = min(results, key=lambda x: x['val_loss'])\n",
    "# print(f\"Best hyperparams: {best}\")\n",
    "# best_params = {k: best[k] for k in ['units', 'dropout', 'lr', 'n_layers']}"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3fa42942",
   "metadata": {},
   "source": [
    "Best hyperparams: {'units': 32, 'dropout': 0.3, 'lr': 0.01, 'n_layers': 3, 'val_loss': 393.70084746545103}\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "20d864ab-ba3c-457d-94ea-99afc120e687",
   "metadata": {},
   "outputs": [],
   "source": [
    "# X_combined_soccer = torch.cat([X_train_tensor_soccer, X_val_tensor_soccer], dim=0)\n",
    "# y_combined_soccer = torch.cat([y_train_tensor_soccer, y_val_tensor_soccer], dim=0)\n",
    "# combined_dataset_soccer = TensorDataset(X_combined_soccer, y_combined_soccer)\n",
    "# combined_loader_soccer = DataLoader(combined_dataset_soccer, batch_size=32, shuffle=True)\n",
    "\n",
    "# # Create new model using best hyperparameters\n",
    "# final_model = SimpleNN(\n",
    "#     input_dim=X_train_tensor_soccer.shape[1],\n",
    "#     units=32,\n",
    "#     dropout=0.3,\n",
    "#     n_layers=3,\n",
    "# )\n",
    "# optimizer = torch.optim.AdamW(final_model.parameters(), lr=0.01)\n",
    "# criterion = torch.nn.MSELoss()\n",
    "\n",
    "# train_losses = []\n",
    "\n",
    "# for epoch in range(50):\n",
    "#     final_model.train()\n",
    "#     running_loss = 0\n",
    "#     total = 0\n",
    "#     for xb, yb in combined_loader_soccer:\n",
    "#         optimizer.zero_grad()\n",
    "#         preds = final_model(xb)  # shape: [batch_size, num_classes]\n",
    "#         loss = criterion(preds, yb)\n",
    "#         loss.backward()\n",
    "#         optimizer.step()\n",
    "#         running_loss += loss.item() * xb.size(0)\n",
    "#         total += yb.size(0)\n",
    "#     epoch_loss = running_loss / total\n",
    "#     train_losses.append(epoch_loss)\n",
    "#     print(f\"Epoch {epoch+1}, Loss: {epoch_loss:.4f}\")\n",
    "\n",
    "# torch.save(final_model.state_dict(), \"models/final_model_soccer_reg.pth\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "19ccfcf2-0abf-4c87-a32e-6593f3bbce70",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test MSE: 561.44, RMSE: 23.69\n"
     ]
    }
   ],
   "source": [
    "final_model = SimpleNN(\n",
    "    input_dim=X_train_tensor_soccer.shape[1],\n",
    "    units=32,\n",
    "    dropout=0.3,\n",
    "    n_layers=3,\n",
    ")\n",
    "final_model.load_state_dict(torch.load(\"models/final_model_soccer_reg.pth\"))\n",
    "criterion = torch.nn.MSELoss()\n",
    "\n",
    "final_model.eval()\n",
    "with torch.no_grad():\n",
    "    preds = final_model(X_test_tensor_soccer)\n",
    "    mse = criterion(preds, y_test_tensor_soccer).item()\n",
    "    rmse = mse ** 0.5\n",
    "    print(f\"Test MSE: {mse:.2f}, RMSE: {rmse:.2f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "04eceb2f-2862-4a91-a232-e6230752848d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Top 10 biggest misses:\n",
      "                                                 title          club  \\\n",
      "126   Puma Chivas Retro Street 2025 Chivas Jersey 2025           NaN   \n",
      "518  Adidas Spain #6 A. Iniesta 2010 Away Soccer Je...           NaN   \n",
      "734  Barcelona FC Spain - Nike 2022/2023 Home Jerse...     Barcelona   \n",
      "321        La Liga Jersey Sleeve Badge Patch 2024-2025           NaN   \n",
      "705  Adidas Argentina National Football Team Soccer...           NaN   \n",
      "388  Peru Men's 2018 Umbro Red/White Replica Soccer...           NaN   \n",
      "458  CHELSEA 2006/2008 HOME FOOTBALL SHIRT KIT SOCC...       Chelsea   \n",
      "938  Club America 24/25 Third Dri Fit Adv Nike Socc...  Club America   \n",
      "607  Official Japan 2022 Home Jersey Shirt - Mitoma...           NaN   \n",
      "856  Costa Rica Lotto 2014 World Cup Soccer Jersey ...           NaN   \n",
      "\n",
      "       country      condition  Predicted  Actual       Error  \\\n",
      "126        NaN  New with tags  43.517834  279.99 -236.472166   \n",
      "518      Spain           Used  39.213799   98.00  -58.786201   \n",
      "734      Spain            New  72.505219  120.00  -47.494781   \n",
      "321        NaN            New  47.645702    8.00   39.645702   \n",
      "705  Argentina            New  62.366676  100.00  -37.633324   \n",
      "388        NaN            New  48.557644   85.00  -36.442356   \n",
      "458        NaN           Used  61.732376   29.99   31.742376   \n",
      "938        NaN            New  54.075001   84.95  -30.874999   \n",
      "607        NaN           Used  74.029221   99.00  -24.970779   \n",
      "856        NaN           Used  54.488083   30.00   24.488083   \n",
      "\n",
      "                   Price_Tag  \n",
      "126   Potentially Overpriced  \n",
      "518   Potentially Overpriced  \n",
      "734   Potentially Overpriced  \n",
      "321  Potentially Underpriced  \n",
      "705   Potentially Overpriced  \n",
      "388   Potentially Overpriced  \n",
      "458  Potentially Underpriced  \n",
      "938   Potentially Overpriced  \n",
      "607   Potentially Overpriced  \n",
      "856  Potentially Underpriced  \n"
     ]
    }
   ],
   "source": [
    "final_model = SimpleNN(\n",
    "    input_dim=X_train_tensor_soccer.shape[1],\n",
    "    units=32,\n",
    "    dropout=0.3,\n",
    "    n_layers=3,\n",
    ")\n",
    "final_model.load_state_dict(torch.load(\"models/final_model_soccer_reg.pth\"))\n",
    "\n",
    "final_model.eval()\n",
    "with torch.no_grad():\n",
    "    preds = final_model(X_test_tensor_soccer).cpu().numpy().flatten()\n",
    "    targets = y_test_soccer.values.flatten()\n",
    "\n",
    "errors = preds - targets   # positive = underpriced, negative = overpriced\n",
    "abs_errors = np.abs(errors)\n",
    "\n",
    "top_n = 10\n",
    "biggest_misses_pos = np.argsort(abs_errors)[-top_n:][::-1]  # positions in test set\n",
    "orig_idx = X_test_soccer.index[biggest_misses_pos]           # original df_soccer index\n",
    "\n",
    "\n",
    "misses_df = df_soccer.loc[orig_idx, [\"title\", \"club\", \"country\", \"condition\"]].copy()\n",
    "misses_df[\"Predicted\"] = preds[biggest_misses_pos]\n",
    "misses_df[\"Actual\"] = targets[biggest_misses_pos]\n",
    "misses_df[\"Error\"] = errors[biggest_misses_pos]\n",
    "misses_df[\"Price_Tag\"] = np.where(\n",
    "    misses_df[\"Error\"] > 0,\n",
    "    \"Potentially Underpriced\",\n",
    "    \"Potentially Overpriced\"\n",
    ")\n",
    "\n",
    "print(f\"Top {top_n} biggest misses:\")\n",
    "print(misses_df.sort_values(by=\"Error\", key=np.abs, ascending=False))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "id": "1fdaf8bb",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>title</th>\n",
       "      <th>club</th>\n",
       "      <th>country</th>\n",
       "      <th>condition</th>\n",
       "      <th>Predicted</th>\n",
       "      <th>Actual</th>\n",
       "      <th>Error</th>\n",
       "      <th>Price_Tag</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>126</th>\n",
       "      <td>Puma Chivas Retro Street 2025 Chivas Jersey 2025</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>New with tags</td>\n",
       "      <td>43.517834</td>\n",
       "      <td>279.99</td>\n",
       "      <td>-236.472166</td>\n",
       "      <td>Potentially Overpriced</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>518</th>\n",
       "      <td>Adidas Spain #6 A. Iniesta 2010 Away Soccer Je...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Spain</td>\n",
       "      <td>Used</td>\n",
       "      <td>39.213799</td>\n",
       "      <td>98.00</td>\n",
       "      <td>-58.786201</td>\n",
       "      <td>Potentially Overpriced</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>734</th>\n",
       "      <td>Barcelona FC Spain - Nike 2022/2023 Home Jerse...</td>\n",
       "      <td>Barcelona</td>\n",
       "      <td>Spain</td>\n",
       "      <td>New</td>\n",
       "      <td>72.505219</td>\n",
       "      <td>120.00</td>\n",
       "      <td>-47.494781</td>\n",
       "      <td>Potentially Overpriced</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>321</th>\n",
       "      <td>La Liga Jersey Sleeve Badge Patch 2024-2025</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>New</td>\n",
       "      <td>47.645702</td>\n",
       "      <td>8.00</td>\n",
       "      <td>39.645702</td>\n",
       "      <td>Potentially Underpriced</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>705</th>\n",
       "      <td>Adidas Argentina National Football Team Soccer...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Argentina</td>\n",
       "      <td>New</td>\n",
       "      <td>62.366676</td>\n",
       "      <td>100.00</td>\n",
       "      <td>-37.633324</td>\n",
       "      <td>Potentially Overpriced</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>388</th>\n",
       "      <td>Peru Men's 2018 Umbro Red/White Replica Soccer...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>New</td>\n",
       "      <td>48.557644</td>\n",
       "      <td>85.00</td>\n",
       "      <td>-36.442356</td>\n",
       "      <td>Potentially Overpriced</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>458</th>\n",
       "      <td>CHELSEA 2006/2008 HOME FOOTBALL SHIRT KIT SOCC...</td>\n",
       "      <td>Chelsea</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Used</td>\n",
       "      <td>61.732376</td>\n",
       "      <td>29.99</td>\n",
       "      <td>31.742376</td>\n",
       "      <td>Potentially Underpriced</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>938</th>\n",
       "      <td>Club America 24/25 Third Dri Fit Adv Nike Socc...</td>\n",
       "      <td>Club America</td>\n",
       "      <td>NaN</td>\n",
       "      <td>New</td>\n",
       "      <td>54.075001</td>\n",
       "      <td>84.95</td>\n",
       "      <td>-30.874999</td>\n",
       "      <td>Potentially Overpriced</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>607</th>\n",
       "      <td>Official Japan 2022 Home Jersey Shirt - Mitoma...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Used</td>\n",
       "      <td>74.029221</td>\n",
       "      <td>99.00</td>\n",
       "      <td>-24.970779</td>\n",
       "      <td>Potentially Overpriced</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>856</th>\n",
       "      <td>Costa Rica Lotto 2014 World Cup Soccer Jersey ...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Used</td>\n",
       "      <td>54.488083</td>\n",
       "      <td>30.00</td>\n",
       "      <td>24.488083</td>\n",
       "      <td>Potentially Underpriced</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                 title          club  \\\n",
       "126   Puma Chivas Retro Street 2025 Chivas Jersey 2025           NaN   \n",
       "518  Adidas Spain #6 A. Iniesta 2010 Away Soccer Je...           NaN   \n",
       "734  Barcelona FC Spain - Nike 2022/2023 Home Jerse...     Barcelona   \n",
       "321        La Liga Jersey Sleeve Badge Patch 2024-2025           NaN   \n",
       "705  Adidas Argentina National Football Team Soccer...           NaN   \n",
       "388  Peru Men's 2018 Umbro Red/White Replica Soccer...           NaN   \n",
       "458  CHELSEA 2006/2008 HOME FOOTBALL SHIRT KIT SOCC...       Chelsea   \n",
       "938  Club America 24/25 Third Dri Fit Adv Nike Socc...  Club America   \n",
       "607  Official Japan 2022 Home Jersey Shirt - Mitoma...           NaN   \n",
       "856  Costa Rica Lotto 2014 World Cup Soccer Jersey ...           NaN   \n",
       "\n",
       "       country      condition  Predicted  Actual       Error  \\\n",
       "126        NaN  New with tags  43.517834  279.99 -236.472166   \n",
       "518      Spain           Used  39.213799   98.00  -58.786201   \n",
       "734      Spain            New  72.505219  120.00  -47.494781   \n",
       "321        NaN            New  47.645702    8.00   39.645702   \n",
       "705  Argentina            New  62.366676  100.00  -37.633324   \n",
       "388        NaN            New  48.557644   85.00  -36.442356   \n",
       "458        NaN           Used  61.732376   29.99   31.742376   \n",
       "938        NaN            New  54.075001   84.95  -30.874999   \n",
       "607        NaN           Used  74.029221   99.00  -24.970779   \n",
       "856        NaN           Used  54.488083   30.00   24.488083   \n",
       "\n",
       "                   Price_Tag  \n",
       "126   Potentially Overpriced  \n",
       "518   Potentially Overpriced  \n",
       "734   Potentially Overpriced  \n",
       "321  Potentially Underpriced  \n",
       "705   Potentially Overpriced  \n",
       "388   Potentially Overpriced  \n",
       "458  Potentially Underpriced  \n",
       "938   Potentially Overpriced  \n",
       "607   Potentially Overpriced  \n",
       "856  Potentially Underpriced  "
      ]
     },
     "execution_count": 50,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "misses_df.sort_values(by=\"Error\", key=np.abs, ascending=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b6b280c2",
   "metadata": {},
   "source": [
    "#### Log Price regression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "id": "fb5b4880-5468-418b-ab24-344abc690157",
   "metadata": {},
   "outputs": [],
   "source": [
    "y_soccer = df_soccer['price.value']\n",
    "                         \n",
    "X_train_soccer, X_test_soccer, y_train_soccer, y_test_soccer = train_test_split(X_encoded_soccer, y_soccer,\n",
    "                                                                 test_size=0.20, random_state=1216) \n",
    "\n",
    "X_train_soccer, X_val_soccer, y_train_soccer, y_val_soccer = train_test_split(X_train_soccer, y_train_soccer,\n",
    "                                                                test_size=0.20, random_state=1216)\n",
    "\n",
    "y_train_log = np.log(y_train_soccer.values)\n",
    "y_val_log = np.log(y_val_soccer.values)\n",
    "y_test_log = np.log(y_test_soccer.values)\n",
    "\n",
    "y_train_tensor_soccer_log = torch.tensor(y_train_log, dtype=torch.float32).view(-1, 1)\n",
    "y_val_tensor_soccer_log = torch.tensor(y_val_log, dtype=torch.float32).view(-1, 1)\n",
    "y_test_tensor_soccer_log = torch.tensor(y_test_log, dtype=torch.float32).view(-1, 1)\n",
    "\n",
    "train_dataset_soccer_log = TensorDataset(X_train_tensor_soccer, y_train_tensor_soccer_log)\n",
    "val_dataset_soccer_log = TensorDataset(X_val_tensor_soccer, y_val_tensor_soccer_log)\n",
    "test_dataset_soccer_log = TensorDataset(X_test_tensor_soccer, y_test_tensor_soccer_log)\n",
    "\n",
    "train_loader_soccer_log = DataLoader(train_dataset_soccer_log, batch_size=32, shuffle=True)\n",
    "val_loader_soccer_log = DataLoader(val_dataset_soccer_log, batch_size=32, shuffle=False)\n",
    "test_loader_soccer_log = DataLoader(test_dataset_soccer_log, batch_size=32, shuffle=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "id": "36f96f6c-d9bc-4eae-bacb-46cf0ef8f498",
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.optim.lr_scheduler import ReduceLROnPlateau\n",
    "\n",
    "def train_and_evaluate(units, dropout, lr, n_layers, epochs=30, patience=5):\n",
    "    model = SimpleNN(input_dim=X_train_tensor_soccer.shape[1], units=units, dropout=dropout, n_layers=n_layers)\n",
    "    criterion = torch.nn.MSELoss()\n",
    "    optimizer = torch.optim.AdamW(model.parameters(), lr=lr)\n",
    "    scheduler = ReduceLROnPlateau(optimizer, 'min', patience=2, factor=0.5)\n",
    "    \n",
    "    best_val_loss = float('inf')\n",
    "    epochs_no_improve = 0\n",
    "\n",
    "    for epoch in range(epochs):\n",
    "        model.train()\n",
    "        for xb, yb in train_loader_soccer_log:\n",
    "            optimizer.zero_grad()\n",
    "            preds = model(xb)\n",
    "            loss = criterion(preds, yb)\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "\n",
    "        # Validation\n",
    "        model.eval()\n",
    "        val_loss = 0\n",
    "        val_total = 0\n",
    "        with torch.no_grad():\n",
    "            for xb, yb in val_loader_soccer_log:\n",
    "                preds = model(xb)\n",
    "                loss = criterion(preds, yb)\n",
    "                val_loss += loss.item() * xb.size(0)\n",
    "                val_total += yb.size(0)\n",
    "        val_loss /= val_total\n",
    "        print(f\"Epoch {epoch+1}, Val Loss: {val_loss:.4f}\")\n",
    "\n",
    "        scheduler.step(val_loss)\n",
    "\n",
    "        # Early stopping\n",
    "        if val_loss < best_val_loss:\n",
    "            best_val_loss = val_loss\n",
    "            epochs_no_improve = 0\n",
    "        else:\n",
    "            epochs_no_improve += 1\n",
    "            if epochs_no_improve >= patience:\n",
    "                print(\"Early stopping!\")\n",
    "                break\n",
    "    return best_val_loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "id": "c6713568-8ce0-43dc-ad4e-15be15459a78",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1, Val Loss: 15.9464\n",
      "Epoch 2, Val Loss: 15.3721\n",
      "Epoch 3, Val Loss: 14.5996\n",
      "Epoch 4, Val Loss: 13.7892\n",
      "Epoch 5, Val Loss: 12.9066\n",
      "Epoch 6, Val Loss: 12.1414\n",
      "Epoch 7, Val Loss: 11.3632\n",
      "Epoch 8, Val Loss: 10.5610\n",
      "Epoch 9, Val Loss: 9.8603\n",
      "Epoch 10, Val Loss: 9.1857\n",
      "units=32, dropout=0.3, lr=0.0005, n_layers=1, val_loss=9.1857\n",
      "Epoch 1, Val Loss: 14.7862\n",
      "Epoch 2, Val Loss: 14.4776\n",
      "Epoch 3, Val Loss: 13.8569\n",
      "Epoch 4, Val Loss: 13.2439\n",
      "Epoch 5, Val Loss: 12.6670\n",
      "Epoch 6, Val Loss: 12.0756\n",
      "Epoch 7, Val Loss: 11.3982\n",
      "Epoch 8, Val Loss: 10.7673\n",
      "Epoch 9, Val Loss: 10.0875\n",
      "Epoch 10, Val Loss: 9.5817\n",
      "units=32, dropout=0.3, lr=0.0005, n_layers=2, val_loss=9.5817\n",
      "Epoch 1, Val Loss: 14.6377\n",
      "Epoch 2, Val Loss: 13.6439\n",
      "Epoch 3, Val Loss: 12.5928\n",
      "Epoch 4, Val Loss: 12.1022\n",
      "Epoch 5, Val Loss: 11.3627\n",
      "Epoch 6, Val Loss: 10.9768\n",
      "Epoch 7, Val Loss: 10.3362\n",
      "Epoch 8, Val Loss: 9.6682\n",
      "Epoch 9, Val Loss: 9.1612\n",
      "Epoch 10, Val Loss: 8.5691\n",
      "units=32, dropout=0.3, lr=0.0005, n_layers=3, val_loss=8.5691\n",
      "Epoch 1, Val Loss: 14.3066\n",
      "Epoch 2, Val Loss: 12.0638\n",
      "Epoch 3, Val Loss: 9.8570\n",
      "Epoch 4, Val Loss: 8.2252\n",
      "Epoch 5, Val Loss: 6.8949\n",
      "Epoch 6, Val Loss: 5.5867\n",
      "Epoch 7, Val Loss: 4.5051\n",
      "Epoch 8, Val Loss: 3.6293\n",
      "Epoch 9, Val Loss: 2.9036\n",
      "Epoch 10, Val Loss: 2.2637\n",
      "units=32, dropout=0.3, lr=0.001, n_layers=1, val_loss=2.2637\n",
      "Epoch 1, Val Loss: 14.8207\n",
      "Epoch 2, Val Loss: 12.9968\n",
      "Epoch 3, Val Loss: 11.0600\n",
      "Epoch 4, Val Loss: 9.6274\n",
      "Epoch 5, Val Loss: 8.4054\n",
      "Epoch 6, Val Loss: 7.2199\n",
      "Epoch 7, Val Loss: 5.9841\n",
      "Epoch 8, Val Loss: 5.0153\n",
      "Epoch 9, Val Loss: 4.3172\n",
      "Epoch 10, Val Loss: 3.5891\n",
      "units=32, dropout=0.3, lr=0.001, n_layers=2, val_loss=3.5891\n",
      "Epoch 1, Val Loss: 14.3249\n",
      "Epoch 2, Val Loss: 12.9961\n",
      "Epoch 3, Val Loss: 11.2709\n",
      "Epoch 4, Val Loss: 9.7575\n",
      "Epoch 5, Val Loss: 8.7659\n",
      "Epoch 6, Val Loss: 7.4912\n",
      "Epoch 7, Val Loss: 6.3912\n",
      "Epoch 8, Val Loss: 5.2940\n",
      "Epoch 9, Val Loss: 4.4943\n",
      "Epoch 10, Val Loss: 3.6342\n",
      "units=32, dropout=0.3, lr=0.001, n_layers=3, val_loss=3.6342\n",
      "Epoch 1, Val Loss: 11.0075\n",
      "Epoch 2, Val Loss: 4.1148\n",
      "Epoch 3, Val Loss: 0.7477\n",
      "Epoch 4, Val Loss: 0.4400\n",
      "Epoch 5, Val Loss: 0.3764\n",
      "Epoch 6, Val Loss: 0.4302\n",
      "Epoch 7, Val Loss: 0.4551\n",
      "Epoch 8, Val Loss: 0.3845\n",
      "Epoch 9, Val Loss: 0.3907\n",
      "Epoch 10, Val Loss: 0.3674\n",
      "units=32, dropout=0.3, lr=0.005, n_layers=1, val_loss=0.3674\n",
      "Epoch 1, Val Loss: 11.6702\n",
      "Epoch 2, Val Loss: 5.9960\n",
      "Epoch 3, Val Loss: 2.1651\n",
      "Epoch 4, Val Loss: 0.7369\n",
      "Epoch 5, Val Loss: 0.6792\n",
      "Epoch 6, Val Loss: 0.5693\n",
      "Epoch 7, Val Loss: 0.6302\n",
      "Epoch 8, Val Loss: 0.5360\n",
      "Epoch 9, Val Loss: 0.5209\n",
      "Epoch 10, Val Loss: 0.3987\n",
      "units=32, dropout=0.3, lr=0.005, n_layers=2, val_loss=0.3987\n",
      "Epoch 1, Val Loss: 11.3397\n",
      "Epoch 2, Val Loss: 6.3972\n",
      "Epoch 3, Val Loss: 2.0744\n",
      "Epoch 4, Val Loss: 0.6634\n",
      "Epoch 5, Val Loss: 0.4714\n",
      "Epoch 6, Val Loss: 0.7184\n",
      "Epoch 7, Val Loss: 0.5421\n",
      "Epoch 8, Val Loss: 0.5389\n",
      "Epoch 9, Val Loss: 0.4554\n",
      "Epoch 10, Val Loss: 0.3509\n",
      "units=32, dropout=0.3, lr=0.005, n_layers=3, val_loss=0.3509\n",
      "Epoch 1, Val Loss: 5.1783\n",
      "Epoch 2, Val Loss: 0.6207\n",
      "Epoch 3, Val Loss: 0.5945\n",
      "Epoch 4, Val Loss: 0.5327\n",
      "Epoch 5, Val Loss: 0.5452\n",
      "Epoch 6, Val Loss: 0.3890\n",
      "Epoch 7, Val Loss: 0.3783\n",
      "Epoch 8, Val Loss: 0.3018\n",
      "Epoch 9, Val Loss: 0.2674\n",
      "Epoch 10, Val Loss: 0.2666\n",
      "units=32, dropout=0.3, lr=0.01, n_layers=1, val_loss=0.2666\n",
      "Epoch 1, Val Loss: 6.6397\n",
      "Epoch 2, Val Loss: 0.9211\n",
      "Epoch 3, Val Loss: 0.4642\n",
      "Epoch 4, Val Loss: 0.6822\n",
      "Epoch 5, Val Loss: 0.4801\n",
      "Epoch 6, Val Loss: 0.6078\n",
      "Epoch 7, Val Loss: 0.4869\n",
      "Epoch 8, Val Loss: 0.4136\n",
      "Epoch 9, Val Loss: 0.4119\n",
      "Epoch 10, Val Loss: 0.3079\n",
      "units=32, dropout=0.3, lr=0.01, n_layers=2, val_loss=0.3079\n",
      "Epoch 1, Val Loss: 7.6197\n",
      "Epoch 2, Val Loss: 1.0815\n",
      "Epoch 3, Val Loss: 0.7176\n",
      "Epoch 4, Val Loss: 0.7142\n",
      "Epoch 5, Val Loss: 0.4537\n",
      "Epoch 6, Val Loss: 0.5073\n",
      "Epoch 7, Val Loss: 0.4972\n",
      "Epoch 8, Val Loss: 0.4512\n",
      "Epoch 9, Val Loss: 0.3126\n",
      "Epoch 10, Val Loss: 0.3952\n",
      "units=32, dropout=0.3, lr=0.01, n_layers=3, val_loss=0.3126\n",
      "Epoch 1, Val Loss: 12.8383\n",
      "Epoch 2, Val Loss: 11.8341\n",
      "Epoch 3, Val Loss: 10.8002\n",
      "Epoch 4, Val Loss: 9.9678\n",
      "Epoch 5, Val Loss: 9.1733\n",
      "Epoch 6, Val Loss: 8.4880\n",
      "Epoch 7, Val Loss: 7.8166\n",
      "Epoch 8, Val Loss: 7.2599\n",
      "Epoch 9, Val Loss: 6.6199\n",
      "Epoch 10, Val Loss: 6.0192\n",
      "units=32, dropout=0.4, lr=0.0005, n_layers=1, val_loss=6.0192\n",
      "Epoch 1, Val Loss: 16.2502\n",
      "Epoch 2, Val Loss: 15.5768\n",
      "Epoch 3, Val Loss: 14.9246\n",
      "Epoch 4, Val Loss: 14.1692\n",
      "Epoch 5, Val Loss: 13.5632\n",
      "Epoch 6, Val Loss: 12.9880\n",
      "Epoch 7, Val Loss: 12.3564\n",
      "Epoch 8, Val Loss: 11.8465\n",
      "Epoch 9, Val Loss: 11.3960\n",
      "Epoch 10, Val Loss: 10.9298\n",
      "units=32, dropout=0.4, lr=0.0005, n_layers=2, val_loss=10.9298\n",
      "Epoch 1, Val Loss: 13.9642\n",
      "Epoch 2, Val Loss: 13.3678\n",
      "Epoch 3, Val Loss: 12.6356\n",
      "Epoch 4, Val Loss: 11.9918\n",
      "Epoch 5, Val Loss: 11.4018\n",
      "Epoch 6, Val Loss: 10.9888\n",
      "Epoch 7, Val Loss: 10.3133\n",
      "Epoch 8, Val Loss: 9.7431\n",
      "Epoch 9, Val Loss: 9.2159\n",
      "Epoch 10, Val Loss: 8.7338\n",
      "units=32, dropout=0.4, lr=0.0005, n_layers=3, val_loss=8.7338\n",
      "Epoch 1, Val Loss: 14.7118\n",
      "Epoch 2, Val Loss: 13.6006\n",
      "Epoch 3, Val Loss: 12.1155\n",
      "Epoch 4, Val Loss: 10.6452\n",
      "Epoch 5, Val Loss: 9.3387\n",
      "Epoch 6, Val Loss: 8.1395\n",
      "Epoch 7, Val Loss: 6.9560\n",
      "Epoch 8, Val Loss: 5.8715\n",
      "Epoch 9, Val Loss: 4.8325\n",
      "Epoch 10, Val Loss: 4.0448\n",
      "units=32, dropout=0.4, lr=0.001, n_layers=1, val_loss=4.0448\n",
      "Epoch 1, Val Loss: 16.0996\n",
      "Epoch 2, Val Loss: 15.5883\n",
      "Epoch 3, Val Loss: 14.6939\n",
      "Epoch 4, Val Loss: 13.3806\n",
      "Epoch 5, Val Loss: 12.2568\n",
      "Epoch 6, Val Loss: 11.2762\n",
      "Epoch 7, Val Loss: 10.3891\n",
      "Epoch 8, Val Loss: 9.4991\n",
      "Epoch 9, Val Loss: 8.3033\n",
      "Epoch 10, Val Loss: 7.3576\n",
      "units=32, dropout=0.4, lr=0.001, n_layers=2, val_loss=7.3576\n",
      "Epoch 1, Val Loss: 13.3585\n",
      "Epoch 2, Val Loss: 12.1835\n",
      "Epoch 3, Val Loss: 10.9380\n",
      "Epoch 4, Val Loss: 10.0622\n",
      "Epoch 5, Val Loss: 9.0092\n",
      "Epoch 6, Val Loss: 8.0400\n",
      "Epoch 7, Val Loss: 7.0140\n",
      "Epoch 8, Val Loss: 5.8920\n",
      "Epoch 9, Val Loss: 4.7664\n",
      "Epoch 10, Val Loss: 4.0991\n",
      "units=32, dropout=0.4, lr=0.001, n_layers=3, val_loss=4.0991\n",
      "Epoch 1, Val Loss: 8.5505\n",
      "Epoch 2, Val Loss: 2.1912\n",
      "Epoch 3, Val Loss: 0.5427\n",
      "Epoch 4, Val Loss: 0.7406\n",
      "Epoch 5, Val Loss: 0.7030\n",
      "Epoch 6, Val Loss: 0.7551\n",
      "Epoch 7, Val Loss: 0.5383\n",
      "Epoch 8, Val Loss: 0.4466\n",
      "Epoch 9, Val Loss: 0.5253\n",
      "Epoch 10, Val Loss: 0.6655\n",
      "units=32, dropout=0.4, lr=0.005, n_layers=1, val_loss=0.4466\n",
      "Epoch 1, Val Loss: 13.3759\n",
      "Epoch 2, Val Loss: 8.5003\n",
      "Epoch 3, Val Loss: 4.2423\n",
      "Epoch 4, Val Loss: 1.8563\n",
      "Epoch 5, Val Loss: 0.8756\n",
      "Epoch 6, Val Loss: 0.9161\n",
      "Epoch 7, Val Loss: 1.1547\n",
      "Epoch 8, Val Loss: 0.8182\n",
      "Epoch 9, Val Loss: 0.5978\n",
      "Epoch 10, Val Loss: 1.0475\n",
      "units=32, dropout=0.4, lr=0.005, n_layers=2, val_loss=0.5978\n",
      "Epoch 1, Val Loss: 11.0549\n",
      "Epoch 2, Val Loss: 6.1156\n",
      "Epoch 3, Val Loss: 2.3125\n",
      "Epoch 4, Val Loss: 0.7076\n",
      "Epoch 5, Val Loss: 0.6779\n",
      "Epoch 6, Val Loss: 0.8170\n",
      "Epoch 7, Val Loss: 0.6950\n",
      "Epoch 8, Val Loss: 0.6238\n",
      "Epoch 9, Val Loss: 0.5704\n",
      "Epoch 10, Val Loss: 0.5312\n",
      "units=32, dropout=0.4, lr=0.005, n_layers=3, val_loss=0.5312\n",
      "Epoch 1, Val Loss: 4.5715\n",
      "Epoch 2, Val Loss: 0.4514\n",
      "Epoch 3, Val Loss: 0.5095\n",
      "Epoch 4, Val Loss: 0.4057\n",
      "Epoch 5, Val Loss: 0.3596\n",
      "Epoch 6, Val Loss: 0.3618\n",
      "Epoch 7, Val Loss: 0.3172\n",
      "Epoch 8, Val Loss: 0.3312\n",
      "Epoch 9, Val Loss: 0.3299\n",
      "Epoch 10, Val Loss: 0.3639\n",
      "units=32, dropout=0.4, lr=0.01, n_layers=1, val_loss=0.3172\n",
      "Epoch 1, Val Loss: 7.4834\n",
      "Epoch 2, Val Loss: 1.9443\n",
      "Epoch 3, Val Loss: 1.2991\n",
      "Epoch 4, Val Loss: 1.2494\n",
      "Epoch 5, Val Loss: 0.5286\n",
      "Epoch 6, Val Loss: 0.3683\n",
      "Epoch 7, Val Loss: 0.7563\n",
      "Epoch 8, Val Loss: 0.5222\n",
      "Epoch 9, Val Loss: 0.4761\n",
      "Epoch 10, Val Loss: 0.4267\n",
      "units=32, dropout=0.4, lr=0.01, n_layers=2, val_loss=0.3683\n",
      "Epoch 1, Val Loss: 10.4837\n",
      "Epoch 2, Val Loss: 3.4695\n",
      "Epoch 3, Val Loss: 1.0298\n",
      "Epoch 4, Val Loss: 1.5359\n",
      "Epoch 5, Val Loss: 0.8962\n",
      "Epoch 6, Val Loss: 0.6720\n",
      "Epoch 7, Val Loss: 0.5090\n",
      "Epoch 8, Val Loss: 0.6764\n",
      "Epoch 9, Val Loss: 0.5636\n",
      "Epoch 10, Val Loss: 0.6367\n",
      "units=32, dropout=0.4, lr=0.01, n_layers=3, val_loss=0.5090\n",
      "Epoch 1, Val Loss: 14.1462\n",
      "Epoch 2, Val Loss: 13.0397\n",
      "Epoch 3, Val Loss: 11.8645\n",
      "Epoch 4, Val Loss: 10.9294\n",
      "Epoch 5, Val Loss: 10.1318\n",
      "Epoch 6, Val Loss: 9.4858\n",
      "Epoch 7, Val Loss: 8.7617\n",
      "Epoch 8, Val Loss: 8.1134\n",
      "Epoch 9, Val Loss: 7.4369\n",
      "Epoch 10, Val Loss: 6.9282\n",
      "units=32, dropout=0.5, lr=0.0005, n_layers=1, val_loss=6.9282\n",
      "Epoch 1, Val Loss: 15.0896\n",
      "Epoch 2, Val Loss: 14.5432\n",
      "Epoch 3, Val Loss: 13.7903\n",
      "Epoch 4, Val Loss: 13.2221\n",
      "Epoch 5, Val Loss: 12.6374\n",
      "Epoch 6, Val Loss: 12.1685\n",
      "Epoch 7, Val Loss: 11.7276\n",
      "Epoch 8, Val Loss: 11.1787\n",
      "Epoch 9, Val Loss: 10.7289\n",
      "Epoch 10, Val Loss: 10.2575\n",
      "units=32, dropout=0.5, lr=0.0005, n_layers=2, val_loss=10.2575\n",
      "Epoch 1, Val Loss: 16.2818\n",
      "Epoch 2, Val Loss: 16.3219\n",
      "Epoch 3, Val Loss: 15.9546\n",
      "Epoch 4, Val Loss: 15.6557\n",
      "Epoch 5, Val Loss: 15.3255\n",
      "Epoch 6, Val Loss: 14.9345\n",
      "Epoch 7, Val Loss: 14.4159\n",
      "Epoch 8, Val Loss: 14.1155\n",
      "Epoch 9, Val Loss: 13.6276\n",
      "Epoch 10, Val Loss: 13.2984\n",
      "units=32, dropout=0.5, lr=0.0005, n_layers=3, val_loss=13.2984\n",
      "Epoch 1, Val Loss: 13.5348\n",
      "Epoch 2, Val Loss: 11.3499\n",
      "Epoch 3, Val Loss: 9.2056\n",
      "Epoch 4, Val Loss: 7.5155\n",
      "Epoch 5, Val Loss: 6.2977\n",
      "Epoch 6, Val Loss: 5.1824\n",
      "Epoch 7, Val Loss: 4.2410\n",
      "Epoch 8, Val Loss: 3.3230\n",
      "Epoch 9, Val Loss: 2.6766\n",
      "Epoch 10, Val Loss: 2.1364\n",
      "units=32, dropout=0.5, lr=0.001, n_layers=1, val_loss=2.1364\n",
      "Epoch 1, Val Loss: 14.7231\n",
      "Epoch 2, Val Loss: 13.6652\n",
      "Epoch 3, Val Loss: 12.3925\n",
      "Epoch 4, Val Loss: 11.2623\n",
      "Epoch 5, Val Loss: 10.4195\n",
      "Epoch 6, Val Loss: 9.5143\n",
      "Epoch 7, Val Loss: 8.7762\n",
      "Epoch 8, Val Loss: 7.7431\n",
      "Epoch 9, Val Loss: 6.9293\n",
      "Epoch 10, Val Loss: 5.9331\n",
      "units=32, dropout=0.5, lr=0.001, n_layers=2, val_loss=5.9331\n",
      "Epoch 1, Val Loss: 15.0642\n",
      "Epoch 2, Val Loss: 14.2143\n",
      "Epoch 3, Val Loss: 13.3017\n",
      "Epoch 4, Val Loss: 12.2694\n",
      "Epoch 5, Val Loss: 11.4115\n",
      "Epoch 6, Val Loss: 10.5105\n",
      "Epoch 7, Val Loss: 9.3879\n",
      "Epoch 8, Val Loss: 8.2782\n",
      "Epoch 9, Val Loss: 7.1252\n",
      "Epoch 10, Val Loss: 6.1151\n",
      "units=32, dropout=0.5, lr=0.001, n_layers=3, val_loss=6.1151\n",
      "Epoch 1, Val Loss: 9.0193\n",
      "Epoch 2, Val Loss: 2.5529\n",
      "Epoch 3, Val Loss: 0.5940\n",
      "Epoch 4, Val Loss: 0.4516\n",
      "Epoch 5, Val Loss: 0.5471\n",
      "Epoch 6, Val Loss: 0.6017\n",
      "Epoch 7, Val Loss: 0.4446\n",
      "Epoch 8, Val Loss: 0.3374\n",
      "Epoch 9, Val Loss: 0.4306\n",
      "Epoch 10, Val Loss: 0.4125\n",
      "units=32, dropout=0.5, lr=0.005, n_layers=1, val_loss=0.3374\n",
      "Epoch 1, Val Loss: 11.0091\n",
      "Epoch 2, Val Loss: 6.0849\n",
      "Epoch 3, Val Loss: 2.8981\n",
      "Epoch 4, Val Loss: 1.5504\n",
      "Epoch 5, Val Loss: 1.4791\n",
      "Epoch 6, Val Loss: 1.4574\n",
      "Epoch 7, Val Loss: 1.2986\n",
      "Epoch 8, Val Loss: 1.1353\n",
      "Epoch 9, Val Loss: 1.1065\n",
      "Epoch 10, Val Loss: 1.1997\n",
      "units=32, dropout=0.5, lr=0.005, n_layers=2, val_loss=1.1065\n",
      "Epoch 1, Val Loss: 12.4783\n",
      "Epoch 2, Val Loss: 7.4713\n",
      "Epoch 3, Val Loss: 3.3755\n",
      "Epoch 4, Val Loss: 1.6467\n",
      "Epoch 5, Val Loss: 1.5996\n",
      "Epoch 6, Val Loss: 1.2049\n",
      "Epoch 7, Val Loss: 1.0551\n",
      "Epoch 8, Val Loss: 0.8355\n",
      "Epoch 9, Val Loss: 1.0684\n",
      "Epoch 10, Val Loss: 0.8459\n",
      "units=32, dropout=0.5, lr=0.005, n_layers=3, val_loss=0.8355\n",
      "Epoch 1, Val Loss: 5.0463\n",
      "Epoch 2, Val Loss: 0.6242\n",
      "Epoch 3, Val Loss: 0.5945\n",
      "Epoch 4, Val Loss: 0.7846\n",
      "Epoch 5, Val Loss: 0.7107\n",
      "Epoch 6, Val Loss: 0.4882\n",
      "Epoch 7, Val Loss: 0.4120\n",
      "Epoch 8, Val Loss: 0.3392\n",
      "Epoch 9, Val Loss: 0.3257\n",
      "Epoch 10, Val Loss: 0.3370\n",
      "units=32, dropout=0.5, lr=0.01, n_layers=1, val_loss=0.3257\n",
      "Epoch 1, Val Loss: 5.3882\n",
      "Epoch 2, Val Loss: 1.5282\n",
      "Epoch 3, Val Loss: 1.6740\n",
      "Epoch 4, Val Loss: 1.3355\n",
      "Epoch 5, Val Loss: 1.0488\n",
      "Epoch 6, Val Loss: 0.9270\n",
      "Epoch 7, Val Loss: 0.8053\n",
      "Epoch 8, Val Loss: 0.8158\n",
      "Epoch 9, Val Loss: 0.5974\n",
      "Epoch 10, Val Loss: 0.5701\n",
      "units=32, dropout=0.5, lr=0.01, n_layers=2, val_loss=0.5701\n",
      "Epoch 1, Val Loss: 9.0205\n",
      "Epoch 2, Val Loss: 3.3091\n",
      "Epoch 3, Val Loss: 1.4910\n",
      "Epoch 4, Val Loss: 1.8372\n",
      "Epoch 5, Val Loss: 1.0871\n",
      "Epoch 6, Val Loss: 0.8451\n",
      "Epoch 7, Val Loss: 0.8622\n",
      "Epoch 8, Val Loss: 1.0166\n",
      "Epoch 9, Val Loss: 0.8257\n",
      "Epoch 10, Val Loss: 0.8185\n",
      "units=32, dropout=0.5, lr=0.01, n_layers=3, val_loss=0.8185\n",
      "Epoch 1, Val Loss: 17.7746\n",
      "Epoch 2, Val Loss: 17.5761\n",
      "Epoch 3, Val Loss: 16.3854\n",
      "Epoch 4, Val Loss: 14.9044\n",
      "Epoch 5, Val Loss: 13.5857\n",
      "Epoch 6, Val Loss: 12.2165\n",
      "Epoch 7, Val Loss: 11.0215\n",
      "Epoch 8, Val Loss: 9.9256\n",
      "Epoch 9, Val Loss: 8.8991\n",
      "Epoch 10, Val Loss: 7.9151\n",
      "units=64, dropout=0.3, lr=0.0005, n_layers=1, val_loss=7.9151\n",
      "Epoch 1, Val Loss: 15.7668\n",
      "Epoch 2, Val Loss: 14.9924\n",
      "Epoch 3, Val Loss: 13.7774\n",
      "Epoch 4, Val Loss: 12.3283\n",
      "Epoch 5, Val Loss: 11.2256\n",
      "Epoch 6, Val Loss: 10.3734\n",
      "Epoch 7, Val Loss: 9.2043\n",
      "Epoch 8, Val Loss: 8.2582\n",
      "Epoch 9, Val Loss: 7.4649\n",
      "Epoch 10, Val Loss: 6.6374\n",
      "units=64, dropout=0.3, lr=0.0005, n_layers=2, val_loss=6.6374\n",
      "Epoch 1, Val Loss: 14.2119\n",
      "Epoch 2, Val Loss: 13.1044\n",
      "Epoch 3, Val Loss: 11.6417\n",
      "Epoch 4, Val Loss: 10.4061\n",
      "Epoch 5, Val Loss: 9.4207\n",
      "Epoch 6, Val Loss: 8.5469\n",
      "Epoch 7, Val Loss: 7.6321\n",
      "Epoch 8, Val Loss: 6.8954\n",
      "Epoch 9, Val Loss: 6.0167\n",
      "Epoch 10, Val Loss: 5.3246\n",
      "units=64, dropout=0.3, lr=0.0005, n_layers=3, val_loss=5.3246\n",
      "Epoch 1, Val Loss: 12.7672\n",
      "Epoch 2, Val Loss: 9.6033\n",
      "Epoch 3, Val Loss: 6.6007\n",
      "Epoch 4, Val Loss: 4.5068\n",
      "Epoch 5, Val Loss: 3.0116\n",
      "Epoch 6, Val Loss: 2.0320\n",
      "Epoch 7, Val Loss: 1.3061\n",
      "Epoch 8, Val Loss: 0.9216\n",
      "Epoch 9, Val Loss: 0.6832\n",
      "Epoch 10, Val Loss: 0.5658\n",
      "units=64, dropout=0.3, lr=0.001, n_layers=1, val_loss=0.5658\n",
      "Epoch 1, Val Loss: 13.8765\n",
      "Epoch 2, Val Loss: 10.8353\n",
      "Epoch 3, Val Loss: 8.2205\n",
      "Epoch 4, Val Loss: 6.1802\n",
      "Epoch 5, Val Loss: 4.9519\n",
      "Epoch 6, Val Loss: 3.6880\n",
      "Epoch 7, Val Loss: 2.8407\n",
      "Epoch 8, Val Loss: 2.0942\n",
      "Epoch 9, Val Loss: 1.5300\n",
      "Epoch 10, Val Loss: 1.1596\n",
      "units=64, dropout=0.3, lr=0.001, n_layers=2, val_loss=1.1596\n",
      "Epoch 1, Val Loss: 15.4376\n",
      "Epoch 2, Val Loss: 13.7854\n",
      "Epoch 3, Val Loss: 11.8197\n",
      "Epoch 4, Val Loss: 9.8672\n",
      "Epoch 5, Val Loss: 8.1510\n",
      "Epoch 6, Val Loss: 6.4406\n",
      "Epoch 7, Val Loss: 5.1246\n",
      "Epoch 8, Val Loss: 3.9935\n",
      "Epoch 9, Val Loss: 2.9476\n",
      "Epoch 10, Val Loss: 2.1617\n",
      "units=64, dropout=0.3, lr=0.001, n_layers=3, val_loss=2.1617\n",
      "Epoch 1, Val Loss: 7.4548\n",
      "Epoch 2, Val Loss: 0.9672\n",
      "Epoch 3, Val Loss: 0.3736\n",
      "Epoch 4, Val Loss: 0.4674\n",
      "Epoch 5, Val Loss: 0.4369\n",
      "Epoch 6, Val Loss: 0.4357\n",
      "Epoch 7, Val Loss: 0.3801\n",
      "Epoch 8, Val Loss: 0.3685\n",
      "Epoch 9, Val Loss: 0.3767\n",
      "Epoch 10, Val Loss: 0.3732\n",
      "units=64, dropout=0.3, lr=0.005, n_layers=1, val_loss=0.3685\n",
      "Epoch 1, Val Loss: 8.9256\n",
      "Epoch 2, Val Loss: 2.0763\n",
      "Epoch 3, Val Loss: 1.0225\n",
      "Epoch 4, Val Loss: 0.7689\n",
      "Epoch 5, Val Loss: 0.3947\n",
      "Epoch 6, Val Loss: 0.4568\n",
      "Epoch 7, Val Loss: 0.5574\n",
      "Epoch 8, Val Loss: 0.3915\n",
      "Epoch 9, Val Loss: 0.3735\n",
      "Epoch 10, Val Loss: 0.4698\n",
      "units=64, dropout=0.3, lr=0.005, n_layers=2, val_loss=0.3735\n",
      "Epoch 1, Val Loss: 8.5566\n",
      "Epoch 2, Val Loss: 1.7727\n",
      "Epoch 3, Val Loss: 0.5520\n",
      "Epoch 4, Val Loss: 1.0166\n",
      "Epoch 5, Val Loss: 0.6001\n",
      "Epoch 6, Val Loss: 0.3974\n",
      "Epoch 7, Val Loss: 0.6899\n",
      "Epoch 8, Val Loss: 0.4814\n",
      "Epoch 9, Val Loss: 0.4844\n",
      "Epoch 10, Val Loss: 0.3416\n",
      "units=64, dropout=0.3, lr=0.005, n_layers=3, val_loss=0.3416\n",
      "Epoch 1, Val Loss: 2.6218\n",
      "Epoch 2, Val Loss: 0.5651\n",
      "Epoch 3, Val Loss: 0.5296\n",
      "Epoch 4, Val Loss: 0.5434\n",
      "Epoch 5, Val Loss: 0.4430\n",
      "Epoch 6, Val Loss: 0.3887\n",
      "Epoch 7, Val Loss: 0.3917\n",
      "Epoch 8, Val Loss: 0.3090\n",
      "Epoch 9, Val Loss: 0.3710\n",
      "Epoch 10, Val Loss: 0.3723\n",
      "units=64, dropout=0.3, lr=0.01, n_layers=1, val_loss=0.3090\n",
      "Epoch 1, Val Loss: 4.5459\n",
      "Epoch 2, Val Loss: 1.2502\n",
      "Epoch 3, Val Loss: 0.9612\n",
      "Epoch 4, Val Loss: 0.3797\n",
      "Epoch 5, Val Loss: 0.6460\n",
      "Epoch 6, Val Loss: 0.3701\n",
      "Epoch 7, Val Loss: 0.2554\n",
      "Epoch 8, Val Loss: 0.5261\n",
      "Epoch 9, Val Loss: 0.3447\n",
      "Epoch 10, Val Loss: 0.2573\n",
      "units=64, dropout=0.3, lr=0.01, n_layers=2, val_loss=0.2554\n",
      "Epoch 1, Val Loss: 4.3101\n",
      "Epoch 2, Val Loss: 1.0533\n",
      "Epoch 3, Val Loss: 1.0861\n",
      "Epoch 4, Val Loss: 0.2848\n",
      "Epoch 5, Val Loss: 0.5909\n",
      "Epoch 6, Val Loss: 0.3862\n",
      "Epoch 7, Val Loss: 0.4085\n",
      "Epoch 8, Val Loss: 0.3762\n",
      "Epoch 9, Val Loss: 0.3014\n",
      "Early stopping!\n",
      "units=64, dropout=0.3, lr=0.01, n_layers=3, val_loss=0.2848\n",
      "Epoch 1, Val Loss: 13.6220\n",
      "Epoch 2, Val Loss: 12.1738\n",
      "Epoch 3, Val Loss: 10.5240\n",
      "Epoch 4, Val Loss: 9.1506\n",
      "Epoch 5, Val Loss: 7.8877\n",
      "Epoch 6, Val Loss: 6.9078\n",
      "Epoch 7, Val Loss: 5.8123\n",
      "Epoch 8, Val Loss: 5.0251\n",
      "Epoch 9, Val Loss: 4.2513\n",
      "Epoch 10, Val Loss: 3.5684\n",
      "units=64, dropout=0.4, lr=0.0005, n_layers=1, val_loss=3.5684\n",
      "Epoch 1, Val Loss: 14.5700\n",
      "Epoch 2, Val Loss: 13.3190\n",
      "Epoch 3, Val Loss: 11.9590\n",
      "Epoch 4, Val Loss: 10.7446\n",
      "Epoch 5, Val Loss: 9.6578\n",
      "Epoch 6, Val Loss: 8.9372\n",
      "Epoch 7, Val Loss: 8.0372\n",
      "Epoch 8, Val Loss: 7.0869\n",
      "Epoch 9, Val Loss: 6.3830\n",
      "Epoch 10, Val Loss: 5.7326\n",
      "units=64, dropout=0.4, lr=0.0005, n_layers=2, val_loss=5.7326\n",
      "Epoch 1, Val Loss: 14.5322\n",
      "Epoch 2, Val Loss: 13.8998\n",
      "Epoch 3, Val Loss: 13.0590\n",
      "Epoch 4, Val Loss: 12.1221\n",
      "Epoch 5, Val Loss: 11.4411\n",
      "Epoch 6, Val Loss: 10.6662\n",
      "Epoch 7, Val Loss: 9.9389\n",
      "Epoch 8, Val Loss: 8.9431\n",
      "Epoch 9, Val Loss: 7.9932\n",
      "Epoch 10, Val Loss: 7.2200\n",
      "units=64, dropout=0.4, lr=0.0005, n_layers=3, val_loss=7.2200\n",
      "Epoch 1, Val Loss: 13.0486\n",
      "Epoch 2, Val Loss: 9.9800\n",
      "Epoch 3, Val Loss: 6.9898\n",
      "Epoch 4, Val Loss: 4.8515\n",
      "Epoch 5, Val Loss: 3.4511\n",
      "Epoch 6, Val Loss: 2.2976\n",
      "Epoch 7, Val Loss: 1.5717\n",
      "Epoch 8, Val Loss: 1.0770\n",
      "Epoch 9, Val Loss: 0.8125\n",
      "Epoch 10, Val Loss: 0.6221\n",
      "units=64, dropout=0.4, lr=0.001, n_layers=1, val_loss=0.6221\n",
      "Epoch 1, Val Loss: 14.0090\n",
      "Epoch 2, Val Loss: 11.8097\n",
      "Epoch 3, Val Loss: 9.2291\n",
      "Epoch 4, Val Loss: 7.4050\n",
      "Epoch 5, Val Loss: 6.0454\n",
      "Epoch 6, Val Loss: 4.6967\n",
      "Epoch 7, Val Loss: 3.5812\n",
      "Epoch 8, Val Loss: 2.6750\n",
      "Epoch 9, Val Loss: 2.1089\n",
      "Epoch 10, Val Loss: 1.8090\n",
      "units=64, dropout=0.4, lr=0.001, n_layers=2, val_loss=1.8090\n",
      "Epoch 1, Val Loss: 13.9551\n",
      "Epoch 2, Val Loss: 12.2807\n",
      "Epoch 3, Val Loss: 10.4061\n",
      "Epoch 4, Val Loss: 8.5762\n",
      "Epoch 5, Val Loss: 7.1108\n",
      "Epoch 6, Val Loss: 5.5697\n",
      "Epoch 7, Val Loss: 4.2058\n",
      "Epoch 8, Val Loss: 3.3422\n",
      "Epoch 9, Val Loss: 2.4902\n",
      "Epoch 10, Val Loss: 1.9303\n",
      "units=64, dropout=0.4, lr=0.001, n_layers=3, val_loss=1.9303\n",
      "Epoch 1, Val Loss: 7.4235\n",
      "Epoch 2, Val Loss: 0.9334\n",
      "Epoch 3, Val Loss: 0.4077\n",
      "Epoch 4, Val Loss: 0.5239\n",
      "Epoch 5, Val Loss: 0.4331\n",
      "Epoch 6, Val Loss: 0.4637\n",
      "Epoch 7, Val Loss: 0.3259\n",
      "Epoch 8, Val Loss: 0.3267\n",
      "Epoch 9, Val Loss: 0.3260\n",
      "Epoch 10, Val Loss: 0.3181\n",
      "units=64, dropout=0.4, lr=0.005, n_layers=1, val_loss=0.3181\n",
      "Epoch 1, Val Loss: 9.1667\n",
      "Epoch 2, Val Loss: 2.7451\n",
      "Epoch 3, Val Loss: 0.8946\n",
      "Epoch 4, Val Loss: 1.0881\n",
      "Epoch 5, Val Loss: 0.8653\n",
      "Epoch 6, Val Loss: 0.6698\n",
      "Epoch 7, Val Loss: 0.5613\n",
      "Epoch 8, Val Loss: 0.7018\n",
      "Epoch 9, Val Loss: 0.5787\n",
      "Epoch 10, Val Loss: 0.5517\n",
      "units=64, dropout=0.4, lr=0.005, n_layers=2, val_loss=0.5517\n",
      "Epoch 1, Val Loss: 8.3850\n",
      "Epoch 2, Val Loss: 1.8025\n",
      "Epoch 3, Val Loss: 0.7875\n",
      "Epoch 4, Val Loss: 1.0299\n",
      "Epoch 5, Val Loss: 0.6748\n",
      "Epoch 6, Val Loss: 0.6570\n",
      "Epoch 7, Val Loss: 0.5419\n",
      "Epoch 8, Val Loss: 0.5869\n",
      "Epoch 9, Val Loss: 0.4265\n",
      "Epoch 10, Val Loss: 0.4870\n",
      "units=64, dropout=0.4, lr=0.005, n_layers=3, val_loss=0.4265\n",
      "Epoch 1, Val Loss: 2.0120\n",
      "Epoch 2, Val Loss: 0.8952\n",
      "Epoch 3, Val Loss: 0.5534\n",
      "Epoch 4, Val Loss: 0.6405\n",
      "Epoch 5, Val Loss: 0.5291\n",
      "Epoch 6, Val Loss: 0.4113\n",
      "Epoch 7, Val Loss: 0.4553\n",
      "Epoch 8, Val Loss: 0.4867\n",
      "Epoch 9, Val Loss: 0.3672\n",
      "Epoch 10, Val Loss: 0.3502\n",
      "units=64, dropout=0.4, lr=0.01, n_layers=1, val_loss=0.3502\n",
      "Epoch 1, Val Loss: 4.2013\n",
      "Epoch 2, Val Loss: 1.8201\n",
      "Epoch 3, Val Loss: 0.9803\n",
      "Epoch 4, Val Loss: 0.3264\n",
      "Epoch 5, Val Loss: 0.7130\n",
      "Epoch 6, Val Loss: 0.4283\n",
      "Epoch 7, Val Loss: 0.4340\n",
      "Epoch 8, Val Loss: 0.5024\n",
      "Epoch 9, Val Loss: 0.4600\n",
      "Early stopping!\n",
      "units=64, dropout=0.4, lr=0.01, n_layers=2, val_loss=0.3264\n",
      "Epoch 1, Val Loss: 5.9229\n",
      "Epoch 2, Val Loss: 1.3894\n",
      "Epoch 3, Val Loss: 1.4745\n",
      "Epoch 4, Val Loss: 0.6749\n",
      "Epoch 5, Val Loss: 0.8538\n",
      "Epoch 6, Val Loss: 0.4636\n",
      "Epoch 7, Val Loss: 0.6321\n",
      "Epoch 8, Val Loss: 0.4242\n",
      "Epoch 9, Val Loss: 0.5202\n",
      "Epoch 10, Val Loss: 0.4010\n",
      "units=64, dropout=0.4, lr=0.01, n_layers=3, val_loss=0.4010\n",
      "Epoch 1, Val Loss: 14.7604\n",
      "Epoch 2, Val Loss: 13.1269\n",
      "Epoch 3, Val Loss: 11.4527\n",
      "Epoch 4, Val Loss: 9.9774\n",
      "Epoch 5, Val Loss: 8.7818\n",
      "Epoch 6, Val Loss: 7.6515\n",
      "Epoch 7, Val Loss: 6.7223\n",
      "Epoch 8, Val Loss: 5.8309\n",
      "Epoch 9, Val Loss: 5.0131\n",
      "Epoch 10, Val Loss: 4.3683\n",
      "units=64, dropout=0.5, lr=0.0005, n_layers=1, val_loss=4.3683\n",
      "Epoch 1, Val Loss: 15.9293\n",
      "Epoch 2, Val Loss: 15.2374\n",
      "Epoch 3, Val Loss: 14.4280\n",
      "Epoch 4, Val Loss: 13.3361\n",
      "Epoch 5, Val Loss: 12.3956\n",
      "Epoch 6, Val Loss: 11.6219\n",
      "Epoch 7, Val Loss: 10.7970\n",
      "Epoch 8, Val Loss: 10.1098\n",
      "Epoch 9, Val Loss: 9.4348\n",
      "Epoch 10, Val Loss: 8.6111\n",
      "units=64, dropout=0.5, lr=0.0005, n_layers=2, val_loss=8.6111\n",
      "Epoch 1, Val Loss: 15.2009\n",
      "Epoch 2, Val Loss: 14.5307\n",
      "Epoch 3, Val Loss: 13.8401\n",
      "Epoch 4, Val Loss: 13.0359\n",
      "Epoch 5, Val Loss: 12.3973\n",
      "Epoch 6, Val Loss: 11.4847\n",
      "Epoch 7, Val Loss: 10.8143\n",
      "Epoch 8, Val Loss: 10.0522\n",
      "Epoch 9, Val Loss: 9.1992\n",
      "Epoch 10, Val Loss: 8.6649\n",
      "units=64, dropout=0.5, lr=0.0005, n_layers=3, val_loss=8.6649\n",
      "Epoch 1, Val Loss: 12.4555\n",
      "Epoch 2, Val Loss: 9.4460\n",
      "Epoch 3, Val Loss: 6.6220\n",
      "Epoch 4, Val Loss: 4.6786\n",
      "Epoch 5, Val Loss: 3.3694\n",
      "Epoch 6, Val Loss: 2.3959\n",
      "Epoch 7, Val Loss: 1.7393\n",
      "Epoch 8, Val Loss: 1.2732\n",
      "Epoch 9, Val Loss: 0.9999\n",
      "Epoch 10, Val Loss: 0.8275\n",
      "units=64, dropout=0.5, lr=0.001, n_layers=1, val_loss=0.8275\n",
      "Epoch 1, Val Loss: 13.0532\n",
      "Epoch 2, Val Loss: 11.1731\n",
      "Epoch 3, Val Loss: 9.2413\n",
      "Epoch 4, Val Loss: 7.4040\n",
      "Epoch 5, Val Loss: 6.0522\n",
      "Epoch 6, Val Loss: 4.9149\n",
      "Epoch 7, Val Loss: 3.9133\n",
      "Epoch 8, Val Loss: 3.1108\n",
      "Epoch 9, Val Loss: 2.5417\n",
      "Epoch 10, Val Loss: 2.0956\n",
      "units=64, dropout=0.5, lr=0.001, n_layers=2, val_loss=2.0956\n",
      "Epoch 1, Val Loss: 12.9526\n",
      "Epoch 2, Val Loss: 11.0760\n",
      "Epoch 3, Val Loss: 9.6348\n",
      "Epoch 4, Val Loss: 8.0131\n",
      "Epoch 5, Val Loss: 6.4739\n",
      "Epoch 6, Val Loss: 5.3230\n",
      "Epoch 7, Val Loss: 3.9372\n",
      "Epoch 8, Val Loss: 3.1839\n",
      "Epoch 9, Val Loss: 2.4968\n",
      "Epoch 10, Val Loss: 1.7774\n",
      "units=64, dropout=0.5, lr=0.001, n_layers=3, val_loss=1.7774\n",
      "Epoch 1, Val Loss: 6.9473\n",
      "Epoch 2, Val Loss: 0.9448\n",
      "Epoch 3, Val Loss: 0.5196\n",
      "Epoch 4, Val Loss: 0.5858\n",
      "Epoch 5, Val Loss: 0.5521\n",
      "Epoch 6, Val Loss: 0.4298\n",
      "Epoch 7, Val Loss: 0.4669\n",
      "Epoch 8, Val Loss: 0.6660\n",
      "Epoch 9, Val Loss: 0.6714\n",
      "Epoch 10, Val Loss: 0.5088\n",
      "units=64, dropout=0.5, lr=0.005, n_layers=1, val_loss=0.4298\n",
      "Epoch 1, Val Loss: 9.6925\n",
      "Epoch 2, Val Loss: 3.5557\n",
      "Epoch 3, Val Loss: 1.4774\n",
      "Epoch 4, Val Loss: 1.6545\n",
      "Epoch 5, Val Loss: 1.1399\n",
      "Epoch 6, Val Loss: 0.8105\n",
      "Epoch 7, Val Loss: 0.8173\n",
      "Epoch 8, Val Loss: 0.8945\n",
      "Epoch 9, Val Loss: 0.9597\n",
      "Epoch 10, Val Loss: 0.9075\n",
      "units=64, dropout=0.5, lr=0.005, n_layers=2, val_loss=0.8105\n",
      "Epoch 1, Val Loss: 7.5904\n",
      "Epoch 2, Val Loss: 1.5781\n",
      "Epoch 3, Val Loss: 1.0027\n",
      "Epoch 4, Val Loss: 1.4702\n",
      "Epoch 5, Val Loss: 1.0798\n",
      "Epoch 6, Val Loss: 1.0708\n",
      "Epoch 7, Val Loss: 0.9129\n",
      "Epoch 8, Val Loss: 0.7555\n",
      "Epoch 9, Val Loss: 0.7863\n",
      "Epoch 10, Val Loss: 1.0832\n",
      "units=64, dropout=0.5, lr=0.005, n_layers=3, val_loss=0.7555\n",
      "Epoch 1, Val Loss: 2.7961\n",
      "Epoch 2, Val Loss: 0.4918\n",
      "Epoch 3, Val Loss: 0.5995\n",
      "Epoch 4, Val Loss: 0.3717\n",
      "Epoch 5, Val Loss: 0.4936\n",
      "Epoch 6, Val Loss: 0.3058\n",
      "Epoch 7, Val Loss: 0.2934\n",
      "Epoch 8, Val Loss: 0.3346\n",
      "Epoch 9, Val Loss: 0.4837\n",
      "Epoch 10, Val Loss: 0.3964\n",
      "units=64, dropout=0.5, lr=0.01, n_layers=1, val_loss=0.2934\n",
      "Epoch 1, Val Loss: 4.6584\n",
      "Epoch 2, Val Loss: 2.5684\n",
      "Epoch 3, Val Loss: 1.7783\n",
      "Epoch 4, Val Loss: 1.0286\n",
      "Epoch 5, Val Loss: 0.8118\n",
      "Epoch 6, Val Loss: 0.8876\n",
      "Epoch 7, Val Loss: 0.8838\n",
      "Epoch 8, Val Loss: 0.6192\n",
      "Epoch 9, Val Loss: 0.8245\n",
      "Epoch 10, Val Loss: 0.5671\n",
      "units=64, dropout=0.5, lr=0.01, n_layers=2, val_loss=0.5671\n",
      "Epoch 1, Val Loss: 4.3240\n",
      "Epoch 2, Val Loss: 1.9840\n",
      "Epoch 3, Val Loss: 1.5509\n",
      "Epoch 4, Val Loss: 0.8449\n",
      "Epoch 5, Val Loss: 0.8302\n",
      "Epoch 6, Val Loss: 0.9644\n",
      "Epoch 7, Val Loss: 0.7262\n",
      "Epoch 8, Val Loss: 0.7929\n",
      "Epoch 9, Val Loss: 0.5744\n",
      "Epoch 10, Val Loss: 0.5438\n",
      "units=64, dropout=0.5, lr=0.01, n_layers=3, val_loss=0.5438\n"
     ]
    }
   ],
   "source": [
    "from itertools import product\n",
    "param_grid = {\n",
    "    'units': [32, 64],\n",
    "    'dropout': [0.3, 0.4, 0.5],\n",
    "    'learning_rate': [0.0005, 0.001, 0.005, 0.01],\n",
    "    'n_layers': [1, 2, 3]\n",
    "}\n",
    "results = []\n",
    "for units, dropout, lr, n_layers in product(param_grid['units'], param_grid['dropout'], param_grid['learning_rate'],\n",
    "                                 param_grid['n_layers']):\n",
    "    val_loss = train_and_evaluate(units, dropout, lr, n_layers, 10)\n",
    "    print(f\"units={units}, dropout={dropout}, lr={lr}, n_layers={n_layers}, val_loss={val_loss:.4f}\")\n",
    "    results.append({'units': units, 'dropout': dropout, 'lr': lr, 'n_layers': n_layers, 'val_loss': val_loss})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "id": "24fdc60d-701e-4877-8ec2-ba22742812e6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Best hyperparams: {'units': 64, 'dropout': 0.3, 'lr': 0.01, 'n_layers': 2, 'val_loss': 0.25541515786106844}\n"
     ]
    }
   ],
   "source": [
    "best = min(results, key=lambda x: x['val_loss'])\n",
    "print(f\"Best hyperparams: {best}\")\n",
    "best_params = {k: best[k] for k in ['units', 'dropout', 'lr', 'n_layers']}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "id": "f2070cdb-75a4-41d9-8427-a486d9205dc3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1, Loss: 5.8759\n",
      "Epoch 2, Loss: 1.0882\n",
      "Epoch 3, Loss: 0.6879\n",
      "Epoch 4, Loss: 0.5320\n",
      "Epoch 5, Loss: 0.5450\n",
      "Epoch 6, Loss: 0.5390\n",
      "Epoch 7, Loss: 0.5047\n",
      "Epoch 8, Loss: 0.4200\n",
      "Epoch 9, Loss: 0.4235\n",
      "Epoch 10, Loss: 0.4136\n",
      "Epoch 11, Loss: 0.4130\n",
      "Epoch 12, Loss: 0.3547\n",
      "Epoch 13, Loss: 0.3477\n",
      "Epoch 14, Loss: 0.3088\n",
      "Epoch 15, Loss: 0.3336\n",
      "Epoch 16, Loss: 0.3474\n",
      "Epoch 17, Loss: 0.3163\n",
      "Epoch 18, Loss: 0.3302\n",
      "Epoch 19, Loss: 0.2853\n",
      "Epoch 20, Loss: 0.2849\n",
      "Epoch 21, Loss: 0.2767\n",
      "Epoch 22, Loss: 0.2807\n",
      "Epoch 23, Loss: 0.2696\n",
      "Epoch 24, Loss: 0.2543\n",
      "Epoch 25, Loss: 0.2373\n",
      "Epoch 26, Loss: 0.2620\n",
      "Epoch 27, Loss: 0.2331\n",
      "Epoch 28, Loss: 0.2016\n",
      "Epoch 29, Loss: 0.2651\n",
      "Epoch 30, Loss: 0.2307\n",
      "Epoch 31, Loss: 0.2295\n",
      "Epoch 32, Loss: 0.2324\n",
      "Epoch 33, Loss: 0.2125\n",
      "Epoch 34, Loss: 0.2219\n",
      "Epoch 35, Loss: 0.2033\n",
      "Epoch 36, Loss: 0.1944\n",
      "Epoch 37, Loss: 0.2102\n",
      "Epoch 38, Loss: 0.1663\n",
      "Epoch 39, Loss: 0.1898\n",
      "Epoch 40, Loss: 0.1938\n",
      "Epoch 41, Loss: 0.1790\n",
      "Epoch 42, Loss: 0.1857\n",
      "Epoch 43, Loss: 0.1982\n",
      "Epoch 44, Loss: 0.1993\n",
      "Epoch 45, Loss: 0.1987\n",
      "Epoch 46, Loss: 0.1902\n",
      "Epoch 47, Loss: 0.1656\n",
      "Epoch 48, Loss: 0.1599\n",
      "Epoch 49, Loss: 0.1703\n",
      "Epoch 50, Loss: 0.1515\n"
     ]
    }
   ],
   "source": [
    "X_combined_soccer = torch.cat([X_train_tensor_soccer, X_val_tensor_soccer], dim=0)\n",
    "y_combined_soccer_log = torch.cat([y_train_tensor_soccer_log, y_val_tensor_soccer_log], dim=0)\n",
    "combined_dataset_soccer_log = TensorDataset(X_combined_soccer, y_combined_soccer_log)\n",
    "combined_loader_soccer_log = DataLoader(combined_dataset_soccer_log, batch_size=32, shuffle=True)\n",
    "\n",
    "# Create new model using best hyperparameters\n",
    "final_model = SimpleNN(\n",
    "    input_dim=X_train_tensor_soccer.shape[1],\n",
    "    units=best_params['units'],\n",
    "    dropout=best_params['dropout'],\n",
    "    n_layers=best_params['n_layers'],\n",
    ")\n",
    "optimizer = torch.optim.AdamW(final_model.parameters(), lr=best_params['lr'])\n",
    "criterion = torch.nn.MSELoss()\n",
    "\n",
    "train_losses = []\n",
    "\n",
    "for epoch in range(50):\n",
    "    final_model.train()\n",
    "    running_loss = 0\n",
    "    total = 0\n",
    "    for xb, yb in combined_loader_soccer_log:\n",
    "        optimizer.zero_grad()\n",
    "        preds = final_model(xb)\n",
    "        loss = criterion(preds, yb)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        running_loss += loss.item() * xb.size(0)\n",
    "        total += yb.size(0)\n",
    "    epoch_loss = running_loss / total\n",
    "    train_losses.append(epoch_loss)\n",
    "    print(f\"Epoch {epoch+1}, Loss: {epoch_loss:.4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "id": "e9177c84-6d66-44ac-8dbe-34974395fd56",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test RMSE (log scale): 0.3409\n",
      "Test RMSE (original scale): 25.1927\n"
     ]
    }
   ],
   "source": [
    "# Compute residuals on val set to estimate sigma for back-transformation\n",
    "final_model.eval()\n",
    "with torch.no_grad():\n",
    "    preds_val_log = final_model(X_val_tensor_soccer)\n",
    "    residuals_log = preds_val_log - y_val_tensor_soccer_log\n",
    "    sigma2 = torch.var(residuals_log)  # used in log-normal correction\n",
    "\n",
    "# Evaluate on test set with log-normal correction\n",
    "all_preds = []\n",
    "all_targets = []\n",
    "all_preds_log = []\n",
    "all_targets_log = []\n",
    "with torch.no_grad():\n",
    "    for xb, yb in test_loader_soccer_log:  \n",
    "        preds_log = final_model(xb)\n",
    "        preds = torch.exp(preds_log + sigma2 / 2)  # apply correction\n",
    "        targets = torch.exp(yb)\n",
    "        all_preds_log.append(preds_log)\n",
    "        all_targets_log.append(yb)\n",
    "        all_preds.append(preds)\n",
    "        all_targets.append(targets)\n",
    "\n",
    "all_preds_log = torch.cat(all_preds_log).cpu().numpy()\n",
    "all_targets_log = torch.cat(all_targets_log).cpu().numpy()\n",
    "all_preds = torch.cat(all_preds).cpu().numpy()\n",
    "all_targets = torch.cat(all_targets).cpu().numpy()\n",
    "\n",
    "rmse_log = np.sqrt(np.mean((all_preds_log - all_targets_log)**2))\n",
    "print(f\"Test RMSE (log scale): {rmse_log:.4f}\")\n",
    "rmse = np.sqrt(np.mean((all_preds - all_targets)**2))\n",
    "print(f\"Test RMSE (original scale): {rmse:.4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "id": "1a745522-0f62-40df-8bec-61401b566aa0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Raw predictions (log scale): tensor([[3.4371],\n",
      "        [3.6048],\n",
      "        [3.2966],\n",
      "        [4.0944],\n",
      "        [3.4133]])\n",
      "Exp predictions: tensor([[31.9947],\n",
      "        [37.8358],\n",
      "        [27.8016],\n",
      "        [61.7379],\n",
      "        [31.2438]])\n",
      "Raw targets (log scale): tensor([[3.6889],\n",
      "        [3.3673],\n",
      "        [3.3666],\n",
      "        [4.0942],\n",
      "        [3.4337]])\n",
      "Exp targets: tensor([[40.0000],\n",
      "        [29.0000],\n",
      "        [28.9800],\n",
      "        [59.9900],\n",
      "        [30.9900]])\n"
     ]
    }
   ],
   "source": [
    "print(\"Raw predictions (log scale):\", preds_log[:5])\n",
    "print(\"Exp predictions:\", preds[:5])\n",
    "print(\"Raw targets (log scale):\", yb[:5])\n",
    "print(\"Exp targets:\", targets[:5])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4bc6e511",
   "metadata": {},
   "source": [
    "#### MLR"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "id": "0fd3fd23",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                            OLS Regression Results                            \n",
      "==============================================================================\n",
      "Dep. Variable:            price.value   R-squared:                       0.602\n",
      "Model:                            OLS   Adj. R-squared:                  0.549\n",
      "Method:                 Least Squares   F-statistic:                     11.33\n",
      "Date:                Sun, 10 Aug 2025   Prob (F-statistic):           2.40e-68\n",
      "Time:                        14:16:33   Log-Likelihood:                -2563.1\n",
      "No. Observations:                 595   AIC:                             5268.\n",
      "Df Residuals:                     524   BIC:                             5580.\n",
      "Df Model:                          70                                         \n",
      "Covariance Type:            nonrobust                                         \n",
      "============================================================================================================\n",
      "                                               coef    std err          t      P>|t|      [0.025      0.975]\n",
      "------------------------------------------------------------------------------------------------------------\n",
      "const                                       40.7597     11.087      3.676      0.000      18.979      62.540\n",
      "seller.feedbackPercentage                    1.0305      0.866      1.190      0.234      -0.670       2.731\n",
      "seller.feedbackScore                         0.6099      0.896      0.681      0.496      -1.150       2.370\n",
      "marketingPrice.discountPercentage           -1.4803      0.972     -1.522      0.129      -3.390       0.430\n",
      "shipping_cost                               -2.6863      1.393     -1.929      0.054      -5.422       0.050\n",
      "days_listed                                 -0.4322      0.907     -0.477      0.634      -2.213       1.349\n",
      "seller_item_count                            0.5107      1.059      0.482      0.630      -1.571       2.592\n",
      "year                                        -2.5738      0.995     -2.588      0.010      -4.528      -0.620\n",
      "additional_image_count                       1.2752      1.099      1.160      0.247      -0.885       3.435\n",
      "title_length                                 2.4164      1.186      2.038      0.042       0.087       4.746\n",
      "topRatedBuyingExperience                    17.2868      4.506      3.837      0.000       8.435      26.138\n",
      "priorityListing                              1.9234      2.540      0.757      0.449      -3.066       6.912\n",
      "Messi                                       -2.7058      6.164     -0.439      0.661     -14.816       9.404\n",
      "condition_New with tags                      1.0015      5.216      0.192      0.848      -9.246      11.249\n",
      "condition_New without tags                   5.8660     13.525      0.434      0.665     -20.704      32.436\n",
      "condition_Pre-owned                          0.8853     19.906      0.044      0.965     -38.219      39.990\n",
      "condition_Unspecified                       15.1309      7.651      1.978      0.049       0.100      30.162\n",
      "condition_Used                              -5.3013      2.902     -1.826      0.068     -11.003       0.401\n",
      "itemLocation.country_AR                     26.4931     17.271      1.534      0.126      -7.437      60.423\n",
      "itemLocation.country_BD                   1.039e-13   5.19e-14      1.999      0.046    1.81e-15    2.06e-13\n",
      "itemLocation.country_BE                     14.6978     13.779      1.067      0.287     -12.371      41.766\n",
      "itemLocation.country_BR                  -1.054e-13   9.71e-14     -1.085      0.278   -2.96e-13    8.54e-14\n",
      "itemLocation.country_CA                     -1.8724     13.061     -0.143      0.886     -27.531      23.786\n",
      "itemLocation.country_CL                     99.0671     23.695      4.181      0.000      52.519     145.615\n",
      "itemLocation.country_DE                     11.9786     13.828      0.866      0.387     -15.186      39.144\n",
      "itemLocation.country_FR                     11.2091     12.428      0.902      0.368     -13.206      35.624\n",
      "itemLocation.country_GB                      2.8033     22.939      0.122      0.903     -42.261      47.867\n",
      "itemLocation.country_ID                     12.3632     22.868      0.541      0.589     -32.561      57.288\n",
      "itemLocation.country_IL                    -20.6939     22.683     -0.912      0.362     -65.254      23.866\n",
      "itemLocation.country_JO                    -15.9581     22.682     -0.704      0.482     -60.517      28.600\n",
      "itemLocation.country_JP                    145.8880     16.553      8.813      0.000     113.369     178.407\n",
      "itemLocation.country_KR                    100.9128     15.694      6.430      0.000      70.081     131.745\n",
      "itemLocation.country_LK                     -7.4095     15.428     -0.480      0.631     -37.717      22.898\n",
      "itemLocation.country_MA                     21.4462     23.900      0.897      0.370     -25.506      68.398\n",
      "itemLocation.country_MX                      6.0615     23.267      0.261      0.795     -39.646      51.769\n",
      "itemLocation.country_MY                    -35.5714     22.765     -1.563      0.119     -80.294       9.151\n",
      "itemLocation.country_NL                     20.7139     16.262      1.274      0.203     -11.233      52.661\n",
      "itemLocation.country_NZ                    -13.4119     12.227     -1.097      0.273     -37.431      10.607\n",
      "itemLocation.country_PE                     55.3432     18.581      2.978      0.003      18.840      91.846\n",
      "itemLocation.country_PH                     10.2432     15.551      0.659      0.510     -20.306      40.793\n",
      "itemLocation.country_PK                      1.9196     16.529      0.116      0.908     -30.552      34.391\n",
      "itemLocation.country_PT                      8.4757     16.316      0.519      0.604     -23.576      40.528\n",
      "itemLocation.country_SA                     13.1129     15.289      0.858      0.391     -16.922      43.148\n",
      "itemLocation.country_SG                     26.2810     22.728      1.156      0.248     -18.367      70.929\n",
      "itemLocation.country_SV                    -12.2951     21.254     -0.578      0.563     -54.048      29.458\n",
      "itemLocation.country_TH                     14.1923     16.403      0.865      0.387     -18.031      46.415\n",
      "itemLocation.country_TW                     -7.4583     13.375     -0.558      0.577     -33.733      18.817\n",
      "itemLocation.country_UA                     28.0634     13.668      2.053      0.041       1.213      54.914\n",
      "itemLocation.country_US                     -0.1315     11.932     -0.011      0.991     -23.572      23.309\n",
      "category_name_Soccer-International Clubs     9.3113      4.161      2.238      0.026       1.138      17.485\n",
      "category_name_Soccer-MLS                    21.1957      9.429      2.248      0.025       2.672      39.719\n",
      "category_name_Soccer-National Teams         10.1597      4.291      2.368      0.018       1.730      18.590\n",
      "category_name_Soccer-Other                  -7.6599      7.180     -1.067      0.287     -21.764       6.444\n",
      "country_Brazil                               1.7285      5.249      0.329      0.742      -8.582      12.039\n",
      "country_England                            -28.1371     11.563     -2.433      0.015     -50.852      -5.422\n",
      "country_France                              17.1337      6.134      2.793      0.005       5.083      29.184\n",
      "country_Germany                            -12.2400      7.714     -1.587      0.113     -27.394       2.914\n",
      "country_Italy                              -11.1294      7.959     -1.398      0.163     -26.764       4.506\n",
      "country_Mexico                               2.8027      3.055      0.917      0.359      -3.199       8.804\n",
      "country_South Korea                         29.2025     10.747      2.717      0.007       8.089      50.316\n",
      "country_Spain                               45.8008      9.898      4.627      0.000      26.356      65.245\n",
      "country_USA                                 20.7460      5.736      3.617      0.000       9.477      32.015\n",
      "club_Barcelona                               4.9290      4.697      1.049      0.294      -4.297      14.156\n",
      "club_Bayern Munich                          24.0813     11.373      2.117      0.035       1.739      46.423\n",
      "club_Chelsea                                 9.3598      6.278      1.491      0.137      -2.974      21.693\n",
      "club_Club America                           -2.1240      7.050     -0.301      0.763     -15.975      11.727\n",
      "club_Inter Miami                           -58.1386     20.489     -2.838      0.005     -98.389     -17.888\n",
      "club_Inter Milan                             5.2089     11.639      0.448      0.655     -17.655      28.073\n",
      "club_Juventus                                9.5225      6.864      1.387      0.166      -3.962      23.007\n",
      "club_Liverpool                              14.0721      7.544      1.865      0.063      -0.748      28.892\n",
      "club_Manchester City                        -1.2709     12.306     -0.103      0.918     -25.446      22.904\n",
      "club_Manchester United                       1.0281      4.093      0.251      0.802      -7.013       9.069\n",
      "club_Paris Saint Germain                   -10.1370      8.934     -1.135      0.257     -27.688       7.414\n",
      "club_Real Madrid                             4.6615      3.530      1.321      0.187      -2.273      11.596\n",
      "==============================================================================\n",
      "Omnibus:                      296.334   Durbin-Watson:                   1.933\n",
      "Prob(Omnibus):                  0.000   Jarque-Bera (JB):             8151.750\n",
      "Skew:                           1.617   Prob(JB):                         0.00\n",
      "Kurtosis:                      20.842   Cond. No.                     1.42e+16\n",
      "==============================================================================\n",
      "\n",
      "Notes:\n",
      "[1] Standard Errors assume that the covariance matrix of the errors is correctly specified.\n",
      "[2] The smallest eigenvalue is 5.94e-30. This might indicate that there are\n",
      "strong multicollinearity problems or that the design matrix is singular.\n"
     ]
    }
   ],
   "source": [
    "X_combined_soccer = pd.concat([X_train_soccer, X_val_soccer], axis=0)\n",
    "y_combined_soccer = pd.concat([y_train_soccer, y_val_soccer], axis=0)\n",
    "\n",
    "# Add constant (intercept)\n",
    "X = sm.add_constant(X_combined_soccer.astype(float))  \n",
    "model = sm.OLS(y_combined_soccer, X).fit()\n",
    "\n",
    "# Get summary\n",
    "print(model.summary())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "id": "5e4788d8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                            OLS Regression Results                            \n",
      "==============================================================================\n",
      "Dep. Variable:            price.value   R-squared:                       0.606\n",
      "Model:                            OLS   Adj. R-squared:                  0.553\n",
      "Method:                 Least Squares   F-statistic:                     11.50\n",
      "Date:                Sun, 10 Aug 2025   Prob (F-statistic):           2.78e-69\n",
      "Time:                        14:16:33   Log-Likelihood:                -88.574\n",
      "No. Observations:                 595   AIC:                             319.1\n",
      "Df Residuals:                     524   BIC:                             630.7\n",
      "Df Model:                          70                                         \n",
      "Covariance Type:            nonrobust                                         \n",
      "============================================================================================================\n",
      "                                               coef    std err          t      P>|t|      [0.025      0.975]\n",
      "------------------------------------------------------------------------------------------------------------\n",
      "const                                        3.3770      0.173     19.494      0.000       3.037       3.717\n",
      "seller.feedbackPercentage                    0.0165      0.014      1.221      0.223      -0.010       0.043\n",
      "seller.feedbackScore                         0.0121      0.014      0.865      0.387      -0.015       0.040\n",
      "marketingPrice.discountPercentage           -0.0245      0.015     -1.611      0.108      -0.054       0.005\n",
      "shipping_cost                               -0.0527      0.022     -2.421      0.016      -0.095      -0.010\n",
      "days_listed                                 -0.0003      0.014     -0.019      0.985      -0.028       0.028\n",
      "seller_item_count                            0.0228      0.017      1.377      0.169      -0.010       0.055\n",
      "year                                        -0.0515      0.016     -3.317      0.001      -0.082      -0.021\n",
      "additional_image_count                       0.0197      0.017      1.149      0.251      -0.014       0.053\n",
      "title_length                                 0.0292      0.019      1.578      0.115      -0.007       0.066\n",
      "topRatedBuyingExperience                     0.1984      0.070      2.818      0.005       0.060       0.337\n",
      "priorityListing                              0.0407      0.040      1.026      0.306      -0.037       0.119\n",
      "Messi                                        0.0274      0.096      0.284      0.777      -0.162       0.217\n",
      "condition_New with tags                      0.3623      0.082      4.445      0.000       0.202       0.522\n",
      "condition_New without tags                   0.5410      0.211      2.560      0.011       0.126       0.956\n",
      "condition_Pre-owned                          0.4220      0.311      1.357      0.175      -0.189       1.033\n",
      "condition_Unspecified                        0.3327      0.120      2.783      0.006       0.098       0.568\n",
      "condition_Used                              -0.0778      0.045     -1.716      0.087      -0.167       0.011\n",
      "itemLocation.country_AR                      0.3854      0.270      1.428      0.154      -0.145       0.916\n",
      "itemLocation.country_BD                   4.598e-17   8.12e-16      0.057      0.955   -1.55e-15    1.64e-15\n",
      "itemLocation.country_BE                      0.1924      0.215      0.894      0.372      -0.231       0.615\n",
      "itemLocation.country_BR                  -1.982e-16   1.52e-15     -0.131      0.896   -3.18e-15    2.78e-15\n",
      "itemLocation.country_CA                     -0.1097      0.204     -0.537      0.591      -0.511       0.291\n",
      "itemLocation.country_CL                      1.2123      0.370      3.274      0.001       0.485       1.940\n",
      "itemLocation.country_DE                      0.1402      0.216      0.649      0.517      -0.284       0.565\n",
      "itemLocation.country_FR                      0.1521      0.194      0.783      0.434      -0.229       0.534\n",
      "itemLocation.country_GB                     -0.0001      0.358     -0.000      1.000      -0.704       0.704\n",
      "itemLocation.country_ID                      0.2344      0.357      0.656      0.512      -0.468       0.936\n",
      "itemLocation.country_IL                     -0.5832      0.354     -1.646      0.100      -1.279       0.113\n",
      "itemLocation.country_JO                     -0.3706      0.354     -1.046      0.296      -1.067       0.326\n",
      "itemLocation.country_JP                      1.1551      0.259      4.466      0.000       0.647       1.663\n",
      "itemLocation.country_KR                      1.1699      0.245      4.771      0.000       0.688       1.652\n",
      "itemLocation.country_LK                     -0.2764      0.241     -1.147      0.252      -0.750       0.197\n",
      "itemLocation.country_MA                      0.3628      0.373      0.972      0.332      -0.371       1.096\n",
      "itemLocation.country_MX                      0.0699      0.364      0.192      0.848      -0.644       0.784\n",
      "itemLocation.country_MY                     -1.9772      0.356     -5.559      0.000      -2.676      -1.278\n",
      "itemLocation.country_NL                      0.3694      0.254      1.454      0.147      -0.130       0.869\n",
      "itemLocation.country_NZ                     -0.4314      0.191     -2.258      0.024      -0.807      -0.056\n",
      "itemLocation.country_PE                      0.9312      0.290      3.207      0.001       0.361       1.502\n",
      "itemLocation.country_PH                      0.1407      0.243      0.579      0.563      -0.337       0.618\n",
      "itemLocation.country_PK                      0.0229      0.258      0.089      0.929      -0.484       0.530\n",
      "itemLocation.country_PT                      0.1633      0.255      0.641      0.522      -0.338       0.664\n",
      "itemLocation.country_SA                      0.1921      0.239      0.804      0.422      -0.277       0.661\n",
      "itemLocation.country_SG                      0.4763      0.355      1.341      0.180      -0.221       1.174\n",
      "itemLocation.country_SV                     -0.4476      0.332     -1.348      0.178      -1.100       0.205\n",
      "itemLocation.country_TH                      0.1723      0.256      0.672      0.502      -0.331       0.676\n",
      "itemLocation.country_TW                     -0.2061      0.209     -0.986      0.324      -0.617       0.204\n",
      "itemLocation.country_UA                      0.1755      0.214      0.822      0.411      -0.244       0.595\n",
      "itemLocation.country_US                     -0.0696      0.186     -0.373      0.709      -0.436       0.297\n",
      "category_name_Soccer-International Clubs     0.5591      0.065      8.600      0.000       0.431       0.687\n",
      "category_name_Soccer-MLS                     0.7424      0.147      5.039      0.000       0.453       1.032\n",
      "category_name_Soccer-National Teams          0.5561      0.067      8.294      0.000       0.424       0.688\n",
      "category_name_Soccer-Other                   0.1941      0.112      1.730      0.084      -0.026       0.415\n",
      "country_Brazil                               0.0805      0.082      0.982      0.326      -0.081       0.242\n",
      "country_England                             -0.5254      0.181     -2.908      0.004      -0.880      -0.171\n",
      "country_France                               0.2773      0.096      2.893      0.004       0.089       0.466\n",
      "country_Germany                             -0.2050      0.121     -1.701      0.090      -0.442       0.032\n",
      "country_Italy                               -0.0796      0.124     -0.640      0.522      -0.324       0.165\n",
      "country_Mexico                               0.0800      0.048      1.676      0.094      -0.014       0.174\n",
      "country_South Korea                          0.1374      0.168      0.818      0.413      -0.192       0.467\n",
      "country_Spain                                0.2669      0.155      1.726      0.085      -0.037       0.571\n",
      "country_USA                                  0.3089      0.090      3.446      0.001       0.133       0.485\n",
      "club_Barcelona                               0.0491      0.073      0.670      0.503      -0.095       0.193\n",
      "club_Bayern Munich                           0.4057      0.178      2.283      0.023       0.057       0.755\n",
      "club_Chelsea                                 0.1750      0.098      1.784      0.075      -0.018       0.368\n",
      "club_Club America                           -0.0754      0.110     -0.684      0.494      -0.292       0.141\n",
      "club_Inter Miami                            -1.2601      0.320     -3.936      0.000      -1.889      -0.631\n",
      "club_Inter Milan                             0.1143      0.182      0.629      0.530      -0.243       0.472\n",
      "club_Juventus                                0.1964      0.107      1.831      0.068      -0.014       0.407\n",
      "club_Liverpool                               0.1751      0.118      1.485      0.138      -0.056       0.407\n",
      "club_Manchester City                        -0.0291      0.192     -0.151      0.880      -0.407       0.349\n",
      "club_Manchester United                       0.0668      0.064      1.044      0.297      -0.059       0.192\n",
      "club_Paris Saint Germain                    -0.1950      0.140     -1.397      0.163      -0.469       0.079\n",
      "club_Real Madrid                             0.0851      0.055      1.544      0.123      -0.023       0.194\n",
      "==============================================================================\n",
      "Omnibus:                      146.338   Durbin-Watson:                   2.016\n",
      "Prob(Omnibus):                  0.000   Jarque-Bera (JB):             1059.907\n",
      "Skew:                          -0.880   Prob(JB):                    6.98e-231\n",
      "Kurtosis:                       9.297   Cond. No.                     1.42e+16\n",
      "==============================================================================\n",
      "\n",
      "Notes:\n",
      "[1] Standard Errors assume that the covariance matrix of the errors is correctly specified.\n",
      "[2] The smallest eigenvalue is 5.94e-30. This might indicate that there are\n",
      "strong multicollinearity problems or that the design matrix is singular.\n"
     ]
    }
   ],
   "source": [
    "# Add constant (intercept)\n",
    "y_combined_soccer_log = np.log(y_combined_soccer)\n",
    "X = sm.add_constant(X_combined_soccer.astype(float))  # or: sm.add_constant(df[['feature1', 'feature2', ...]])\n",
    "model_log = sm.OLS(y_combined_soccer_log, X).fit()\n",
    "\n",
    "# Get summary\n",
    "print(model_log.summary())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "id": "a1dc4852",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Original Price - Test RMSE: 29.8098\n",
      "Log Price - Test RMSE: 0.3825\n",
      "Log Price Converted - Test RMSE: 28.5054\n"
     ]
    }
   ],
   "source": [
    "X_test_soccer_const = sm.add_constant(X_test_soccer, has_constant='add')\n",
    "\n",
    "y_test_pred_orig = model.predict(X_test_soccer_const)\n",
    "test_rmse_orig = np.sqrt(mean_squared_error(y_test_soccer, y_test_pred_orig))\n",
    "print(f\"Original Price - Test RMSE: {test_rmse_orig:.4f}\")\n",
    "\n",
    "y_test_pred_log = model_log.predict(X_test_soccer_const)\n",
    "y_test_pred_log = np.array(y_test_pred_log, dtype=float)\n",
    "y_test_pred_log_exp = np.exp(y_test_pred_log)  # back-transform\n",
    "\n",
    "test_rmse_log = np.sqrt(mean_squared_error(np.log(y_test_soccer), y_test_pred_log))\n",
    "print(f\"Log Price - Test RMSE: {test_rmse_log:.4f}\")\n",
    "test_rmse_log_exp = np.sqrt(mean_squared_error(y_test_soccer, y_test_pred_log_exp))\n",
    "print(f\"Log Price Converted - Test RMSE: {test_rmse_log_exp:.4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "id": "4c27a673",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/svg+xml": [
       "<?xml version=\"1.0\" encoding=\"UTF-8\" standalone=\"no\"?>\n",
       "<!DOCTYPE svg PUBLIC \"-//W3C//DTD SVG 1.1//EN\"\n",
       " \"http://www.w3.org/Graphics/SVG/1.1/DTD/svg11.dtd\">\n",
       "<!-- Generated by graphviz version 12.2.1 (20241206.2353)\n",
       " -->\n",
       "<!-- Title: G Pages: 1 -->\n",
       "<svg width=\"372pt\" height=\"268pt\"\n",
       " viewBox=\"0.00 0.00 371.60 268.20\" xmlns=\"http://www.w3.org/2000/svg\" xmlns:xlink=\"http://www.w3.org/1999/xlink\">\n",
       "<g id=\"graph0\" class=\"graph\" transform=\"scale(1 1) rotate(0) translate(4 264.2)\">\n",
       "<title>G</title>\n",
       "<polygon fill=\"white\" stroke=\"none\" points=\"-4,4 -4,-264.2 367.6,-264.2 367.6,4 -4,4\"/>\n",
       "<!-- x1 -->\n",
       "<g id=\"node1\" class=\"node\">\n",
       "<title>x1</title>\n",
       "<ellipse fill=\"skyblue\" stroke=\"black\" cx=\"12.6\" cy=\"-247.6\" rx=\"12.6\" ry=\"12.6\"/>\n",
       "<text text-anchor=\"middle\" x=\"12.6\" y=\"-242.55\" font-family=\"Times,serif\" font-size=\"14.00\">x</text>\n",
       "</g>\n",
       "<!-- x2 -->\n",
       "<g id=\"node2\" class=\"node\">\n",
       "<title>x2</title>\n",
       "<ellipse fill=\"skyblue\" stroke=\"black\" cx=\"12.6\" cy=\"-200.6\" rx=\"12.6\" ry=\"12.6\"/>\n",
       "<text text-anchor=\"middle\" x=\"12.6\" y=\"-195.55\" font-family=\"Times,serif\" font-size=\"14.00\">x</text>\n",
       "</g>\n",
       "<!-- x1&#45;&gt;x2 -->\n",
       "<!-- a1 -->\n",
       "<g id=\"node7\" class=\"node\">\n",
       "<title>a1</title>\n",
       "<ellipse fill=\"lightgreen\" stroke=\"black\" cx=\"181.8\" cy=\"-200.6\" rx=\"12.6\" ry=\"12.6\"/>\n",
       "<text text-anchor=\"middle\" x=\"181.8\" y=\"-195.55\" font-family=\"Times,serif\" font-size=\"14.00\">a</text>\n",
       "</g>\n",
       "<!-- x1&#45;&gt;a1 -->\n",
       "<!-- x1&#45;&gt;a1 -->\n",
       "<g id=\"edge17\" class=\"edge\">\n",
       "<title>x1&#45;&gt;a1</title>\n",
       "<path fill=\"none\" stroke=\"black\" d=\"M25.62,-248.75C55.37,-245.18 132.38,-223.7 165.4,-209.95\"/>\n",
       "<polygon fill=\"black\" stroke=\"black\" points=\"165.77,-211.7 169.64,-208.09 164.36,-208.5 165.77,-211.7\"/>\n",
       "</g>\n",
       "<!-- a2 -->\n",
       "<g id=\"node8\" class=\"node\">\n",
       "<title>a2</title>\n",
       "<ellipse fill=\"lightgreen\" stroke=\"black\" cx=\"181.8\" cy=\"-153.6\" rx=\"12.6\" ry=\"12.6\"/>\n",
       "<text text-anchor=\"middle\" x=\"181.8\" y=\"-148.55\" font-family=\"Times,serif\" font-size=\"14.00\">a</text>\n",
       "</g>\n",
       "<!-- x1&#45;&gt;a2 -->\n",
       "<g id=\"edge18\" class=\"edge\">\n",
       "<title>x1&#45;&gt;a2</title>\n",
       "<path fill=\"none\" stroke=\"black\" d=\"M24.06,-241.72C52.44,-225.76 131.06,-181.56 164.93,-162.52\"/>\n",
       "<polygon fill=\"black\" stroke=\"black\" points=\"165.52,-164.2 169.03,-160.22 163.81,-161.14 165.52,-164.2\"/>\n",
       "</g>\n",
       "<!-- a3 -->\n",
       "<g id=\"node9\" class=\"node\">\n",
       "<title>a3</title>\n",
       "<ellipse fill=\"lightgreen\" stroke=\"black\" cx=\"181.8\" cy=\"-106.6\" rx=\"12.6\" ry=\"12.6\"/>\n",
       "<text text-anchor=\"middle\" x=\"181.8\" y=\"-101.55\" font-family=\"Times,serif\" font-size=\"14.00\">a</text>\n",
       "</g>\n",
       "<!-- x1&#45;&gt;a3 -->\n",
       "<g id=\"edge19\" class=\"edge\">\n",
       "<title>x1&#45;&gt;a3</title>\n",
       "<path fill=\"none\" stroke=\"black\" d=\"M22.59,-240.02C50.15,-216.78 133.58,-146.42 166.87,-118.35\"/>\n",
       "<polygon fill=\"black\" stroke=\"black\" points=\"167.91,-119.76 170.61,-115.19 165.66,-117.08 167.91,-119.76\"/>\n",
       "</g>\n",
       "<!-- a4 -->\n",
       "<g id=\"node10\" class=\"node\">\n",
       "<title>a4</title>\n",
       "<ellipse fill=\"lightgreen\" stroke=\"black\" cx=\"181.8\" cy=\"-59.6\" rx=\"12.6\" ry=\"12.6\"/>\n",
       "<text text-anchor=\"middle\" x=\"181.8\" y=\"-54.55\" font-family=\"Times,serif\" font-size=\"14.00\">a</text>\n",
       "</g>\n",
       "<!-- x1&#45;&gt;a4 -->\n",
       "<g id=\"edge20\" class=\"edge\">\n",
       "<title>x1&#45;&gt;a4</title>\n",
       "<path fill=\"none\" stroke=\"black\" d=\"M21.66,-238.53C48.76,-208.07 136.62,-109.28 168.83,-73.06\"/>\n",
       "<polygon fill=\"black\" stroke=\"black\" points=\"169.8,-74.61 171.81,-69.71 167.18,-72.28 169.8,-74.61\"/>\n",
       "</g>\n",
       "<!-- x3 -->\n",
       "<g id=\"node3\" class=\"node\">\n",
       "<title>x3</title>\n",
       "<ellipse fill=\"skyblue\" stroke=\"black\" cx=\"12.6\" cy=\"-153.6\" rx=\"12.6\" ry=\"12.6\"/>\n",
       "<text text-anchor=\"middle\" x=\"12.6\" y=\"-148.55\" font-family=\"Times,serif\" font-size=\"14.00\">x</text>\n",
       "</g>\n",
       "<!-- x2&#45;&gt;x3 -->\n",
       "<!-- x2&#45;&gt;a1 -->\n",
       "<g id=\"edge21\" class=\"edge\">\n",
       "<title>x2&#45;&gt;a1</title>\n",
       "<path fill=\"none\" stroke=\"black\" d=\"M25.62,-200.6C54.55,-200.6 128.14,-200.6 162.55,-200.6\"/>\n",
       "<polygon fill=\"black\" stroke=\"black\" points=\"162.37,-202.35 167.37,-200.6 162.37,-198.85 162.37,-202.35\"/>\n",
       "</g>\n",
       "<!-- x2&#45;&gt;a2 -->\n",
       "<!-- x2&#45;&gt;a2 -->\n",
       "<g id=\"edge22\" class=\"edge\">\n",
       "<title>x2&#45;&gt;a2</title>\n",
       "<path fill=\"none\" stroke=\"black\" d=\"M25.62,-201.75C55.37,-198.18 132.38,-176.7 165.4,-162.95\"/>\n",
       "<polygon fill=\"black\" stroke=\"black\" points=\"165.77,-164.7 169.64,-161.09 164.36,-161.5 165.77,-164.7\"/>\n",
       "</g>\n",
       "<!-- x2&#45;&gt;a3 -->\n",
       "<g id=\"edge23\" class=\"edge\">\n",
       "<title>x2&#45;&gt;a3</title>\n",
       "<path fill=\"none\" stroke=\"black\" d=\"M24.06,-194.72C52.44,-178.76 131.06,-134.56 164.93,-115.52\"/>\n",
       "<polygon fill=\"black\" stroke=\"black\" points=\"165.52,-117.2 169.03,-113.22 163.81,-114.14 165.52,-117.2\"/>\n",
       "</g>\n",
       "<!-- x2&#45;&gt;a4 -->\n",
       "<g id=\"edge24\" class=\"edge\">\n",
       "<title>x2&#45;&gt;a4</title>\n",
       "<path fill=\"none\" stroke=\"black\" d=\"M22.59,-193.02C50.15,-169.78 133.58,-99.42 166.87,-71.35\"/>\n",
       "<polygon fill=\"black\" stroke=\"black\" points=\"167.91,-72.76 170.61,-68.19 165.66,-70.08 167.91,-72.76\"/>\n",
       "</g>\n",
       "<!-- x4 -->\n",
       "<g id=\"node4\" class=\"node\">\n",
       "<title>x4</title>\n",
       "<ellipse fill=\"skyblue\" stroke=\"black\" cx=\"12.6\" cy=\"-106.6\" rx=\"12.6\" ry=\"12.6\"/>\n",
       "<text text-anchor=\"middle\" x=\"12.6\" y=\"-101.55\" font-family=\"Times,serif\" font-size=\"14.00\">x</text>\n",
       "</g>\n",
       "<!-- x3&#45;&gt;x4 -->\n",
       "<!-- x3&#45;&gt;a1 -->\n",
       "<g id=\"edge25\" class=\"edge\">\n",
       "<title>x3&#45;&gt;a1</title>\n",
       "<path fill=\"none\" stroke=\"black\" d=\"M25.09,-156.83C53.85,-164.91 129.05,-186.05 163.32,-195.69\"/>\n",
       "<polygon fill=\"black\" stroke=\"black\" points=\"162.48,-197.27 167.77,-196.94 163.43,-193.9 162.48,-197.27\"/>\n",
       "</g>\n",
       "<!-- x3&#45;&gt;a2 -->\n",
       "<g id=\"edge26\" class=\"edge\">\n",
       "<title>x3&#45;&gt;a2</title>\n",
       "<path fill=\"none\" stroke=\"black\" d=\"M25.62,-153.6C54.55,-153.6 128.14,-153.6 162.55,-153.6\"/>\n",
       "<polygon fill=\"black\" stroke=\"black\" points=\"162.37,-155.35 167.37,-153.6 162.37,-151.85 162.37,-155.35\"/>\n",
       "</g>\n",
       "<!-- x3&#45;&gt;a3 -->\n",
       "<!-- x3&#45;&gt;a3 -->\n",
       "<g id=\"edge27\" class=\"edge\">\n",
       "<title>x3&#45;&gt;a3</title>\n",
       "<path fill=\"none\" stroke=\"black\" d=\"M25.62,-154.75C55.37,-151.18 132.38,-129.7 165.4,-115.95\"/>\n",
       "<polygon fill=\"black\" stroke=\"black\" points=\"165.77,-117.7 169.64,-114.09 164.36,-114.5 165.77,-117.7\"/>\n",
       "</g>\n",
       "<!-- x3&#45;&gt;a4 -->\n",
       "<g id=\"edge28\" class=\"edge\">\n",
       "<title>x3&#45;&gt;a4</title>\n",
       "<path fill=\"none\" stroke=\"black\" d=\"M24.06,-147.72C52.44,-131.76 131.06,-87.56 164.93,-68.52\"/>\n",
       "<polygon fill=\"black\" stroke=\"black\" points=\"165.52,-70.2 169.03,-66.22 163.81,-67.14 165.52,-70.2\"/>\n",
       "</g>\n",
       "<!-- x5 -->\n",
       "<g id=\"node5\" class=\"node\">\n",
       "<title>x5</title>\n",
       "<ellipse fill=\"skyblue\" stroke=\"black\" cx=\"12.6\" cy=\"-59.6\" rx=\"12.6\" ry=\"12.6\"/>\n",
       "<text text-anchor=\"middle\" x=\"12.6\" y=\"-54.55\" font-family=\"Times,serif\" font-size=\"14.00\">x</text>\n",
       "</g>\n",
       "<!-- x4&#45;&gt;x5 -->\n",
       "<!-- x4&#45;&gt;a1 -->\n",
       "<g id=\"edge29\" class=\"edge\">\n",
       "<title>x4&#45;&gt;a1</title>\n",
       "<path fill=\"none\" stroke=\"black\" d=\"M24.06,-112.48C52.44,-128.44 131.06,-172.64 164.93,-191.68\"/>\n",
       "<polygon fill=\"black\" stroke=\"black\" points=\"163.81,-193.06 169.03,-193.98 165.52,-190 163.81,-193.06\"/>\n",
       "</g>\n",
       "<!-- x4&#45;&gt;a2 -->\n",
       "<g id=\"edge30\" class=\"edge\">\n",
       "<title>x4&#45;&gt;a2</title>\n",
       "<path fill=\"none\" stroke=\"black\" d=\"M25.09,-109.83C53.85,-117.91 129.05,-139.05 163.32,-148.69\"/>\n",
       "<polygon fill=\"black\" stroke=\"black\" points=\"162.48,-150.27 167.77,-149.94 163.43,-146.9 162.48,-150.27\"/>\n",
       "</g>\n",
       "<!-- x4&#45;&gt;a3 -->\n",
       "<g id=\"edge31\" class=\"edge\">\n",
       "<title>x4&#45;&gt;a3</title>\n",
       "<path fill=\"none\" stroke=\"black\" d=\"M25.62,-106.6C54.55,-106.6 128.14,-106.6 162.55,-106.6\"/>\n",
       "<polygon fill=\"black\" stroke=\"black\" points=\"162.37,-108.35 167.37,-106.6 162.37,-104.85 162.37,-108.35\"/>\n",
       "</g>\n",
       "<!-- x4&#45;&gt;a4 -->\n",
       "<g id=\"edge32\" class=\"edge\">\n",
       "<title>x4&#45;&gt;a4</title>\n",
       "<path fill=\"none\" stroke=\"black\" d=\"M25.09,-103.37C53.85,-95.29 129.05,-74.15 163.32,-64.51\"/>\n",
       "<polygon fill=\"black\" stroke=\"black\" points=\"163.43,-66.3 167.77,-63.26 162.48,-62.93 163.43,-66.3\"/>\n",
       "</g>\n",
       "<!-- x6 -->\n",
       "<g id=\"node6\" class=\"node\">\n",
       "<title>x6</title>\n",
       "<ellipse fill=\"skyblue\" stroke=\"black\" cx=\"12.6\" cy=\"-12.6\" rx=\"12.6\" ry=\"12.6\"/>\n",
       "<text text-anchor=\"middle\" x=\"12.6\" y=\"-7.55\" font-family=\"Times,serif\" font-size=\"14.00\">x</text>\n",
       "</g>\n",
       "<!-- x5&#45;&gt;x6 -->\n",
       "<!-- x5&#45;&gt;a1 -->\n",
       "<g id=\"edge33\" class=\"edge\">\n",
       "<title>x5&#45;&gt;a1</title>\n",
       "<path fill=\"none\" stroke=\"black\" d=\"M22.59,-67.18C50.15,-90.42 133.58,-160.78 166.87,-188.85\"/>\n",
       "<polygon fill=\"black\" stroke=\"black\" points=\"165.66,-190.12 170.61,-192.01 167.91,-187.44 165.66,-190.12\"/>\n",
       "</g>\n",
       "<!-- x5&#45;&gt;a2 -->\n",
       "<g id=\"edge34\" class=\"edge\">\n",
       "<title>x5&#45;&gt;a2</title>\n",
       "<path fill=\"none\" stroke=\"black\" d=\"M24.06,-65.48C52.44,-81.44 131.06,-125.64 164.93,-144.68\"/>\n",
       "<polygon fill=\"black\" stroke=\"black\" points=\"163.81,-146.06 169.03,-146.98 165.52,-143 163.81,-146.06\"/>\n",
       "</g>\n",
       "<!-- x5&#45;&gt;a3 -->\n",
       "<g id=\"edge35\" class=\"edge\">\n",
       "<title>x5&#45;&gt;a3</title>\n",
       "<path fill=\"none\" stroke=\"black\" d=\"M25.09,-62.83C53.85,-70.91 129.05,-92.05 163.32,-101.69\"/>\n",
       "<polygon fill=\"black\" stroke=\"black\" points=\"162.48,-103.27 167.77,-102.94 163.43,-99.9 162.48,-103.27\"/>\n",
       "</g>\n",
       "<!-- x5&#45;&gt;a4 -->\n",
       "<g id=\"edge36\" class=\"edge\">\n",
       "<title>x5&#45;&gt;a4</title>\n",
       "<path fill=\"none\" stroke=\"black\" d=\"M25.62,-59.6C54.55,-59.6 128.14,-59.6 162.55,-59.6\"/>\n",
       "<polygon fill=\"black\" stroke=\"black\" points=\"162.37,-61.35 167.37,-59.6 162.37,-57.85 162.37,-61.35\"/>\n",
       "</g>\n",
       "<!-- x6&#45;&gt;a1 -->\n",
       "<g id=\"edge37\" class=\"edge\">\n",
       "<title>x6&#45;&gt;a1</title>\n",
       "<path fill=\"none\" stroke=\"black\" d=\"M21.66,-21.67C48.76,-52.13 136.62,-150.92 168.83,-187.14\"/>\n",
       "<polygon fill=\"black\" stroke=\"black\" points=\"167.18,-187.92 171.81,-190.49 169.8,-185.59 167.18,-187.92\"/>\n",
       "</g>\n",
       "<!-- x6&#45;&gt;a2 -->\n",
       "<g id=\"edge38\" class=\"edge\">\n",
       "<title>x6&#45;&gt;a2</title>\n",
       "<path fill=\"none\" stroke=\"black\" d=\"M22.59,-20.18C50.15,-43.42 133.58,-113.78 166.87,-141.85\"/>\n",
       "<polygon fill=\"black\" stroke=\"black\" points=\"165.66,-143.12 170.61,-145.01 167.91,-140.44 165.66,-143.12\"/>\n",
       "</g>\n",
       "<!-- x6&#45;&gt;a3 -->\n",
       "<g id=\"edge39\" class=\"edge\">\n",
       "<title>x6&#45;&gt;a3</title>\n",
       "<path fill=\"none\" stroke=\"black\" d=\"M24.06,-18.48C52.44,-34.44 131.06,-78.64 164.93,-97.68\"/>\n",
       "<polygon fill=\"black\" stroke=\"black\" points=\"163.81,-99.06 169.03,-99.98 165.52,-96 163.81,-99.06\"/>\n",
       "</g>\n",
       "<!-- x6&#45;&gt;a4 -->\n",
       "<g id=\"edge40\" class=\"edge\">\n",
       "<title>x6&#45;&gt;a4</title>\n",
       "<path fill=\"none\" stroke=\"black\" d=\"M25.09,-15.83C53.85,-23.91 129.05,-45.05 163.32,-54.69\"/>\n",
       "<polygon fill=\"black\" stroke=\"black\" points=\"162.48,-56.27 167.77,-55.94 163.43,-52.9 162.48,-56.27\"/>\n",
       "</g>\n",
       "<!-- a1&#45;&gt;a2 -->\n",
       "<!-- y1 -->\n",
       "<g id=\"node11\" class=\"node\">\n",
       "<title>y1</title>\n",
       "<ellipse fill=\"salmon\" stroke=\"black\" cx=\"351\" cy=\"-200.6\" rx=\"12.6\" ry=\"12.6\"/>\n",
       "<text text-anchor=\"middle\" x=\"351\" y=\"-195.55\" font-family=\"Times,serif\" font-size=\"14.00\">y</text>\n",
       "</g>\n",
       "<!-- a1&#45;&gt;y1 -->\n",
       "<!-- a1&#45;&gt;y1 -->\n",
       "<g id=\"edge41\" class=\"edge\">\n",
       "<title>a1&#45;&gt;y1</title>\n",
       "<path fill=\"none\" stroke=\"black\" d=\"M194.03,-204.99C222.59,-209.82 298.13,-210.12 332.52,-205.89\"/>\n",
       "<polygon fill=\"black\" stroke=\"black\" points=\"332.62,-207.64 337.32,-205.2 332.12,-204.18 332.62,-207.64\"/>\n",
       "</g>\n",
       "<!-- y2 -->\n",
       "<g id=\"node12\" class=\"node\">\n",
       "<title>y2</title>\n",
       "<ellipse fill=\"salmon\" stroke=\"black\" cx=\"351\" cy=\"-153.6\" rx=\"12.6\" ry=\"12.6\"/>\n",
       "<text text-anchor=\"middle\" x=\"351\" y=\"-148.55\" font-family=\"Times,serif\" font-size=\"14.00\">y</text>\n",
       "</g>\n",
       "<!-- a1&#45;&gt;y2 -->\n",
       "<g id=\"edge42\" class=\"edge\">\n",
       "<title>a1&#45;&gt;y2</title>\n",
       "<path fill=\"none\" stroke=\"black\" d=\"M194.29,-197.37C223.05,-189.29 298.25,-168.15 332.52,-158.51\"/>\n",
       "<polygon fill=\"black\" stroke=\"black\" points=\"332.63,-160.3 336.97,-157.26 331.68,-156.93 332.63,-160.3\"/>\n",
       "</g>\n",
       "<!-- y3 -->\n",
       "<g id=\"node13\" class=\"node\">\n",
       "<title>y3</title>\n",
       "<ellipse fill=\"salmon\" stroke=\"black\" cx=\"351\" cy=\"-106.6\" rx=\"12.6\" ry=\"12.6\"/>\n",
       "<text text-anchor=\"middle\" x=\"351\" y=\"-101.55\" font-family=\"Times,serif\" font-size=\"14.00\">y</text>\n",
       "</g>\n",
       "<!-- a1&#45;&gt;y3 -->\n",
       "<g id=\"edge43\" class=\"edge\">\n",
       "<title>a1&#45;&gt;y3</title>\n",
       "<path fill=\"none\" stroke=\"black\" d=\"M193.26,-194.72C221.64,-178.76 300.26,-134.56 334.13,-115.52\"/>\n",
       "<polygon fill=\"black\" stroke=\"black\" points=\"334.72,-117.2 338.23,-113.22 333.01,-114.14 334.72,-117.2\"/>\n",
       "</g>\n",
       "<!-- a2&#45;&gt;a3 -->\n",
       "<!-- a2&#45;&gt;y1 -->\n",
       "<g id=\"edge44\" class=\"edge\">\n",
       "<title>a2&#45;&gt;y1</title>\n",
       "<path fill=\"none\" stroke=\"black\" d=\"M194.29,-156.83C223.05,-164.91 298.25,-186.05 332.52,-195.69\"/>\n",
       "<polygon fill=\"black\" stroke=\"black\" points=\"331.68,-197.27 336.97,-196.94 332.63,-193.9 331.68,-197.27\"/>\n",
       "</g>\n",
       "<!-- a2&#45;&gt;y2 -->\n",
       "<!-- a2&#45;&gt;y2 -->\n",
       "<g id=\"edge45\" class=\"edge\">\n",
       "<title>a2&#45;&gt;y2</title>\n",
       "<path fill=\"none\" stroke=\"black\" d=\"M194.03,-157.99C222.59,-162.82 298.13,-163.12 332.52,-158.89\"/>\n",
       "<polygon fill=\"black\" stroke=\"black\" points=\"332.62,-160.64 337.32,-158.2 332.12,-157.18 332.62,-160.64\"/>\n",
       "</g>\n",
       "<!-- a2&#45;&gt;y3 -->\n",
       "<g id=\"edge46\" class=\"edge\">\n",
       "<title>a2&#45;&gt;y3</title>\n",
       "<path fill=\"none\" stroke=\"black\" d=\"M194.29,-150.37C223.05,-142.29 298.25,-121.15 332.52,-111.51\"/>\n",
       "<polygon fill=\"black\" stroke=\"black\" points=\"332.63,-113.3 336.97,-110.26 331.68,-109.93 332.63,-113.3\"/>\n",
       "</g>\n",
       "<!-- a3&#45;&gt;a4 -->\n",
       "<!-- a3&#45;&gt;y1 -->\n",
       "<g id=\"edge47\" class=\"edge\">\n",
       "<title>a3&#45;&gt;y1</title>\n",
       "<path fill=\"none\" stroke=\"black\" d=\"M193.26,-112.48C221.64,-128.44 300.26,-172.64 334.13,-191.68\"/>\n",
       "<polygon fill=\"black\" stroke=\"black\" points=\"333.01,-193.06 338.23,-193.98 334.72,-190 333.01,-193.06\"/>\n",
       "</g>\n",
       "<!-- a3&#45;&gt;y2 -->\n",
       "<g id=\"edge48\" class=\"edge\">\n",
       "<title>a3&#45;&gt;y2</title>\n",
       "<path fill=\"none\" stroke=\"black\" d=\"M194.29,-109.83C223.05,-117.91 298.25,-139.05 332.52,-148.69\"/>\n",
       "<polygon fill=\"black\" stroke=\"black\" points=\"331.68,-150.27 336.97,-149.94 332.63,-146.9 331.68,-150.27\"/>\n",
       "</g>\n",
       "<!-- a3&#45;&gt;y3 -->\n",
       "<!-- a3&#45;&gt;y3 -->\n",
       "<g id=\"edge49\" class=\"edge\">\n",
       "<title>a3&#45;&gt;y3</title>\n",
       "<path fill=\"none\" stroke=\"black\" d=\"M194.03,-110.99C222.59,-115.82 298.13,-116.12 332.52,-111.89\"/>\n",
       "<polygon fill=\"black\" stroke=\"black\" points=\"332.62,-113.64 337.32,-111.2 332.12,-110.18 332.62,-113.64\"/>\n",
       "</g>\n",
       "<!-- a4&#45;&gt;y1 -->\n",
       "<g id=\"edge50\" class=\"edge\">\n",
       "<title>a4&#45;&gt;y1</title>\n",
       "<path fill=\"none\" stroke=\"black\" d=\"M191.79,-67.18C219.35,-90.42 302.78,-160.78 336.07,-188.85\"/>\n",
       "<polygon fill=\"black\" stroke=\"black\" points=\"334.86,-190.12 339.81,-192.01 337.11,-187.44 334.86,-190.12\"/>\n",
       "</g>\n",
       "<!-- a4&#45;&gt;y2 -->\n",
       "<g id=\"edge51\" class=\"edge\">\n",
       "<title>a4&#45;&gt;y2</title>\n",
       "<path fill=\"none\" stroke=\"black\" d=\"M193.26,-65.48C221.64,-81.44 300.26,-125.64 334.13,-144.68\"/>\n",
       "<polygon fill=\"black\" stroke=\"black\" points=\"333.01,-146.06 338.23,-146.98 334.72,-143 333.01,-146.06\"/>\n",
       "</g>\n",
       "<!-- a4&#45;&gt;y3 -->\n",
       "<g id=\"edge52\" class=\"edge\">\n",
       "<title>a4&#45;&gt;y3</title>\n",
       "<path fill=\"none\" stroke=\"black\" d=\"M194.29,-62.83C223.05,-70.91 298.25,-92.05 332.52,-101.69\"/>\n",
       "<polygon fill=\"black\" stroke=\"black\" points=\"331.68,-103.27 336.97,-102.94 332.63,-99.9 331.68,-103.27\"/>\n",
       "</g>\n",
       "<!-- y1&#45;&gt;y2 -->\n",
       "<!-- y2&#45;&gt;y3 -->\n",
       "</g>\n",
       "</svg>\n"
      ],
      "text/plain": [
       "<graphviz.sources.Source at 0x330760040>"
      ]
     },
     "execution_count": 62,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from graphviz import Source\n",
    "dot = \"\"\"digraph G {\n",
    "  rankdir=LR;\n",
    "  splines=line;\n",
    "  ranksep=2;    // default ~1, bigger is wider\n",
    "  nodesep=0.3;  // default ~0.6, smaller is shorter vertically\n",
    "  node [shape=circle, style=filled, width=0.35, fixedsize=true];\n",
    "  edge [arrowsize=0.5]\n",
    "\n",
    "  // Input layer (6 nodes)\n",
    "  x1 [label=\"x\", fillcolor=skyblue];\n",
    "  x2 [label=\"x\", fillcolor=skyblue];\n",
    "  x3 [label=\"x\", fillcolor=skyblue];\n",
    "  x4 [label=\"x\", fillcolor=skyblue];\n",
    "  x5 [label=\"x\", fillcolor=skyblue];\n",
    "  x6 [label=\"x\", fillcolor=skyblue];\n",
    "\n",
    "  // Hidden layer (4 nodes)\n",
    "  a1 [label=\"a\", fillcolor=lightgreen];\n",
    "  a2 [label=\"a\", fillcolor=lightgreen];\n",
    "  a3 [label=\"a\", fillcolor=lightgreen];\n",
    "  a4 [label=\"a\", fillcolor=lightgreen];\n",
    "\n",
    "  // Output layer (3 nodes)\n",
    "  y1 [label=\"y\", fillcolor=salmon];\n",
    "  y2 [label=\"y\", fillcolor=salmon];\n",
    "  y3 [label=\"y\", fillcolor=salmon];\n",
    "\n",
    "  { rank=same; x1; x2; x3; x4; x5; x6; }\n",
    "  { rank=same; a1; a2; a3; a4; }\n",
    "  { rank=same; y1; y2; y3; }\n",
    "\n",
    "  x1 -> x2 [style=invis];\n",
    "  x2 -> x3 [style=invis];\n",
    "  x3 -> x4 [style=invis];\n",
    "  x4 -> x5 [style=invis];\n",
    "  x5 -> x6 [style=invis];\n",
    "\n",
    "  a1 -> a2 [style=invis];\n",
    "  a2 -> a3 [style=invis];\n",
    "  a3 -> a4 [style=invis];\n",
    "\n",
    "  y1 -> y2 [style=invis];\n",
    "  y2 -> y3 [style=invis];\n",
    "\n",
    "  x1 -> a1 [style=invis];\n",
    "  a1 -> y1 [style=invis];\n",
    "  x2 -> a2 [style=invis];\n",
    "  a2 -> y2 [style=invis];\n",
    "  x3 -> a3 [style=invis];\n",
    "  a3 -> y3 [style=invis];\n",
    "\n",
    "  x1 -> {a1 a2 a3 a4};\n",
    "  x2 -> {a1 a2 a3 a4};\n",
    "  x3 -> {a1 a2 a3 a4};\n",
    "  x4 -> {a1 a2 a3 a4};\n",
    "  x5 -> {a1 a2 a3 a4};\n",
    "  x6 -> {a1 a2 a3 a4};\n",
    "\n",
    "  a1 -> {y1 y2 y3};\n",
    "  a2 -> {y1 y2 y3};\n",
    "  a3 -> {y1 y2 y3};\n",
    "  a4 -> {y1 y2 y3};\n",
    "}\n",
    "\"\"\"\n",
    "Source(dot)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "id": "bb62089c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'images/5/nn_graph.png'"
      ]
     },
     "execution_count": 63,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import graphviz\n",
    "graph = graphviz.Source(dot)\n",
    "graph.render('images/5/nn_graph', format='png', cleanup=True)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
